{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "864f7d1a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-03T02:56:27.059993Z",
     "iopub.status.busy": "2026-02-03T02:56:27.059237Z",
     "iopub.status.idle": "2026-02-03T02:56:28.575920Z",
     "shell.execute_reply": "2026-02-03T02:56:28.574843Z"
    },
    "papermill": {
     "duration": 1.524708,
     "end_time": "2026-02-03T02:56:28.578153",
     "exception": false,
     "start_time": "2026-02-03T02:56:27.053445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/penyisihan-datavidia-10/sample_submission.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2025.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/NDVI (vegetation index)/indeks-ndvi-jakarta.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/kualitas-air-sungai/data-kualitas-air-sungai-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/libur-nasional/dataset-libur-nasional-dan-weekend.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki3-jagakarsa.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki2-kelapagading.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki1-bundaranhi.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki5-kebonjeruk.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki4-lubangbuaya.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/jumlah-penduduk/data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25139de7",
   "metadata": {
    "papermill": {
     "duration": 0.003908,
     "end_time": "2026-02-03T02:56:28.585996",
     "exception": false,
     "start_time": "2026-02-03T02:56:28.582088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading & Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e5f5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T02:56:28.595637Z",
     "iopub.status.busy": "2026-02-03T02:56:28.595138Z",
     "iopub.status.idle": "2026-02-03T02:56:35.727402Z",
     "shell.execute_reply": "2026-02-03T02:56:35.726221Z"
    },
    "papermill": {
     "duration": 7.140133,
     "end_time": "2026-02-03T02:56:35.730057",
     "exception": false,
     "start_time": "2026-02-03T02:56:28.589924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
      "/tmp/ipykernel_17/255431024.py:111: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1 SUMMARY (ISPU) ===\n",
      "Files loaded: 16\n",
      "Rows raw (all stations): 10,541\n",
      "Rows valid stations (pre-agg): 8,740\n",
      "Rows daily (after agg): 8,637\n",
      "Stations (valid): 5 -> ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "Date range: 2010-01-01 -> 2023-11-30\n",
      "CUTOFF_DATE (forecasting POV): 2025-09-01\n",
      "\n",
      "Label counts (kategori_3) on daily table:\n",
      "kategori_3\n",
      "<NA>           6833\n",
      "SEDANG         1358\n",
      "BAIK            236\n",
      "TIDAK SEHAT     210\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Missing rate (top):\n",
      "kategori_3           0.791131\n",
      "pm_duakomalima       0.623133\n",
      "pm_sepuluh           0.211416\n",
      "sulfur_dioksida      0.185365\n",
      "ozon                 0.182471\n",
      "nitrogen_dioksida    0.182239\n",
      "karbon_monoksida     0.178534\n",
      "max                  0.001042\n",
      "dtype: float64\n",
      "\n",
      "Per-station coverage:\n",
      "              n_days   min_date   max_date  label_na_rate\n",
      "stasiun_code                                             \n",
      "DKI1            1745 2010-01-01 2023-11-30       0.793123\n",
      "DKI2            1723 2010-01-01 2023-11-30       0.789321\n",
      "DKI3            1723 2010-01-01 2023-11-30       0.790482\n",
      "DKI4            1723 2010-01-01 2023-11-30       0.790482\n",
      "DKI5            1723 2010-01-01 2023-11-30       0.792223\n",
      "\n",
      "Per-file date parsing diagnostics (worst -> best):\n",
      "                                                 file  n_rows  \\\n",
      "1   data-indeks-standar-pencemar-udara-(ispu)-di-p...    1830   \n",
      "2   data-indeks-standar-pencemar-udara-(ispu)-di-p...    1215   \n",
      "6   indeks-standar-pencemaran-udara-(ispu)-tahun-2...     365   \n",
      "13  indeks-standar-pencemaran-udara-(ispu)-tahun-2...     366   \n",
      "5   indeks-standar-pencemaran-udara-(ispu)-tahun-2...     366   \n",
      "9   indeks-standar-pencemaran-udara-(ispu)-tahun-2...    1830   \n",
      "4   indeks-standar-pencemaran-udara-(ispu)-tahun-2...     365   \n",
      "7   indeks-standar-pencemaran-udara-(ispu)-tahun-2...    1825   \n",
      "12  indeks-standar-pencemaran-udara-(ispu)-tahun-2...     345   \n",
      "15  indeks-standar-pencemaran-udara-(ispu)-tahun-2...     365   \n",
      "\n",
      "    date_parse_rate max_date_parsed  \n",
      "1          0.000000             NaT  \n",
      "2          0.000000             NaT  \n",
      "6          0.361644      2013-12-12  \n",
      "13         0.390710      2020-12-12  \n",
      "5          0.393443      2012-12-12  \n",
      "9          0.393443      2016-12-12  \n",
      "4          0.394521      2011-12-12  \n",
      "7          0.394521      2014-12-12  \n",
      "12         0.417391      2019-12-12  \n",
      "15         0.997260      2022-12-30  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Data Loading & Sanity Checks (ISPU 2010–2025) — ONE CELL (REVISI FINAL)\n",
    "# Tujuan:\n",
    "# - Load semua file ISPU (2010–2025) dari folder /ISPU\n",
    "# - Standarisasi kolom (tanggal, stasiun_code, polutan, max, kategori)\n",
    "# - Labeling sesuai FAQ panitia -> 3 kelas: BAIK / SEDANG / TIDAK SEHAT\n",
    "# - Buang stasiun tidak valid (hanya DKI1..DKI5)\n",
    "# - Deduplicate (tanggal, stasiun_code) dengan agregasi aman\n",
    "# Output penting untuk stage berikutnya:\n",
    "# - df_ispu  (daily clean)\n",
    "# - CUTOFF_DATE, DATA_ROOT, VALID_STATIONS\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n",
    "ISPU_DIR  = DATA_ROOT / \"ISPU\"\n",
    "CUTOFF_DATE = pd.Timestamp(\"2025-09-01\")  # POV sebelum tanggal ini (aturan panitia)\n",
    "\n",
    "VALID_STATIONS = [\"DKI1\", \"DKI2\", \"DKI3\", \"DKI4\", \"DKI5\"]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def norm_colname(c: str) -> str:\n",
    "    c = str(c).strip().lower()\n",
    "    c = re.sub(r\"[()\\[\\]{}]\", \"\", c)\n",
    "    c = re.sub(r\"[%/]\", \"_\", c)\n",
    "    c = re.sub(r\"[^a-z0-9]+\", \"_\", c)\n",
    "    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "    return c\n",
    "\n",
    "def read_csv_try_enc(p: Path) -> pd.DataFrame:\n",
    "    # sederhana tapi robust untuk encoding berbeda\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.read_csv(p)  # last resort\n",
    "\n",
    "def coalesce_numeric(df: pd.DataFrame, candidates):\n",
    "    cols = [c for c in candidates if c in df.columns]\n",
    "    if len(cols) == 0:\n",
    "        return pd.Series(np.nan, index=df.index)\n",
    "    tmp = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return tmp.mean(axis=1, skipna=True)\n",
    "\n",
    "def coalesce_text(df: pd.DataFrame, candidates):\n",
    "    cols = [c for c in candidates if c in df.columns]\n",
    "    if len(cols) == 0:\n",
    "        return pd.Series(np.nan, index=df.index)\n",
    "    tmp = df[cols].astype(str)\n",
    "    tmp = tmp.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    return tmp.bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "def mode_nonnull(x: pd.Series):\n",
    "    x = x.dropna()\n",
    "    if len(x) == 0:\n",
    "        return np.nan\n",
    "    m = x.mode()\n",
    "    return m.iloc[0] if len(m) else np.nan\n",
    "\n",
    "def extract_stasiun_code(stasiun_series: pd.Series) -> pd.Series:\n",
    "    s = stasiun_series.astype(str).str.upper()\n",
    "    code = s.str.extract(r\"(DKI\\s*\\d+)\", expand=False)\n",
    "    code = code.str.replace(\" \", \"\", regex=False)\n",
    "    return code  # bisa NaN kalau tidak match\n",
    "\n",
    "# ---------- column synonym sets (hindari duplikat nama kolom) ----------\n",
    "DATE_CANDS = [\"tanggal\", \"time\", \"date\", \"waktu\", \"tgl\", \"tanggal_pengukuran\"]\n",
    "STA_CANDS  = [\"stasiun\", \"stasiun_id\", \"station\", \"lokasi\", \"nama_stasiun\"]\n",
    "\n",
    "COLS_NUM = {\n",
    "    \"pm_sepuluh\":        [\"pm_sepuluh\",\"pm10\",\"pm_10\",\"pm10_\",\"pm_10_\"],\n",
    "    \"pm_duakomalima\":    [\"pm_duakomalima\",\"pm25\",\"pm2_5\",\"pm_2_5\",\"pm2.5\",\"pm_2.5\"],\n",
    "    \"sulfur_dioksida\":   [\"sulfur_dioksida\",\"so2\"],\n",
    "    \"karbon_monoksida\":  [\"karbon_monoksida\",\"co\"],\n",
    "    \"ozon\":              [\"ozon\",\"o3\"],\n",
    "    \"nitrogen_dioksida\": [\"nitrogen_dioksida\",\"no2\"],\n",
    "    \"max\":               [\"max\",\"maks\",\"nilai_maks\",\"nilai_max\"],\n",
    "}\n",
    "\n",
    "COLS_TXT = {\n",
    "    \"parameter_pencemar_kritis\": [\"parameter_pencemar_kritis\",\"parameter_kritis\",\"parameter\",\"pencemar_kritis\"],\n",
    "    \"kategori_raw\":              [\"kategori\",\"category\",\"kategori_ispu\",\"kategori_ispa\",\"status\"],\n",
    "}\n",
    "\n",
    "# ---------- load all ISPU files ----------\n",
    "ispu_files = sorted(ISPU_DIR.glob(\"*.csv\"))\n",
    "if len(ispu_files) == 0:\n",
    "    raise FileNotFoundError(f\"Tidak menemukan file ISPU di: {ISPU_DIR}\")\n",
    "\n",
    "rows = []\n",
    "file_stats = []\n",
    "\n",
    "for p in ispu_files:\n",
    "    df0 = read_csv_try_enc(p)\n",
    "    df0.columns = [norm_colname(c) for c in df0.columns]\n",
    "    df0[\"__source_file\"] = p.name\n",
    "\n",
    "    # tanggal & stasiun (ambil dari kandidat yang tersedia)\n",
    "    tanggal = coalesce_text(df0, DATE_CANDS)\n",
    "    stasiun = coalesce_text(df0, STA_CANDS)\n",
    "\n",
    "    df1 = pd.DataFrame({\n",
    "        \"tanggal\": pd.to_datetime(tanggal, errors=\"coerce\", dayfirst=True),\n",
    "        \"stasiun\": stasiun,\n",
    "        \"__source_file\": df0[\"__source_file\"].values,\n",
    "    })\n",
    "\n",
    "    # numeric pollutants\n",
    "    for k, cands in COLS_NUM.items():\n",
    "        df1[k] = coalesce_numeric(df0, cands)\n",
    "\n",
    "    # text\n",
    "    for k, cands in COLS_TXT.items():\n",
    "        df1[k] = coalesce_text(df0, cands)\n",
    "\n",
    "    # station code\n",
    "    df1[\"stasiun_code\"] = extract_stasiun_code(df1[\"stasiun\"])\n",
    "\n",
    "    # quick file parse stats\n",
    "    parsed_rate = float(df1[\"tanggal\"].notna().mean())\n",
    "    max_dt = df1[\"tanggal\"].max()\n",
    "    file_stats.append((p.name, len(df1), parsed_rate, max_dt))\n",
    "\n",
    "    rows.append(df1)\n",
    "\n",
    "df_raw = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# ---------- basic validity ----------\n",
    "df_raw = df_raw.loc[df_raw[\"tanggal\"].notna()].copy()\n",
    "df_raw[\"stasiun_code\"] = df_raw[\"stasiun_code\"].astype(\"string\")\n",
    "\n",
    "# ---------- map kategori -> 3 classes ----------\n",
    "cat = df_raw[\"kategori_raw\"].astype(\"string\").str.upper().str.strip()\n",
    "cat = cat.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "cat = cat.replace({\n",
    "    \"SANGAT TIDAK SEHAT\": \"TIDAK SEHAT\",\n",
    "    \"BERBAHAYA\": \"TIDAK SEHAT\",\n",
    "    \"TIDAKSEHAT\": \"TIDAK SEHAT\",\n",
    "    \"TIDAK  SEHAT\": \"TIDAK SEHAT\",\n",
    "})\n",
    "df_raw[\"kategori_3\"] = cat.where(cat.isin([\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\"]), np.nan)\n",
    "\n",
    "# ---------- keep only valid stations (drop NAN / unknown) ----------\n",
    "df_raw_valid = df_raw.loc[df_raw[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "\n",
    "# ---------- deduplicate per (tanggal, stasiun_code) ----------\n",
    "num_cols = list(COLS_NUM.keys())\n",
    "group_keys = [\"tanggal\", \"stasiun_code\"]\n",
    "\n",
    "agg = {c: \"mean\" for c in num_cols}\n",
    "agg.update({\n",
    "    \"kategori_3\": mode_nonnull,\n",
    "    \"stasiun\": mode_nonnull,\n",
    "    \"parameter_pencemar_kritis\": mode_nonnull,\n",
    "    \"__source_file\": mode_nonnull,\n",
    "})\n",
    "\n",
    "df_ispu = (\n",
    "    df_raw_valid.groupby(group_keys, as_index=False)\n",
    "    .agg(agg)\n",
    "    .sort_values(group_keys)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ---------- summaries ----------\n",
    "min_date = df_ispu[\"tanggal\"].min()\n",
    "max_date = df_ispu[\"tanggal\"].max()\n",
    "\n",
    "print(\"=== STAGE 1 SUMMARY (ISPU) ===\")\n",
    "print(f\"Files loaded: {len(ispu_files)}\")\n",
    "print(f\"Rows raw (all stations): {len(df_raw):,}\")\n",
    "print(f\"Rows valid stations (pre-agg): {len(df_raw_valid):,}\")\n",
    "print(f\"Rows daily (after agg): {len(df_ispu):,}\")\n",
    "print(f\"Stations (valid): {df_ispu['stasiun_code'].nunique()} -> {sorted(df_ispu['stasiun_code'].unique().tolist())}\")\n",
    "print(f\"Date range: {min_date.date()} -> {max_date.date()}\")\n",
    "print(f\"CUTOFF_DATE (forecasting POV): {CUTOFF_DATE.date()}\")\n",
    "\n",
    "print(\"\\nLabel counts (kategori_3) on daily table:\")\n",
    "print(df_ispu[\"kategori_3\"].value_counts(dropna=False))\n",
    "\n",
    "miss = df_ispu[num_cols + [\"kategori_3\"]].isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nMissing rate (top):\")\n",
    "print(miss.head(15))\n",
    "\n",
    "per_sta = df_ispu.groupby(\"stasiun_code\").agg(\n",
    "    n_days=(\"tanggal\", \"count\"),\n",
    "    min_date=(\"tanggal\", \"min\"),\n",
    "    max_date=(\"tanggal\", \"max\"),\n",
    "    label_na_rate=(\"kategori_3\", lambda s: float(s.isna().mean()))\n",
    ").sort_values(\"n_days\", ascending=False)\n",
    "print(\"\\nPer-station coverage:\")\n",
    "print(per_sta)\n",
    "\n",
    "# per-file date parse diagnostics (untuk cek kenapa 2024/2025 bisa gagal)\n",
    "df_file_stats = pd.DataFrame(file_stats, columns=[\"file\",\"n_rows\",\"date_parse_rate\",\"max_date_parsed\"])\n",
    "df_file_stats = df_file_stats.sort_values([\"date_parse_rate\",\"max_date_parsed\"], ascending=[True, True])\n",
    "print(\"\\nPer-file date parsing diagnostics (worst -> best):\")\n",
    "print(df_file_stats.head(10))\n",
    "\n",
    "# handy globals for next stages\n",
    "ISPU_MIN_DATE = min_date\n",
    "ISPU_MAX_DATE = max_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3b22e",
   "metadata": {
    "papermill": {
     "duration": 0.003698,
     "end_time": "2026-02-03T02:56:35.737851",
     "exception": false,
     "start_time": "2026-02-03T02:56:35.734153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Master Table Building (Correct Joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1e187f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T02:56:35.747708Z",
     "iopub.status.busy": "2026-02-03T02:56:35.747273Z",
     "iopub.status.idle": "2026-02-03T02:56:36.321283Z",
     "shell.execute_reply": "2026-02-03T02:56:36.319497Z"
    },
    "papermill": {
     "duration": 0.582841,
     "end_time": "2026-02-03T02:56:36.324349",
     "exception": false,
     "start_time": "2026-02-03T02:56:35.741508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/3166822607.py:128: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  df_ndvi[\"tanggal\"] = pd.to_datetime(df_ndvi[\"tanggal\"], errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 2 SUMMARY ===\n",
      "Submission rows: 180\n",
      "Target date range: 2025-01-09 -> 2025-12-11\n",
      "Stations in submission: ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "\n",
      "Last observed date per station (from ISPU hist):\n",
      "stasiun_code\n",
      "DKI1   2023-11-30\n",
      "DKI2   2023-11-30\n",
      "DKI3   2023-11-30\n",
      "DKI4   2023-11-30\n",
      "DKI5   2023-11-30\n",
      "Name: tanggal, dtype: datetime64[ns]\n",
      "\n",
      "Horizon_days summary:\n",
      "count    180.000000\n",
      "mean     573.500000\n",
      "std      105.308406\n",
      "min      406.000000\n",
      "25%      488.750000\n",
      "50%      573.000000\n",
      "75%      658.000000\n",
      "max      742.000000\n",
      "Name: horizon_days, dtype: float64\n",
      "\n",
      "df_targets preview:\n",
      "                id tanggal_target stasiun_code last_obs_date  horizon_days  \\\n",
      "0  2025-09-01_DKI1     2025-01-09         DKI1    2023-11-30           406   \n",
      "1  2025-09-01_DKI2     2025-01-09         DKI2    2023-11-30           406   \n",
      "2  2025-09-01_DKI3     2025-01-09         DKI3    2023-11-30           406   \n",
      "3  2025-09-01_DKI4     2025-01-09         DKI4    2023-11-30           406   \n",
      "4  2025-09-01_DKI5     2025-01-09         DKI5    2023-11-30           406   \n",
      "5  2025-09-02_DKI1     2025-02-09         DKI1    2023-11-30           437   \n",
      "6  2025-09-02_DKI2     2025-02-09         DKI2    2023-11-30           437   \n",
      "7  2025-09-02_DKI3     2025-02-09         DKI3    2023-11-30           437   \n",
      "8  2025-09-02_DKI4     2025-02-09         DKI4    2023-11-30           437   \n",
      "9  2025-09-02_DKI5     2025-02-09         DKI5    2023-11-30           437   \n",
      "\n",
      "   is_weekend  is_holiday_nasional  \n",
      "0           0                    0  \n",
      "1           0                    0  \n",
      "2           0                    0  \n",
      "3           0                    0  \n",
      "4           0                    0  \n",
      "5           0                    0  \n",
      "6           0                    0  \n",
      "7           0                    0  \n",
      "8           0                    0  \n",
      "9           0                    0  \n",
      "\n",
      "ISPU hist max date: 2023-11-30 | gap to CUTOFF_DATE (days): 641\n",
      "\n",
      "Aux tables shapes:\n",
      "NDVI: (1810, 4) | Weather: (11280, 25) | Water monthly: (4, 5) | Pop yearly: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Master Table Building (Correct Joins) — ONE CELL\n",
    "# Tujuan:\n",
    "# - Parse sample_submission -> (id, tanggal_target, stasiun_code)\n",
    "# - Build df_targets (forecast frame) + join kalender & libur (allowed)\n",
    "# - Compute last_obs_date per stasiun from df_ispu + horizon_days\n",
    "# - Load aux tables (NDVI, cuaca, air sungai, penduduk) clean minimal (dipakai stage berikutnya)\n",
    "# Output utama:\n",
    "# - df_sub, df_calendar, df_targets\n",
    "# - df_ispu_hist, last_obs_by_station\n",
    "# - df_ndvi, df_weather, df_water_m, df_pop_y\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- guards ----------\n",
    "need = [\"df_ispu\", \"DATA_ROOT\", \"CUTOFF_DATE\", \"VALID_STATIONS\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from STAGE 1: {miss}\")\n",
    "\n",
    "SUB_PATH   = DATA_ROOT / \"sample_submission.csv\"\n",
    "HOL_PATH   = DATA_ROOT / \"libur-nasional\" / \"dataset-libur-nasional-dan-weekend.csv\"\n",
    "NDVI_PATH  = DATA_ROOT / \"NDVI (vegetation index)\" / \"indeks-ndvi-jakarta.csv\"\n",
    "WATER_PATH = DATA_ROOT / \"kualitas-air-sungai\" / \"data-kualitas-air-sungai-komponen-data.csv\"\n",
    "POP_PATH   = DATA_ROOT / \"jumlah-penduduk\" / \"data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\"\n",
    "WEATHER_DIR = DATA_ROOT / \"cuaca-harian\"\n",
    "\n",
    "def norm_colname(c: str) -> str:\n",
    "    c = str(c).strip().lower()\n",
    "    c = re.sub(r\"[()\\[\\]{}]\", \"\", c)\n",
    "    c = re.sub(r\"[%/]\", \"_\", c)\n",
    "    c = re.sub(r\"[^a-z0-9]+\", \"_\", c)\n",
    "    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "    return c\n",
    "\n",
    "def extract_station(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(str).str.upper()\n",
    "    code = x.str.extract(r\"(DKI\\s*\\d+)\", expand=False)\n",
    "    return code.str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "def extract_date(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(str)\n",
    "    tok = x.str.extract(r\"(\\d{4}[-/]\\d{2}[-/]\\d{2}|\\d{2}[-/]\\d{2}[-/]\\d{4})\", expand=False)\n",
    "    return pd.to_datetime(tok, errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# ============================================================\n",
    "# 2.1 Parse sample_submission -> df_sub\n",
    "# ============================================================\n",
    "df_sub0 = pd.read_csv(SUB_PATH)\n",
    "df_sub0.columns = [norm_colname(c) for c in df_sub0.columns]\n",
    "id_col = \"id\" if \"id\" in df_sub0.columns else df_sub0.columns[0]\n",
    "\n",
    "df_sub = pd.DataFrame({\n",
    "    \"id\": df_sub0[id_col].astype(str),\n",
    "})\n",
    "df_sub[\"tanggal_target\"] = extract_date(df_sub[\"id\"])\n",
    "df_sub[\"stasiun_code\"] = extract_station(df_sub[\"id\"])\n",
    "\n",
    "df_sub = df_sub.loc[df_sub[\"tanggal_target\"].notna() & df_sub[\"stasiun_code\"].notna()].copy()\n",
    "df_sub = df_sub.loc[df_sub[\"stasiun_code\"].isin(VALID_STATIONS)].reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# 2.2 ISPU history (untuk last_obs_date) — batasi sebelum CUTOFF_DATE\n",
    "# ============================================================\n",
    "df_ispu_hist = df_ispu.loc[\n",
    "    df_ispu[\"stasiun_code\"].isin(VALID_STATIONS) &\n",
    "    (df_ispu[\"tanggal\"] < CUTOFF_DATE)\n",
    "].copy()\n",
    "\n",
    "last_obs_by_station = df_ispu_hist.groupby(\"stasiun_code\")[\"tanggal\"].max()\n",
    "\n",
    "# ============================================================\n",
    "# 2.3 Calendar table + holiday (allowed future features)\n",
    "# ============================================================\n",
    "cal_min = min(df_ispu_hist[\"tanggal\"].min(), df_sub[\"tanggal_target\"].min())\n",
    "cal_max = max(df_ispu_hist[\"tanggal\"].max(), df_sub[\"tanggal_target\"].max())\n",
    "\n",
    "df_calendar = pd.DataFrame({\"tanggal\": pd.date_range(cal_min, cal_max, freq=\"D\")})\n",
    "df_calendar[\"year\"] = df_calendar[\"tanggal\"].dt.year\n",
    "df_calendar[\"month\"] = df_calendar[\"tanggal\"].dt.month\n",
    "df_calendar[\"day\"] = df_calendar[\"tanggal\"].dt.day\n",
    "df_calendar[\"dow\"] = df_calendar[\"tanggal\"].dt.dayofweek\n",
    "df_calendar[\"dayofyear\"] = df_calendar[\"tanggal\"].dt.dayofyear\n",
    "\n",
    "df_calendar[\"sin_doy\"] = np.sin(2*np.pi*df_calendar[\"dayofyear\"]/365.25)\n",
    "df_calendar[\"cos_doy\"] = np.cos(2*np.pi*df_calendar[\"dayofyear\"]/365.25)\n",
    "df_calendar[\"sin_month\"] = np.sin(2*np.pi*df_calendar[\"month\"]/12.0)\n",
    "df_calendar[\"cos_month\"] = np.cos(2*np.pi*df_calendar[\"month\"]/12.0)\n",
    "\n",
    "df_hol = pd.read_csv(HOL_PATH)\n",
    "df_hol.columns = [norm_colname(c) for c in df_hol.columns]\n",
    "df_hol[\"tanggal\"] = pd.to_datetime(df_hol[\"tanggal\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "keep_h = [c for c in [\"tanggal\",\"is_holiday_nasional\",\"nama_libur\",\"is_weekend\",\"day_name\"] if c in df_hol.columns]\n",
    "df_hol = df_hol[keep_h].dropna(subset=[\"tanggal\"]).drop_duplicates(\"tanggal\")\n",
    "\n",
    "df_calendar = df_calendar.merge(df_hol, on=\"tanggal\", how=\"left\")\n",
    "\n",
    "# fallback weekend/holiday values\n",
    "df_calendar[\"is_weekend\"] = df_calendar[\"is_weekend\"].fillna((df_calendar[\"dow\"] >= 5).astype(int)).astype(int)\n",
    "df_calendar[\"is_holiday_nasional\"] = df_calendar[\"is_holiday_nasional\"].fillna(0).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 2.4 df_targets = submission index + kalender/libur + last_obs/horizon\n",
    "# ============================================================\n",
    "df_targets = df_sub.merge(\n",
    "    df_calendar,\n",
    "    left_on=\"tanggal_target\",\n",
    "    right_on=\"tanggal\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"tanggal\"])\n",
    "\n",
    "df_targets[\"last_obs_date\"] = df_targets[\"stasiun_code\"].map(last_obs_by_station)\n",
    "df_targets[\"horizon_days\"] = (df_targets[\"tanggal_target\"] - df_targets[\"last_obs_date\"]).dt.days\n",
    "\n",
    "# ============================================================\n",
    "# 2.5 Load auxiliary tables (clean minimal; dipakai stage berikutnya)\n",
    "# ============================================================\n",
    "\n",
    "# NDVI\n",
    "df_ndvi = pd.read_csv(NDVI_PATH)\n",
    "df_ndvi.columns = [norm_colname(c) for c in df_ndvi.columns]\n",
    "if \"tanggal\" in df_ndvi.columns:\n",
    "    df_ndvi[\"tanggal\"] = pd.to_datetime(df_ndvi[\"tanggal\"], errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "# map station id -> stasiun_code\n",
    "if \"stasiun_code\" not in df_ndvi.columns:\n",
    "    sid = df_ndvi[\"stasiun_id\"] if \"stasiun_id\" in df_ndvi.columns else (df_ndvi[\"stasiun\"] if \"stasiun\" in df_ndvi.columns else pd.Series(\"\", index=df_ndvi.index))\n",
    "    df_ndvi[\"stasiun_code\"] = sid.astype(str).str.upper().str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "df_ndvi = df_ndvi.loc[df_ndvi[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "\n",
    "# Weather (5 file DKI1..DKI5)\n",
    "w_files = sorted(WEATHER_DIR.glob(\"cuaca-harian-*.csv\"))\n",
    "w_list = []\n",
    "for p in w_files:\n",
    "    m = re.search(r\"dki(\\d+)\", p.name.lower())\n",
    "    st_code = f\"DKI{m.group(1)}\" if m else None\n",
    "    if st_code not in VALID_STATIONS:\n",
    "        continue\n",
    "    d = pd.read_csv(p)\n",
    "    d.columns = [norm_colname(c) for c in d.columns]\n",
    "    tcol = [c for c in [\"time\",\"tanggal\",\"date\",\"waktu\"] if c in d.columns][0]\n",
    "    d[\"tanggal\"] = pd.to_datetime(d[tcol], errors=\"coerce\", dayfirst=True)\n",
    "    d[\"stasiun_code\"] = st_code\n",
    "    d = d.dropna(subset=[\"tanggal\"]).drop(columns=[tcol], errors=\"ignore\")\n",
    "    w_list.append(d)\n",
    "\n",
    "df_weather = pd.concat(w_list, ignore_index=True) if len(w_list) else pd.DataFrame(columns=[\"tanggal\",\"stasiun_code\"])\n",
    "\n",
    "# Water quality -> aggregate monthly\n",
    "df_water = pd.read_csv(WATER_PATH)\n",
    "df_water.columns = [norm_colname(c) for c in df_water.columns]\n",
    "\n",
    "for c in [\"hasil_pengukuran\",\"baku_mutu\"]:\n",
    "    if c in df_water.columns:\n",
    "        df_water[c] = pd.to_numeric(df_water[c], errors=\"coerce\")\n",
    "\n",
    "# derive year/month\n",
    "if \"periode_data\" in df_water.columns:\n",
    "    df_water[\"tahun\"] = pd.to_numeric(df_water[\"periode_data\"], errors=\"coerce\")\n",
    "else:\n",
    "    df_water[\"tahun\"] = np.nan\n",
    "if \"bulan_sampling\" in df_water.columns:\n",
    "    df_water[\"bulan\"] = pd.to_numeric(df_water[\"bulan_sampling\"], errors=\"coerce\")\n",
    "else:\n",
    "    df_water[\"bulan\"] = np.nan\n",
    "\n",
    "df_water[\"exceed_ratio\"] = df_water[\"hasil_pengukuran\"] / df_water[\"baku_mutu\"]\n",
    "\n",
    "df_water_m = (\n",
    "    df_water.dropna(subset=[\"tahun\",\"bulan\"])\n",
    "           .groupby([\"tahun\",\"bulan\"], as_index=False)\n",
    "           .agg(\n",
    "               water_exceed_mean=(\"exceed_ratio\",\"mean\"),\n",
    "               water_exceed_rate=(\"exceed_ratio\", lambda s: float((s > 1).mean())),\n",
    "               water_n=(\"exceed_ratio\",\"count\")\n",
    "           )\n",
    ")\n",
    "\n",
    "# Population -> yearly total + yoy\n",
    "df_pop = pd.read_csv(POP_PATH)\n",
    "df_pop.columns = [norm_colname(c) for c in df_pop.columns]\n",
    "year_col = \"tahun\" if \"tahun\" in df_pop.columns else (\"periode_data\" if \"periode_data\" in df_pop.columns else None)\n",
    "df_pop[\"tahun\"] = pd.to_numeric(df_pop[year_col], errors=\"coerce\") if year_col else np.nan\n",
    "df_pop[\"jumlah_penduduk\"] = pd.to_numeric(df_pop[\"jumlah_penduduk\"], errors=\"coerce\") if \"jumlah_penduduk\" in df_pop.columns else np.nan\n",
    "\n",
    "df_pop_y = (\n",
    "    df_pop.dropna(subset=[\"tahun\"])\n",
    "          .groupby(\"tahun\", as_index=False)\n",
    "          .agg(pop_total=(\"jumlah_penduduk\",\"sum\"))\n",
    "          .sort_values(\"tahun\")\n",
    ")\n",
    "df_pop_y[\"pop_yoy\"] = df_pop_y[\"pop_total\"].pct_change()\n",
    "\n",
    "# ============================================================\n",
    "# 2.6 QA prints\n",
    "# ============================================================\n",
    "print(\"=== STAGE 2 SUMMARY ===\")\n",
    "print(\"Submission rows:\", len(df_sub))\n",
    "print(\"Target date range:\", df_sub[\"tanggal_target\"].min().date(), \"->\", df_sub[\"tanggal_target\"].max().date())\n",
    "print(\"Stations in submission:\", sorted(df_sub[\"stasiun_code\"].unique().tolist()))\n",
    "\n",
    "print(\"\\nLast observed date per station (from ISPU hist):\")\n",
    "print(last_obs_by_station)\n",
    "\n",
    "print(\"\\nHorizon_days summary:\")\n",
    "print(df_targets[\"horizon_days\"].describe())\n",
    "\n",
    "print(\"\\ndf_targets preview:\")\n",
    "print(df_targets[[\"id\",\"tanggal_target\",\"stasiun_code\",\"last_obs_date\",\"horizon_days\",\"is_weekend\",\"is_holiday_nasional\"]].head(10))\n",
    "\n",
    "# warning if ISPU history ends far before cutoff (affects forecasting difficulty)\n",
    "hist_max = df_ispu_hist[\"tanggal\"].max()\n",
    "gap_days = int((CUTOFF_DATE - hist_max).days) if pd.notna(hist_max) else None\n",
    "print(\"\\nISPU hist max date:\", hist_max.date() if pd.notna(hist_max) else hist_max, \"| gap to CUTOFF_DATE (days):\", gap_days)\n",
    "\n",
    "print(\"\\nAux tables shapes:\")\n",
    "print(\"NDVI:\", df_ndvi.shape, \"| Weather:\", df_weather.shape, \"| Water monthly:\", df_water_m.shape, \"| Pop yearly:\", df_pop_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b2816",
   "metadata": {
    "papermill": {
     "duration": 0.005312,
     "end_time": "2026-02-03T02:56:36.334104",
     "exception": false,
     "start_time": "2026-02-03T02:56:36.328792",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering (Time-Series + Calendar + Robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec350191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T02:56:36.346468Z",
     "iopub.status.busy": "2026-02-03T02:56:36.346053Z",
     "iopub.status.idle": "2026-02-03T02:56:36.583107Z",
     "shell.execute_reply": "2026-02-03T02:56:36.581688Z"
    },
    "papermill": {
     "duration": 0.246129,
     "end_time": "2026-02-03T02:56:36.585374",
     "exception": false,
     "start_time": "2026-02-03T02:56:36.339245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 3 SUMMARY ===\n",
      "HIST_END: 2023-11-30\n",
      "POLL_COLS: ['pm_sepuluh', 'pm_duakomalima', 'sulfur_dioksida', 'karbon_monoksida', 'ozon', 'nitrogen_dioksida', 'max']\n",
      "Target feature rows: 180\n",
      "Feature cols: 88 | cat: 3 | num: 85\n",
      "\n",
      "Missing rate (selected features):\n",
      "horizon_days           0.0\n",
      "is_weekend             0.0\n",
      "is_holiday_nasional    0.0\n",
      "max_lag1               0.0\n",
      "max_rmean7             0.0\n",
      "max_rmean30            0.0\n",
      "max_mon_mean           0.0\n",
      "dtype: float64\n",
      "\n",
      "Targets feature preview:\n",
      "                id tanggal_target stasiun_code  horizon_days  is_weekend  \\\n",
      "0  2025-09-01_DKI1     2025-01-09         DKI1           406           0   \n",
      "1  2025-09-01_DKI2     2025-01-09         DKI2           406           0   \n",
      "2  2025-09-01_DKI3     2025-01-09         DKI3           406           0   \n",
      "3  2025-09-01_DKI4     2025-01-09         DKI4           406           0   \n",
      "4  2025-09-01_DKI5     2025-01-09         DKI5           406           0   \n",
      "5  2025-09-02_DKI1     2025-02-09         DKI1           437           0   \n",
      "6  2025-09-02_DKI2     2025-02-09         DKI2           437           0   \n",
      "7  2025-09-02_DKI3     2025-02-09         DKI3           437           0   \n",
      "8  2025-09-02_DKI4     2025-02-09         DKI4           437           0   \n",
      "9  2025-09-02_DKI5     2025-02-09         DKI5           437           0   \n",
      "\n",
      "   is_holiday_nasional  max_lag1  max_rmean7  max_rmean30  max_mon_mean  \\\n",
      "0                    0      82.0   86.571429    84.300000     47.831081   \n",
      "1                    0      78.0   77.428571    81.800000     47.856164   \n",
      "2                    0      56.0   76.142857    70.000000     45.837931   \n",
      "3                    0     105.0   72.571429    84.633333     43.713793   \n",
      "4                    0      88.0   88.714286    84.500000     36.876712   \n",
      "5                    0      82.0   86.571429    84.300000     50.345588   \n",
      "6                    0      78.0   77.428571    81.800000     41.873134   \n",
      "7                    0      56.0   76.142857    70.000000     47.035156   \n",
      "8                    0     105.0   72.571429    84.633333     46.347015   \n",
      "9                    0      88.0   88.714286    84.500000     39.134328   \n",
      "\n",
      "   p_tidak_sehat_mon  ndvi_last  ndvi_mon_mean  \n",
      "0           0.000000     0.3390       0.176475  \n",
      "1           0.000000     0.1959       0.176821  \n",
      "2           0.000000     0.4449       0.393357  \n",
      "3           0.066667     0.4927       0.363482  \n",
      "4           0.000000     0.1970       0.295121  \n",
      "5           0.000000     0.3390       0.176568  \n",
      "6           0.000000     0.1959       0.195657  \n",
      "7           0.000000     0.4449       0.426793  \n",
      "8           0.000000     0.4927       0.354839  \n",
      "9           0.000000     0.1970       0.287054  \n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — Feature Engineering (Time-Series + Calendar + Forecasting-safe) — ONE CELL\n",
    "# Prinsip:\n",
    "# - Test tidak punya fitur polutan masa depan -> fitur target dibangun dari:\n",
    "#   (A) \"state histori terakhir\" per stasiun (rolling/trend/missingness dari df_ispu_hist)\n",
    "#   (B) kalender/libur pada tanggal_target (sudah ada di df_targets dari Stage 2)\n",
    "#   (C) horizon_days (+ transform)\n",
    "#   (D) opsional: baseline musiman berbasis histori (station x month) untuk polutan/NDVI/weather (global, semua <= hist_end)\n",
    "#\n",
    "# Output utama:\n",
    "# - df_targets_feat : df_targets + fitur state/seasonal (siap untuk inference / jadi template training)\n",
    "# - FEATURE_COLS_TARGET : daftar fitur numerik/kategori yang dipakai (untuk stage 4/5)\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- guards ----------\n",
    "need = [\"df_ispu_hist\", \"df_targets\", \"VALID_STATIONS\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from previous stages: {miss}\")\n",
    "\n",
    "# ============\n",
    "# 3.1 Setup\n",
    "# ============\n",
    "POLL_COLS = [c for c in [\"pm_sepuluh\",\"pm_duakomalima\",\"sulfur_dioksida\",\"karbon_monoksida\",\"ozon\",\"nitrogen_dioksida\",\"max\"] if c in df_ispu_hist.columns]\n",
    "HIST_END = df_ispu_hist[\"tanggal\"].max()  # histori terakhir yang tersedia (sekarang 2023-11-30)\n",
    "\n",
    "# pastikan urut\n",
    "dfh = df_ispu_hist.sort_values([\"stasiun_code\",\"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "# ============\n",
    "# 3.2 Build rolling/trend features on history (past-only, per stasiun)\n",
    "# ============\n",
    "for c in POLL_COLS:\n",
    "    s = dfh.groupby(\"stasiun_code\")[c]\n",
    "    dfh[f\"{c}_lag1\"] = s.shift(1)\n",
    "    dfh[f\"{c}_lag7\"] = s.shift(7)\n",
    "\n",
    "    # rolling pakai jumlah baris (karena data umumnya harian)\n",
    "    dfh[f\"{c}_rmean7\"]  = s.shift(1).rolling(7).mean().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rstd7\"]   = s.shift(1).rolling(7).std().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rmean30\"] = s.shift(1).rolling(30).mean().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rstd30\"]  = s.shift(1).rolling(30).std().reset_index(level=0, drop=True)\n",
    "\n",
    "    # trend pendek vs panjang\n",
    "    dfh[f\"{c}_trend_7_30\"] = dfh[f\"{c}_rmean7\"] - dfh[f\"{c}_rmean30\"]\n",
    "\n",
    "    # missingness rate (rolling 30 obs)\n",
    "    na_s = s.shift(1).isna().astype(float)\n",
    "    dfh[f\"{c}_na_rate30\"] = na_s.rolling(30).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# ambil snapshot terakhir per stasiun (state pada HIST_END)\n",
    "df_last = (\n",
    "    dfh.loc[dfh[\"tanggal\"].eq(HIST_END)]\n",
    "      .groupby(\"stasiun_code\", as_index=False)\n",
    "      .tail(1)\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "keep_last = [\"stasiun_code\"] + [c for c in df_last.columns if any(c.startswith(pc) for pc in POLL_COLS)]\n",
    "df_station_state = df_last[keep_last].copy()\n",
    "\n",
    "# ============\n",
    "# 3.3 Baseline musiman (global) station x month untuk polutan & label prior\n",
    "#     (dipakai untuk target date; semuanya dibangun dari histori <= HIST_END)\n",
    "# ============\n",
    "dfh2 = dfh.copy()\n",
    "dfh2[\"month\"] = dfh2[\"tanggal\"].dt.month\n",
    "\n",
    "# polutan baseline per (stasiun, month)\n",
    "agg_pol = {c: \"mean\" for c in POLL_COLS}\n",
    "df_pol_month = (\n",
    "    dfh2.groupby([\"stasiun_code\",\"month\"], as_index=False)\n",
    "        .agg(agg_pol)\n",
    ")\n",
    "df_pol_month = df_pol_month.rename(columns={c: f\"{c}_mon_mean\" for c in POLL_COLS})\n",
    "\n",
    "# label prior per (stasiun, month) dari baris yang ada label\n",
    "df_lbl = dfh2.loc[dfh2[\"kategori_3\"].notna(), [\"stasiun_code\",\"month\",\"kategori_3\"]].copy()\n",
    "if len(df_lbl) > 0:\n",
    "    tmp = pd.crosstab([df_lbl[\"stasiun_code\"], df_lbl[\"month\"]], df_lbl[\"kategori_3\"], normalize=\"index\").reset_index()\n",
    "    # pastikan kolom ada semua\n",
    "    for k in [\"BAIK\",\"SEDANG\",\"TIDAK SEHAT\"]:\n",
    "        if k not in tmp.columns:\n",
    "            tmp[k] = 0.0\n",
    "    df_lbl_month = tmp.rename(columns={\n",
    "        \"BAIK\": \"p_baik_mon\",\n",
    "        \"SEDANG\": \"p_sedang_mon\",\n",
    "        \"TIDAK SEHAT\": \"p_tidak_sehat_mon\",\n",
    "    })\n",
    "else:\n",
    "    df_lbl_month = pd.DataFrame(columns=[\"stasiun_code\",\"month\",\"p_baik_mon\",\"p_sedang_mon\",\"p_tidak_sehat_mon\"])\n",
    "\n",
    "# ============\n",
    "# 3.4 NDVI features (last-known + monthly baseline) — hanya pakai <= HIST_END\n",
    "# ============\n",
    "df_ndvi_use = globals().get(\"df_ndvi\", pd.DataFrame()).copy()\n",
    "if len(df_ndvi_use) > 0 and {\"tanggal\",\"stasiun_code\"}.issubset(df_ndvi_use.columns):\n",
    "    df_ndvi_use = df_ndvi_use.loc[df_ndvi_use[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "    df_ndvi_use[\"tanggal\"] = pd.to_datetime(df_ndvi_use[\"tanggal\"], errors=\"coerce\", dayfirst=True)\n",
    "    df_ndvi_use = df_ndvi_use.dropna(subset=[\"tanggal\"])\n",
    "    df_ndvi_use = df_ndvi_use.loc[df_ndvi_use[\"tanggal\"] <= HIST_END].copy()\n",
    "\n",
    "    ndvi_col = \"ndvi\" if \"ndvi\" in df_ndvi_use.columns else None\n",
    "    if ndvi_col is not None:\n",
    "        df_ndvi_use[ndvi_col] = pd.to_numeric(df_ndvi_use[ndvi_col], errors=\"coerce\")\n",
    "\n",
    "        df_ndvi_use[\"month\"] = df_ndvi_use[\"tanggal\"].dt.month\n",
    "        df_ndvi_last = (\n",
    "            df_ndvi_use.sort_values([\"stasiun_code\",\"tanggal\"])\n",
    "                       .groupby(\"stasiun_code\", as_index=False)\n",
    "                       .tail(1)[[\"stasiun_code\", ndvi_col]]\n",
    "                       .rename(columns={ndvi_col: \"ndvi_last\"})\n",
    "        )\n",
    "        df_ndvi_month = (\n",
    "            df_ndvi_use.groupby([\"stasiun_code\",\"month\"], as_index=False)[ndvi_col]\n",
    "                       .mean()\n",
    "                       .rename(columns={ndvi_col: \"ndvi_mon_mean\"})\n",
    "        )\n",
    "    else:\n",
    "        df_ndvi_last = pd.DataFrame(columns=[\"stasiun_code\",\"ndvi_last\"])\n",
    "        df_ndvi_month = pd.DataFrame(columns=[\"stasiun_code\",\"month\",\"ndvi_mon_mean\"])\n",
    "else:\n",
    "    df_ndvi_last = pd.DataFrame(columns=[\"stasiun_code\",\"ndvi_last\"])\n",
    "    df_ndvi_month = pd.DataFrame(columns=[\"stasiun_code\",\"month\",\"ndvi_mon_mean\"])\n",
    "\n",
    "# ============\n",
    "# 3.5 Weather monthly baseline (global) — hanya pakai <= HIST_END\n",
    "#     (bukan \"actual future weather\"; hanya pola historis)\n",
    "# ============\n",
    "df_w = globals().get(\"df_weather\", pd.DataFrame()).copy()\n",
    "if len(df_w) > 0 and {\"tanggal\",\"stasiun_code\"}.issubset(df_w.columns):\n",
    "    df_w = df_w.loc[df_w[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "    df_w[\"tanggal\"] = pd.to_datetime(df_w[\"tanggal\"], errors=\"coerce\", dayfirst=True)\n",
    "    df_w = df_w.dropna(subset=[\"tanggal\"])\n",
    "    df_w = df_w.loc[df_w[\"tanggal\"] <= HIST_END].copy()\n",
    "    df_w[\"month\"] = df_w[\"tanggal\"].dt.month\n",
    "\n",
    "    # pilih subset kolom cuaca yang umum; ambil yang ada saja\n",
    "    w_candidates = [\n",
    "        \"temperature_2m_mean\", \"temperature_2m_max\", \"temperature_2m_min\",\n",
    "        \"precipitation_sum\", \"precipitation_hours\",\n",
    "        \"wind_speed_10m_mean\", \"wind_speed_10m_max\", \"wind_speed_10m_min\",\n",
    "        \"relative_humidity_2m_mean\",\n",
    "        \"surface_pressure_mean\",\n",
    "        \"cloud_cover_mean\",\n",
    "        \"shortwave_radiation_sum\",\n",
    "        \"wind_gusts_10m_mean\", \"wind_gusts_10m_max\"\n",
    "    ]\n",
    "    WCOLS = [c for c in w_candidates if c in df_w.columns]\n",
    "    for c in WCOLS:\n",
    "        df_w[c] = pd.to_numeric(df_w[c], errors=\"coerce\")\n",
    "\n",
    "    if len(WCOLS) > 0:\n",
    "        df_w_month = (\n",
    "            df_w.groupby([\"stasiun_code\",\"month\"], as_index=False)[WCOLS]\n",
    "                .mean()\n",
    "        )\n",
    "        df_w_month = df_w_month.rename(columns={c: f\"{c}_mon_mean\" for c in WCOLS})\n",
    "    else:\n",
    "        df_w_month = pd.DataFrame(columns=[\"stasiun_code\",\"month\"])\n",
    "else:\n",
    "    df_w_month = pd.DataFrame(columns=[\"stasiun_code\",\"month\"])\n",
    "\n",
    "# ============\n",
    "# 3.6 Merge all features into target frame\n",
    "# ============\n",
    "df_targets_feat = df_targets.copy()\n",
    "\n",
    "# horizon transforms\n",
    "df_targets_feat[\"horizon_weeks\"]  = df_targets_feat[\"horizon_days\"] / 7.0\n",
    "df_targets_feat[\"horizon_months\"] = df_targets_feat[\"horizon_days\"] / 30.0\n",
    "df_targets_feat[\"log1p_horizon\"]  = np.log1p(df_targets_feat[\"horizon_days\"].clip(lower=0))\n",
    "\n",
    "# merge station last state\n",
    "df_targets_feat = df_targets_feat.merge(df_station_state, on=\"stasiun_code\", how=\"left\")\n",
    "\n",
    "# merge month baselines (polutan + label priors)\n",
    "df_targets_feat = df_targets_feat.merge(df_pol_month, on=[\"stasiun_code\",\"month\"], how=\"left\")\n",
    "df_targets_feat = df_targets_feat.merge(df_lbl_month, on=[\"stasiun_code\",\"month\"], how=\"left\")\n",
    "\n",
    "# merge NDVI\n",
    "df_targets_feat = df_targets_feat.merge(df_ndvi_last, on=\"stasiun_code\", how=\"left\")\n",
    "df_targets_feat = df_targets_feat.merge(df_ndvi_month, on=[\"stasiun_code\",\"month\"], how=\"left\")\n",
    "\n",
    "# merge weather monthly\n",
    "df_targets_feat = df_targets_feat.merge(df_w_month, on=[\"stasiun_code\",\"month\"], how=\"left\")\n",
    "\n",
    "# ============\n",
    "# 3.7 Define feature columns (untuk stage 4/5)\n",
    "# ============\n",
    "# kategori features: stasiun_code, day_name (jika ada), nama_libur (jika ada)\n",
    "CAT_FEATS = [c for c in [\"stasiun_code\",\"day_name\",\"nama_libur\"] if c in df_targets_feat.columns]\n",
    "\n",
    "# numeric features: kalender + horizon + state + baselines + NDVI/weather\n",
    "BASE_NUM = [c for c in [\n",
    "    \"year\",\"month\",\"day\",\"dow\",\"dayofyear\",\"sin_doy\",\"cos_doy\",\"sin_month\",\"cos_month\",\n",
    "    \"is_weekend\",\"is_holiday_nasional\",\n",
    "    \"horizon_days\",\"horizon_weeks\",\"horizon_months\",\"log1p_horizon\"\n",
    "] if c in df_targets_feat.columns]\n",
    "\n",
    "STATE_NUM = [c for c in df_targets_feat.columns if any(c.startswith(pc+\"_\") for pc in POLL_COLS)]\n",
    "PRIOR_NUM = [c for c in [\"p_baik_mon\",\"p_sedang_mon\",\"p_tidak_sehat_mon\",\"ndvi_last\",\"ndvi_mon_mean\"] if c in df_targets_feat.columns]\n",
    "WEATHER_NUM = [c for c in df_targets_feat.columns if c.endswith(\"_mon_mean\") and c not in [f\"{p}_mon_mean\" for p in POLL_COLS] and c not in [\"ndvi_mon_mean\"]]\n",
    "POLL_MON_NUM = [c for c in df_targets_feat.columns if c.endswith(\"_mon_mean\") and any(c.startswith(p) for p in POLL_COLS)]\n",
    "\n",
    "NUM_FEATS = BASE_NUM + STATE_NUM + POLL_MON_NUM + PRIOR_NUM + WEATHER_NUM\n",
    "\n",
    "# dedup lists (jaga urutan)\n",
    "def _dedup(seq):\n",
    "    out, seen = [], set()\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            out.append(x); seen.add(x)\n",
    "    return out\n",
    "\n",
    "CAT_FEATS = _dedup(CAT_FEATS)\n",
    "NUM_FEATS = _dedup(NUM_FEATS)\n",
    "\n",
    "FEATURE_COLS_TARGET = CAT_FEATS + NUM_FEATS\n",
    "\n",
    "# ============\n",
    "# 3.8 Prints (QA)\n",
    "# ============\n",
    "print(\"=== STAGE 3 SUMMARY ===\")\n",
    "print(\"HIST_END:\", HIST_END.date())\n",
    "print(\"POLL_COLS:\", POLL_COLS)\n",
    "print(\"Target feature rows:\", len(df_targets_feat))\n",
    "print(\"Feature cols:\", len(FEATURE_COLS_TARGET), \"| cat:\", len(CAT_FEATS), \"| num:\", len(NUM_FEATS))\n",
    "\n",
    "# cek missing ringan untuk fitur utama\n",
    "check_cols = [\"horizon_days\",\"is_weekend\",\"is_holiday_nasional\"] + [c for c in [\"max_lag1\",\"max_rmean7\",\"max_rmean30\",\"max_mon_mean\"] if c in df_targets_feat.columns]\n",
    "miss_rate = df_targets_feat[check_cols].isna().mean().sort_values(ascending=False)\n",
    "print(\"\\nMissing rate (selected features):\")\n",
    "print(miss_rate)\n",
    "\n",
    "print(\"\\nTargets feature preview:\")\n",
    "show_cols = [\"id\",\"tanggal_target\",\"stasiun_code\",\"horizon_days\",\"is_weekend\",\"is_holiday_nasional\"]\n",
    "show_cols += [c for c in [\"max_lag1\",\"max_rmean7\",\"max_rmean30\",\"max_mon_mean\",\"p_tidak_sehat_mon\",\"ndvi_last\",\"ndvi_mon_mean\"] if c in df_targets_feat.columns]\n",
    "print(df_targets_feat[show_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63a42f",
   "metadata": {
    "papermill": {
     "duration": 0.004271,
     "end_time": "2026-02-03T02:56:36.594410",
     "exception": false,
     "start_time": "2026-02-03T02:56:36.590139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training (Time-Based CV + CatBoost Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d10cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T02:56:36.605653Z",
     "iopub.status.busy": "2026-02-03T02:56:36.605200Z",
     "iopub.status.idle": "2026-02-03T02:57:19.897712Z",
     "shell.execute_reply": "2026-02-03T02:57:19.896331Z"
    },
    "papermill": {
     "duration": 43.302403,
     "end_time": "2026-02-03T02:57:19.900834",
     "exception": false,
     "start_time": "2026-02-03T02:56:36.598431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 4 DATA CHECK ===\n",
      "H_TRAIN: [1, 3, 7, 14, 30, 60, 90, 180] ... [573, 730, 742]\n",
      "Supervised rows after merges: 5025\n",
      "Target_date range: 2022-12-01 -> 2023-11-30\n",
      "Unique target years: [2022, 2023]\n",
      "0:\tlearn: 1.0462247\ttest: 1.0894977\tbest: 1.0894977 (0)\ttotal: 101ms\tremaining: 2m 1s\n",
      "Stopped by overfitting detector  (150 iterations wait)\n",
      "\n",
      "bestTest = 1.061064755\n",
      "bestIteration = 2\n",
      "\n",
      "Shrink model to first 3 iterations.\n",
      "0:\tlearn: 1.0504508\ttest: 1.0613777\tbest: 1.0613777 (0)\ttotal: 26.1ms\tremaining: 31.3s\n",
      "200:\tlearn: 0.2620801\ttest: 0.2740005\tbest: 0.2740005 (200)\ttotal: 5.82s\tremaining: 28.9s\n",
      "400:\tlearn: 0.1724186\ttest: 0.1617870\tbest: 0.1617592 (399)\ttotal: 11.6s\tremaining: 23.1s\n",
      "600:\tlearn: 0.1338823\ttest: 0.1214979\tbest: 0.1214979 (600)\ttotal: 17.4s\tremaining: 17.3s\n",
      "800:\tlearn: 0.1097142\ttest: 0.1018321\tbest: 0.1018321 (800)\ttotal: 23.1s\tremaining: 11.5s\n",
      "1000:\tlearn: 0.0924185\ttest: 0.0875848\tbest: 0.0875783 (993)\ttotal: 29s\tremaining: 5.77s\n",
      "1199:\tlearn: 0.0807649\ttest: 0.0778654\tbest: 0.0778315 (1197)\ttotal: 34.7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.07783153824\n",
      "bestIteration = 1197\n",
      "\n",
      "Shrink model to first 1198 iterations.\n",
      "\n",
      "=== STAGE 4 SUMMARY ===\n",
      "Supervised rows used: 5025\n",
      "Features: 74 | cat: 3 | num: 71\n",
      "\n",
      "CV report:\n",
      "           fold  n_train  n_valid  macro_f1\n",
      "0  time_split_1     4037      988  0.205752\n",
      "1  time_split_2     4583      546  0.971836\n",
      "\n",
      "OOF macro F1: 0.4634956723459905\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Model Training (Forecasting-safe + Time-based CV) — ONE CELL (REVISI FULL v3)\n",
    "# Fix:\n",
    "# - merge_asof sorted global by cutoff_date ✅\n",
    "# - sanitize pd.NA -> np.nan (numeric) dan fill \"NA\" (categorical) ✅\n",
    "# - cat_features pakai NAMA kolom (bukan index) ✅\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# ---------- guards ----------\n",
    "need = [\"df_ispu_hist\", \"df_calendar\", \"df_targets\", \"VALID_STATIONS\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from previous stages: {miss}\")\n",
    "\n",
    "SEED = 42\n",
    "LABELS = [\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\"]\n",
    "label_to_id = {k:i for i,k in enumerate(LABELS)}\n",
    "id_to_label = {i:k for k,i in label_to_id.items()}\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def sanitize_features(df: pd.DataFrame, cat_cols, num_cols):\n",
    "    df = df.copy()\n",
    "    # categorical: pd.NA -> \"NA\"\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\").fillna(\"NA\").astype(str)\n",
    "    # numeric: pd.NA -> np.nan, cast to float\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "# ---------- 4.1 choose training horizons ----------\n",
    "q_h = df_targets[\"horizon_days\"].quantile([0.0, 0.5, 1.0]).astype(int).tolist()\n",
    "H_TRAIN = sorted(set([1,3,7,14,30,60,90,180,365,540,730] + q_h))\n",
    "\n",
    "# ---------- 4.2 build past-only state features on history ----------\n",
    "dfh = df_ispu_hist.sort_values([\"stasiun_code\",\"tanggal\"]).reset_index(drop=True)\n",
    "POLL_COLS = [c for c in [\"pm_sepuluh\",\"pm_duakomalima\",\"sulfur_dioksida\",\"karbon_monoksida\",\"ozon\",\"nitrogen_dioksida\",\"max\"] if c in dfh.columns]\n",
    "\n",
    "for c in POLL_COLS:\n",
    "    g = dfh.groupby(\"stasiun_code\")[c]\n",
    "    dfh[f\"{c}_lag1\"] = g.shift(1)\n",
    "    dfh[f\"{c}_lag7\"] = g.shift(7)\n",
    "    dfh[f\"{c}_rmean7\"]  = g.shift(1).rolling(7).mean().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rstd7\"]   = g.shift(1).rolling(7).std().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rmean30\"] = g.shift(1).rolling(30).mean().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rstd30\"]  = g.shift(1).rolling(30).std().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_trend_7_30\"] = dfh[f\"{c}_rmean7\"] - dfh[f\"{c}_rmean30\"]\n",
    "    na_s = g.shift(1).isna().astype(float)\n",
    "    dfh[f\"{c}_na_rate30\"] = na_s.rolling(30).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "state_cols = [c for c in dfh.columns if any(c.startswith(pc+\"_\") for pc in POLL_COLS)]\n",
    "\n",
    "df_base = dfh[[\"stasiun_code\",\"tanggal\"] + state_cols].rename(columns={\"tanggal\":\"cutoff_date\"}).copy()\n",
    "df_base[\"stasiun_code\"] = df_base[\"stasiun_code\"].astype(\"string\")\n",
    "df_base[\"cutoff_date\"] = pd.to_datetime(df_base[\"cutoff_date\"])\n",
    "df_base = df_base.sort_values([\"cutoff_date\",\"stasiun_code\"]).reset_index(drop=True)\n",
    "\n",
    "# ---------- 4.3 label table ----------\n",
    "df_y = dfh[[\"stasiun_code\",\"tanggal\",\"kategori_3\"]].rename(columns={\"tanggal\":\"target_date\"}).copy()\n",
    "df_y = df_y[df_y[\"kategori_3\"].isin(LABELS)].reset_index(drop=True)\n",
    "if len(df_y) == 0:\n",
    "    raise RuntimeError(\"Tidak ada label kategori_3 valid (BAIK/SEDANG/TIDAK SEHAT) di histori.\")\n",
    "\n",
    "# ---------- 4.4 calendar features for target_date ----------\n",
    "cal_keep = [c for c in [\n",
    "    \"tanggal\",\"year\",\"month\",\"day\",\"dow\",\"dayofyear\",\"sin_doy\",\"cos_doy\",\"sin_month\",\"cos_month\",\n",
    "    \"is_weekend\",\"is_holiday_nasional\",\"day_name\",\"nama_libur\"\n",
    "] if c in df_calendar.columns]\n",
    "df_cal = df_calendar[cal_keep].rename(columns={\"tanggal\":\"target_date\"}).copy()\n",
    "df_cal[\"target_date\"] = pd.to_datetime(df_cal[\"target_date\"])\n",
    "\n",
    "# ---------- 4.5 supervised rows for all horizons ----------\n",
    "parts = []\n",
    "for h in H_TRAIN:\n",
    "    d = df_y.copy()\n",
    "    d[\"horizon_days\"] = int(h)\n",
    "    d[\"cutoff_date\"] = d[\"target_date\"] - pd.to_timedelta(int(h), unit=\"D\")\n",
    "    parts.append(d)\n",
    "\n",
    "df_sup = pd.concat(parts, ignore_index=True)\n",
    "df_sup = df_sup[df_sup[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "\n",
    "df_sup[\"stasiun_code\"] = df_sup[\"stasiun_code\"].astype(\"string\")\n",
    "df_sup[\"target_date\"] = pd.to_datetime(df_sup[\"target_date\"])\n",
    "df_sup[\"cutoff_date\"] = pd.to_datetime(df_sup[\"cutoff_date\"])\n",
    "df_sup = df_sup.sort_values([\"cutoff_date\",\"stasiun_code\",\"target_date\",\"horizon_days\"]).reset_index(drop=True)\n",
    "\n",
    "# merge_asof (need global sort by cutoff_date)\n",
    "df_sup = pd.merge_asof(\n",
    "    df_sup, df_base,\n",
    "    on=\"cutoff_date\", by=\"stasiun_code\",\n",
    "    direction=\"backward\",\n",
    "    allow_exact_matches=True,\n",
    "    tolerance=pd.Timedelta(days=60)\n",
    ")\n",
    "df_sup = df_sup.dropna(subset=state_cols).copy()\n",
    "\n",
    "# merge calendar\n",
    "df_sup = df_sup.merge(df_cal, on=\"target_date\", how=\"left\")\n",
    "\n",
    "# horizon transforms\n",
    "df_sup[\"horizon_weeks\"]  = df_sup[\"horizon_days\"] / 7.0\n",
    "df_sup[\"horizon_months\"] = df_sup[\"horizon_days\"] / 30.0\n",
    "df_sup[\"log1p_horizon\"]  = np.log1p(df_sup[\"horizon_days\"].clip(lower=0))\n",
    "\n",
    "# y\n",
    "df_sup[\"y\"] = df_sup[\"kategori_3\"].map(label_to_id).astype(int)\n",
    "\n",
    "# final sort\n",
    "df_sup = df_sup.sort_values([\"target_date\",\"stasiun_code\",\"horizon_days\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== STAGE 4 DATA CHECK ===\")\n",
    "print(\"H_TRAIN:\", H_TRAIN[:8], \"...\", H_TRAIN[-3:])\n",
    "print(\"Supervised rows after merges:\", len(df_sup))\n",
    "print(\"Target_date range:\", df_sup[\"target_date\"].min().date(), \"->\", df_sup[\"target_date\"].max().date())\n",
    "years = sorted(df_sup[\"target_date\"].dt.year.unique().tolist())\n",
    "print(\"Unique target years:\", years)\n",
    "\n",
    "# ---------- 4.6 feature columns ----------\n",
    "CAT_FEATS_MODEL = [c for c in [\"stasiun_code\",\"day_name\",\"nama_libur\"] if c in df_sup.columns]\n",
    "\n",
    "BASE_NUM = [c for c in [\n",
    "    \"year\",\"month\",\"day\",\"dow\",\"dayofyear\",\"sin_doy\",\"cos_doy\",\"sin_month\",\"cos_month\",\n",
    "    \"is_weekend\",\"is_holiday_nasional\",\n",
    "    \"horizon_days\",\"horizon_weeks\",\"horizon_months\",\"log1p_horizon\"\n",
    "] if c in df_sup.columns]\n",
    "NUM_FEATS_MODEL = BASE_NUM + state_cols\n",
    "\n",
    "def _dedup(seq):\n",
    "    out, seen = [], set()\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            out.append(x); seen.add(x)\n",
    "    return out\n",
    "\n",
    "CAT_FEATS_MODEL = _dedup(CAT_FEATS_MODEL)\n",
    "NUM_FEATS_MODEL = _dedup(NUM_FEATS_MODEL)\n",
    "FEATURE_COLS_MODEL = CAT_FEATS_MODEL + NUM_FEATS_MODEL\n",
    "\n",
    "# sanitize entire table once (prevents pd.NA leaking to CatBoost)\n",
    "df_sup = sanitize_features(df_sup, CAT_FEATS_MODEL, NUM_FEATS_MODEL)\n",
    "\n",
    "# ---------- 4.7 folds (time-based, fallback) ----------\n",
    "folds = []\n",
    "if len(years) >= 3:\n",
    "    for vy in years[-3:]:\n",
    "        tr_mask = df_sup[\"target_date\"].dt.year < vy\n",
    "        va_mask = df_sup[\"target_date\"].dt.year == vy\n",
    "        folds.append((f\"val_year_{vy}\", tr_mask, va_mask))\n",
    "else:\n",
    "    dates = df_sup[\"target_date\"].sort_values().unique()\n",
    "    split_a = dates[int(len(dates)*0.8)]\n",
    "    split_b = dates[int(len(dates)*0.9)]\n",
    "    folds = [\n",
    "        (\"time_split_1\", df_sup[\"target_date\"] < split_a, df_sup[\"target_date\"] >= split_a),\n",
    "        (\"time_split_2\", df_sup[\"target_date\"] < split_b, (df_sup[\"target_date\"] >= split_a) & (df_sup[\"target_date\"] < split_b)),\n",
    "    ]\n",
    "\n",
    "# ---------- 4.8 class weights ----------\n",
    "counts = df_sup[\"y\"].value_counts().to_dict()\n",
    "tot = len(df_sup)\n",
    "class_weights = {k: (tot / (len(LABELS) * counts.get(k, 1))) for k in range(len(LABELS))}\n",
    "\n",
    "# ---------- 4.9 train folds + OOF ----------\n",
    "oof_pred_proba = np.full((len(df_sup), len(LABELS)), np.nan, dtype=float)\n",
    "models = {}\n",
    "cv_rows = []\n",
    "\n",
    "for name, tr_mask, va_mask in folds:\n",
    "    ntr = int(tr_mask.sum())\n",
    "    nva = int(va_mask.sum())\n",
    "    if ntr == 0 or nva == 0:\n",
    "        cv_rows.append({\"fold\": name, \"n_train\": ntr, \"n_valid\": nva, \"macro_f1\": np.nan})\n",
    "        continue\n",
    "\n",
    "    X_tr = df_sup.loc[tr_mask, FEATURE_COLS_MODEL]\n",
    "    y_tr = df_sup.loc[tr_mask, \"y\"].values\n",
    "    X_va = df_sup.loc[va_mask, FEATURE_COLS_MODEL]\n",
    "    y_va = df_sup.loc[va_mask, \"y\"].values\n",
    "\n",
    "    tr_pool = Pool(X_tr, label=y_tr, cat_features=CAT_FEATS_MODEL)\n",
    "    va_pool = Pool(X_va, label=y_va, cat_features=CAT_FEATS_MODEL)\n",
    "\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"MultiClass\",\n",
    "        iterations=1200,\n",
    "        learning_rate=0.06,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3.0,\n",
    "        random_seed=SEED,\n",
    "        class_weights=class_weights,\n",
    "        od_type=\"Iter\",\n",
    "        od_wait=150,\n",
    "        verbose=200\n",
    "    )\n",
    "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
    "\n",
    "    proba = model.predict_proba(X_va)\n",
    "    oof_pred_proba[va_mask.values] = proba\n",
    "    pred = proba.argmax(axis=1)\n",
    "    f1m = float(f1_score(y_va, pred, average=\"macro\"))\n",
    "\n",
    "    models[name] = model\n",
    "    cv_rows.append({\"fold\": name, \"n_train\": ntr, \"n_valid\": nva, \"macro_f1\": f1m})\n",
    "\n",
    "cv_report = pd.DataFrame(cv_rows)\n",
    "\n",
    "oof_mask = ~np.isnan(oof_pred_proba).any(axis=1)\n",
    "oof_true = df_sup.loc[oof_mask, \"y\"].values\n",
    "oof_pred = oof_pred_proba[oof_mask].argmax(axis=1)\n",
    "oof_macro_f1 = float(f1_score(oof_true, oof_pred, average=\"macro\")) if len(oof_true) else np.nan\n",
    "\n",
    "print(\"\\n=== STAGE 4 SUMMARY ===\")\n",
    "print(\"Supervised rows used:\", len(df_sup))\n",
    "print(\"Features:\", len(FEATURE_COLS_MODEL), \"| cat:\", len(CAT_FEATS_MODEL), \"| num:\", len(NUM_FEATS_MODEL))\n",
    "print(\"\\nCV report:\")\n",
    "print(cv_report)\n",
    "print(\"\\nOOF macro F1:\", oof_macro_f1)\n",
    "\n",
    "# expose globals for Stage 5\n",
    "df_train_sup = df_sup\n",
    "FEATURE_COLS_MODEL_USED = FEATURE_COLS_MODEL\n",
    "CAT_FEATS_MODEL_USED = CAT_FEATS_MODEL\n",
    "models_by_fold = models\n",
    "oof_true_label = pd.Series(oof_true).map(id_to_label).values if len(oof_true) else np.array([])\n",
    "oof_pred_label = pd.Series(oof_pred).map(id_to_label).values if len(oof_true) else np.array([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074e399",
   "metadata": {
    "papermill": {
     "duration": 0.004601,
     "end_time": "2026-02-03T02:57:19.910829",
     "exception": false,
     "start_time": "2026-02-03T02:57:19.906228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference, Ensembling, Submission & QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9ea64e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-03T02:57:19.922706Z",
     "iopub.status.busy": "2026-02-03T02:57:19.922279Z",
     "iopub.status.idle": "2026-02-03T02:57:20.161329Z",
     "shell.execute_reply": "2026-02-03T02:57:20.159897Z"
    },
    "papermill": {
     "duration": 0.248552,
     "end_time": "2026-02-03T02:57:20.163800",
     "exception": false,
     "start_time": "2026-02-03T02:57:19.915248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 5 SUMMARY ===\n",
      "[OK] submission: /kaggle/working/submission.csv\n",
      "[OK] qa: /kaggle/working/qa_submission.json\n",
      "QA: {'rows_sample_submission': 455, 'rows_pred_parsed': 455, 'rows_submission': 455, 'missing_pred_after_merge': 0, 'label_set_ok': True, 'pred_distribution': {'SEDANG': 382, 'TIDAK SEHAT': 73}, 'n_models_ensembled': 2, 'proba_min': 0.13974515556888395, 'proba_max': 0.6875149061710245}\n",
      "\n",
      "Submission head:\n",
      "                id     category\n",
      "0  2025-09-01_DKI1  TIDAK SEHAT\n",
      "1  2025-09-01_DKI2  TIDAK SEHAT\n",
      "2  2025-09-01_DKI3       SEDANG\n",
      "3  2025-09-01_DKI4  TIDAK SEHAT\n",
      "4  2025-09-01_DKI5       SEDANG\n",
      "5  2025-09-02_DKI1  TIDAK SEHAT\n",
      "6  2025-09-02_DKI2  TIDAK SEHAT\n",
      "7  2025-09-02_DKI3       SEDANG\n",
      "8  2025-09-02_DKI4  TIDAK SEHAT\n",
      "9  2025-09-02_DKI5       SEDANG\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — Inference (Rebuild from sample_submission), Ensemble, Submission & QA — ONE CELL (REVISI FULL)\n",
    "# - Parse tanggal dari ID pakai format YYYY-MM-DD (AMAN)\n",
    "# - Build fitur inference sesuai FEATURE_COLS_MODEL_USED (Stage 4)\n",
    "# - Ensemble predict_proba dari models_by_fold\n",
    "# - Output /kaggle/working/submission.csv + QA\n",
    "# ============================================================\n",
    "\n",
    "import re, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- guards ----------\n",
    "need = [\"df_ispu_hist\", \"models_by_fold\", \"FEATURE_COLS_MODEL_USED\", \"CAT_FEATS_MODEL_USED\", \"VALID_STATIONS\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from previous stages: {miss}\")\n",
    "\n",
    "LABELS = [\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\"]\n",
    "id_to_label = {i:k for i,k in enumerate(LABELS)}\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n",
    "SUB_PATH  = DATA_ROOT / \"sample_submission.csv\"\n",
    "HOL_PATH  = DATA_ROOT / \"libur-nasional\" / \"dataset-libur-nasional-dan-weekend.csv\"\n",
    "\n",
    "OUT_SUB = Path(\"/kaggle/working/submission.csv\")\n",
    "OUT_QA  = Path(\"/kaggle/working/qa_submission.json\")\n",
    "\n",
    "FEATURE_COLS = FEATURE_COLS_MODEL_USED\n",
    "CAT_COLS = CAT_FEATS_MODEL_USED\n",
    "NUM_COLS = [c for c in FEATURE_COLS if c not in CAT_COLS]\n",
    "\n",
    "def sanitize_features(df: pd.DataFrame, cat_cols, num_cols):\n",
    "    df = df.copy()\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\").fillna(\"NA\").astype(str)\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "def extract_station(id_series: pd.Series) -> pd.Series:\n",
    "    x = id_series.astype(str).str.upper()\n",
    "    code = x.str.extract(r\"(DKI\\s*\\d+)\", expand=False)\n",
    "    return code.str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "def extract_date_ymd(id_series: pd.Series) -> pd.Series:\n",
    "    # ambil token YYYY-MM-DD dari id, parse FIXED format\n",
    "    tok = id_series.astype(str).str.extract(r\"(\\d{4}-\\d{2}-\\d{2})\", expand=False)\n",
    "    return pd.to_datetime(tok, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# ============================================================\n",
    "# 5.1 Read sample_submission & parse id\n",
    "# ============================================================\n",
    "df_sample = pd.read_csv(SUB_PATH)\n",
    "sample_cols = [c.strip() for c in df_sample.columns.tolist()]\n",
    "id_col = sample_cols[0]\n",
    "target_col = sample_cols[1] if len(sample_cols) > 1 else \"kategori\"\n",
    "\n",
    "df_pred = pd.DataFrame({id_col: df_sample[id_col].astype(str)})\n",
    "df_pred[\"target_date\"] = extract_date_ymd(df_pred[id_col])\n",
    "df_pred[\"stasiun_code\"] = extract_station(df_pred[id_col])\n",
    "\n",
    "df_pred = df_pred[df_pred[\"target_date\"].notna() & df_pred[\"stasiun_code\"].notna()].copy()\n",
    "df_pred = df_pred[df_pred[\"stasiun_code\"].isin(VALID_STATIONS)].reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# 5.2 last_obs_date & horizon_days (cutoff efektif = data terakhir yang tersedia)\n",
    "# ============================================================\n",
    "dfh = df_ispu_hist.sort_values([\"stasiun_code\",\"tanggal\"]).reset_index(drop=True)\n",
    "last_obs_by_station = dfh.groupby(\"stasiun_code\")[\"tanggal\"].max()\n",
    "df_pred[\"last_obs_date\"] = df_pred[\"stasiun_code\"].map(last_obs_by_station)\n",
    "df_pred[\"horizon_days\"] = (df_pred[\"target_date\"] - df_pred[\"last_obs_date\"]).dt.days.astype(int)\n",
    "df_pred[\"cutoff_date\"] = df_pred[\"last_obs_date\"]\n",
    "\n",
    "# ============================================================\n",
    "# 5.3 Build state features table (same as Stage 4) then merge to cutoff_date\n",
    "# ============================================================\n",
    "POLL_COLS = [c for c in [\"pm_sepuluh\",\"pm_duakomalima\",\"sulfur_dioksida\",\"karbon_monoksida\",\"ozon\",\"nitrogen_dioksida\",\"max\"] if c in dfh.columns]\n",
    "\n",
    "for c in POLL_COLS:\n",
    "    g = dfh.groupby(\"stasiun_code\")[c]\n",
    "    dfh[f\"{c}_lag1\"] = g.shift(1)\n",
    "    dfh[f\"{c}_lag7\"] = g.shift(7)\n",
    "    dfh[f\"{c}_rmean7\"]  = g.shift(1).rolling(7).mean().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rstd7\"]   = g.shift(1).rolling(7).std().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rmean30\"] = g.shift(1).rolling(30).mean().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_rstd30\"]  = g.shift(1).rolling(30).std().reset_index(level=0, drop=True)\n",
    "    dfh[f\"{c}_trend_7_30\"] = dfh[f\"{c}_rmean7\"] - dfh[f\"{c}_rmean30\"]\n",
    "    dfh[f\"{c}_na_rate30\"]  = g.shift(1).isna().astype(float).rolling(30).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "state_cols = [c for c in dfh.columns if any(c.startswith(pc+\"_\") for pc in POLL_COLS)]\n",
    "df_base = dfh[[\"stasiun_code\",\"tanggal\"] + state_cols].rename(columns={\"tanggal\":\"cutoff_date\"}).copy()\n",
    "df_base[\"stasiun_code\"] = df_base[\"stasiun_code\"].astype(\"string\")\n",
    "df_base[\"cutoff_date\"] = pd.to_datetime(df_base[\"cutoff_date\"])\n",
    "df_base = df_base.sort_values([\"cutoff_date\",\"stasiun_code\"]).reset_index(drop=True)\n",
    "\n",
    "df_pred[\"stasiun_code\"] = df_pred[\"stasiun_code\"].astype(\"string\")\n",
    "df_pred[\"cutoff_date\"] = pd.to_datetime(df_pred[\"cutoff_date\"])\n",
    "df_pred = df_pred.sort_values([\"cutoff_date\",\"stasiun_code\"]).reset_index(drop=True)\n",
    "\n",
    "df_pred = pd.merge_asof(\n",
    "    df_pred, df_base,\n",
    "    on=\"cutoff_date\", by=\"stasiun_code\",\n",
    "    direction=\"backward\",\n",
    "    allow_exact_matches=True,\n",
    "    tolerance=pd.Timedelta(days=60)\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5.4 Calendar + holiday features on target_date (recompute directly, safe)\n",
    "# ============================================================\n",
    "df_pred[\"year\"] = df_pred[\"target_date\"].dt.year\n",
    "df_pred[\"month\"] = df_pred[\"target_date\"].dt.month\n",
    "df_pred[\"day\"] = df_pred[\"target_date\"].dt.day\n",
    "df_pred[\"dow\"] = df_pred[\"target_date\"].dt.dayofweek\n",
    "df_pred[\"dayofyear\"] = df_pred[\"target_date\"].dt.dayofyear\n",
    "df_pred[\"sin_doy\"] = np.sin(2*np.pi*df_pred[\"dayofyear\"]/365.25)\n",
    "df_pred[\"cos_doy\"] = np.cos(2*np.pi*df_pred[\"dayofyear\"]/365.25)\n",
    "df_pred[\"sin_month\"] = np.sin(2*np.pi*df_pred[\"month\"]/12.0)\n",
    "df_pred[\"cos_month\"] = np.cos(2*np.pi*df_pred[\"month\"]/12.0)\n",
    "df_pred[\"is_weekend\"] = (df_pred[\"dow\"] >= 5).astype(int)\n",
    "\n",
    "df_hol = pd.read_csv(HOL_PATH)\n",
    "df_hol.columns = [str(c).strip().lower() for c in df_hol.columns]\n",
    "df_hol[\"tanggal\"] = pd.to_datetime(df_hol[\"tanggal\"], errors=\"coerce\")\n",
    "keep_h = [c for c in [\"tanggal\",\"is_holiday_nasional\",\"nama_libur\",\"day_name\"] if c in df_hol.columns]\n",
    "df_hol = df_hol[keep_h].dropna(subset=[\"tanggal\"]).drop_duplicates(\"tanggal\")\n",
    "\n",
    "df_pred = df_pred.merge(df_hol, left_on=\"target_date\", right_on=\"tanggal\", how=\"left\").drop(columns=[\"tanggal\"], errors=\"ignore\")\n",
    "df_pred[\"is_holiday_nasional\"] = pd.to_numeric(df_pred.get(\"is_holiday_nasional\", 0), errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# horizon transforms\n",
    "df_pred[\"horizon_weeks\"] = df_pred[\"horizon_days\"] / 7.0\n",
    "df_pred[\"horizon_months\"] = df_pred[\"horizon_days\"] / 30.0\n",
    "df_pred[\"log1p_horizon\"] = np.log1p(df_pred[\"horizon_days\"].clip(lower=0))\n",
    "\n",
    "# ============================================================\n",
    "# 5.5 Build X with exact FEATURE_COLS_MODEL_USED (add missing cols if any)\n",
    "# ============================================================\n",
    "X = df_pred.copy()\n",
    "for c in FEATURE_COLS:\n",
    "    if c not in X.columns:\n",
    "        X[c] = np.nan\n",
    "\n",
    "X = sanitize_features(X, CAT_COLS, NUM_COLS)\n",
    "\n",
    "# ============================================================\n",
    "# 5.6 Ensemble predict_proba\n",
    "# ============================================================\n",
    "models = list(models_by_fold.values())\n",
    "if len(models) == 0:\n",
    "    raise RuntimeError(\"models_by_fold kosong. Pastikan Stage 4 sukses.\")\n",
    "\n",
    "proba_sum = None\n",
    "for m in models:\n",
    "    p = np.asarray(m.predict_proba(X[FEATURE_COLS]), dtype=float)\n",
    "    proba_sum = p if proba_sum is None else (proba_sum + p)\n",
    "\n",
    "proba_mean = proba_sum / len(models)\n",
    "pred = proba_mean.argmax(axis=1)\n",
    "pred_label = pd.Series(pred).map(id_to_label).values\n",
    "\n",
    "pred_df = pd.DataFrame({id_col: df_pred[id_col].values, target_col: pred_label})\n",
    "\n",
    "# ============================================================\n",
    "# 5.7 Build final submission in EXACT sample order (NO KeyError)\n",
    "# ============================================================\n",
    "sub = df_sample[[id_col]].merge(pred_df, on=id_col, how=\"left\")\n",
    "\n",
    "# fallback if any missing predictions (should be 0)\n",
    "sub[target_col] = sub[target_col].fillna(\"SEDANG\")\n",
    "\n",
    "sub.to_csv(OUT_SUB, index=False)\n",
    "\n",
    "# ============================================================\n",
    "# 5.8 QA\n",
    "# ============================================================\n",
    "qa = {}\n",
    "qa[\"rows_sample_submission\"] = int(len(df_sample))\n",
    "qa[\"rows_pred_parsed\"] = int(len(df_pred))\n",
    "qa[\"rows_submission\"] = int(len(sub))\n",
    "qa[\"missing_pred_after_merge\"] = int(sub[target_col].isna().sum())\n",
    "qa[\"label_set_ok\"] = bool(set(sub[target_col].unique()).issubset(set(LABELS)))\n",
    "qa[\"pred_distribution\"] = {str(k): int(v) for k,v in sub[target_col].value_counts(dropna=False).to_dict().items()}\n",
    "qa[\"n_models_ensembled\"] = int(len(models))\n",
    "qa[\"proba_min\"] = float(np.nanmin(proba_mean))\n",
    "qa[\"proba_max\"] = float(np.nanmax(proba_mean))\n",
    "\n",
    "OUT_QA.write_text(json.dumps(qa, indent=2))\n",
    "\n",
    "print(\"=== STAGE 5 SUMMARY ===\")\n",
    "print(\"[OK] submission:\", str(OUT_SUB))\n",
    "print(\"[OK] qa:\", str(OUT_QA))\n",
    "print(\"QA:\", qa)\n",
    "print(\"\\nSubmission head:\")\n",
    "print(sub.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15499519,
     "sourceId": 129145,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 57.953777,
   "end_time": "2026-02-03T02:57:20.993889",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-03T02:56:23.040112",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
