{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:45:20.057967Z","iopub.execute_input":"2026-01-30T18:45:20.058383Z","iopub.status.idle":"2026-01-30T18:45:21.906492Z","shell.execute_reply.started":"2026-01-30T18:45:20.058343Z","shell.execute_reply":"2026-01-30T18:45:21.905386Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/penyisihan-datavidia-10/sample_submission.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2025.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/NDVI (vegetation index)/indeks-ndvi-jakarta.csv\n/kaggle/input/penyisihan-datavidia-10/kualitas-air-sungai/data-kualitas-air-sungai-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/libur-nasional/dataset-libur-nasional-dan-weekend.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki3-jagakarsa.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki2-kelapagading.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki1-bundaranhi.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki5-kebonjeruk.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki4-lubangbuaya.csv\n/kaggle/input/penyisihan-datavidia-10/jumlah-penduduk/data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data Loading & Sanity Checks","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 1 — Data Loading & Sanity Checks (ONE CELL) — CatBoost Track (REVISED v2)\n# Fixes:\n# - Series.lower() bug -> use .str.lower()\n# - Keeps robust parsing, dedup, and label/critical unification\n# Outputs (globals):\n#   sub, ID_COL, SUB_TARGET_COL\n#   df_ispu_all, df_train, df_ispu_unlabeled\n#   df_ndvi, df_holiday, df_weather, df_pop, df_river\n#   test_candidates\n# ============================================================\n\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nDATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\nassert DATA_ROOT.exists(), f\"DATA_ROOT not found: {DATA_ROOT}\"\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 200)\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef _read_csv_smart(path: Path) -> pd.DataFrame:\n    seps = [\",\", \";\", \"\\t\", \"|\"]\n    encs = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n    last_err = None\n    for sep in seps:\n        for enc in encs:\n            try:\n                df = pd.read_csv(path, sep=sep, encoding=enc, low_memory=False)\n                if df.shape[1] >= 2:\n                    return df\n            except Exception as e:\n                last_err = e\n    raise RuntimeError(f\"Failed to read: {path}\\nLast error: {last_err}\")\n\ndef _norm_col(c: str) -> str:\n    c = str(c).strip().lower()\n    c = re.sub(r\"[^\\w]+\", \"_\", c)\n    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n    return c\n\ndef _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    cols = {c: _norm_col(c) for c in df.columns}\n    df = df.rename(columns=cols)\n\n    rename = {}\n    for c in df.columns:\n        if c in [\"tanggal\", \"date\", \"time\", \"waktu\"]:\n            rename[c] = \"tanggal\"\n        elif c in [\"stasiun\", \"station\", \"stasiun_id\", \"id_stasiun\"]:\n            rename[c] = \"stasiun\"\n        elif c in [\"periode_data\", \"periode\"]:\n            rename[c] = \"periode_data\"\n        elif c in [\"pm_sepuluh\", \"pm10\", \"pm_10\"]:\n            rename[c] = \"pm10\"\n        elif c in [\"pm_duakomalima\", \"pm2_5\", \"pm25\", \"pm_2_5\", \"pm2_5_\"]:\n            rename[c] = \"pm25\"\n        elif c in [\"sulfur_dioksida\", \"so2\"]:\n            rename[c] = \"so2\"\n        elif c in [\"karbon_monoksida\", \"co\"]:\n            rename[c] = \"co\"\n        elif c in [\"ozon\", \"o3\"]:\n            rename[c] = \"o3\"\n        elif c in [\"nitrogen_dioksida\", \"no2\"]:\n            rename[c] = \"no2\"\n        elif c in [\"parameter_pencemar_kritis\", \"parameter_pencemar\", \"pencemar_kritis\"]:\n            rename[c] = \"parameter_pencemar_kritis\"\n        elif c in [\"max\", \"maks\", \"nilai_maks\", \"indeks_maks\"]:\n            rename[c] = \"max\"\n        elif c in [\"ndvi\", \"vegetation_index\"]:\n            rename[c] = \"ndvi\"\n        elif c in [\"is_holiday_nasional\", \"holiday_nasional\", \"is_holiday\"]:\n            rename[c] = \"is_holiday_nasional\"\n        elif c in [\"is_weekend\", \"weekend\"]:\n            rename[c] = \"is_weekend\"\n        elif c in [\"day_name\", \"nama_hari\"]:\n            rename[c] = \"day_name\"\n        elif c in [\"nama_libur\", \"holiday_name\"]:\n            rename[c] = \"nama_libur\"\n\n        # keep common typos as *_alt\n        elif c == \"categori\":\n            rename[c] = \"kategori_alt\"\n        elif c == \"critical\":\n            rename[c] = \"parameter_pencemar_kritis_alt\"\n\n    return df.rename(columns=rename)\n\ndef parse_date_twopass(s: pd.Series) -> pd.Series:\n    s = s.astype(str).str.strip()\n    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"None\": np.nan})\n    d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n    m = d1.isna()\n    if m.any():\n        d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n        d1.loc[m] = d2\n    return d1\n\ndef _coerce_numeric(df: pd.DataFrame, cols):\n    for c in cols:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef _dedup_keep_most_complete(df: pd.DataFrame, key_cols):\n    if not all(k in df.columns for k in key_cols):\n        return df\n    df = df.copy()\n    df[\"_nn\"] = df.notna().sum(axis=1)\n    idx = df.groupby(key_cols)[\"_nn\"].idxmax()\n    df = df.loc[idx].drop(columns=[\"_nn\"]).reset_index(drop=True)\n    return df\n\ndef _basic_sanity(name: str, df: pd.DataFrame, key_cols=None, date_col=\"tanggal\"):\n    print(f\"\\n--- {name} ---\")\n    print(\"shape:\", df.shape)\n    if key_cols is not None and all(k in df.columns for k in key_cols):\n        print(f\"duplicates on {key_cols}:\", int(df.duplicated(key_cols).sum()))\n    if date_col in df.columns:\n        print(f\"{date_col}: NaT={int(df[date_col].isna().sum())} | range=[{df[date_col].min()} .. {df[date_col].max()}]\")\n    miss = (df.isna().mean().sort_values(ascending=False).head(8) * 100).round(2)\n    print(\"top missing% cols:\")\n    print(miss.to_string())\n\n# ----------------------------\n# 0) sample_submission\n# ----------------------------\nsub = _standardize_columns(_read_csv_smart(DATA_ROOT / \"sample_submission.csv\"))\nID_COL = \"id\" if \"id\" in sub.columns else sub.columns[0]\nSUB_TARGET_COL = \"category\" if \"category\" in sub.columns else sub.columns[-1]\nn_test_expected = len(sub)\n\nprint(\"Loaded sample_submission:\", sub.shape, \"cols:\", list(sub.columns))\nprint(\"ID_COL:\", ID_COL, \"| SUB_TARGET_COL:\", SUB_TARGET_COL)\nprint(\"submission ID unique:\", bool(sub[ID_COL].is_unique))\n\n# ----------------------------\n# 1) ISPU (concat all years) + CLEAN\n# ----------------------------\nispu_files = sorted((DATA_ROOT / \"ISPU\").glob(\"*.csv\"))\nassert len(ispu_files) > 0, \"No ISPU CSV files found.\"\n\nframes = []\nfor p in ispu_files:\n    df = _standardize_columns(_read_csv_smart(p))\n    df[\"source_file\"] = p.name\n    frames.append(df)\n\ndf_ispu_all = pd.concat(frames, ignore_index=True, sort=False)\n\n# unify label + critical columns into canonical names\nif \"kategori_alt\" in df_ispu_all.columns:\n    if \"kategori\" not in df_ispu_all.columns:\n        df_ispu_all[\"kategori\"] = df_ispu_all[\"kategori_alt\"]\n    else:\n        df_ispu_all[\"kategori\"] = df_ispu_all[\"kategori\"].fillna(df_ispu_all[\"kategori_alt\"])\n\nif \"parameter_pencemar_kritis_alt\" in df_ispu_all.columns:\n    if \"parameter_pencemar_kritis\" not in df_ispu_all.columns:\n        df_ispu_all[\"parameter_pencemar_kritis\"] = df_ispu_all[\"parameter_pencemar_kritis_alt\"]\n    else:\n        df_ispu_all[\"parameter_pencemar_kritis\"] = df_ispu_all[\"parameter_pencemar_kritis\"].fillna(df_ispu_all[\"parameter_pencemar_kritis_alt\"])\n\n# robust date parse\nif \"tanggal\" in df_ispu_all.columns:\n    df_ispu_all[\"tanggal\"] = parse_date_twopass(df_ispu_all[\"tanggal\"])\n\n# stasiun cleanup + code\nif \"stasiun\" in df_ispu_all.columns:\n    df_ispu_all[\"stasiun\"] = df_ispu_all[\"stasiun\"].astype(str).str.strip()\n    df_ispu_all[\"stasiun_code\"] = (\n        df_ispu_all[\"stasiun\"]\n        .str.upper()\n        .str.extract(r\"(DKI\\s*\\d+)\", expand=False)\n        .str.replace(\" \", \"\", regex=False)\n    )\nelse:\n    df_ispu_all[\"stasiun_code\"] = np.nan\n\n# numeric casts\ndf_ispu_all = _coerce_numeric(df_ispu_all, [\"pm10\", \"pm25\", \"so2\", \"co\", \"o3\", \"no2\", \"max\"])\n\n# drop rows missing key fields\ndf_ispu_all = df_ispu_all.dropna(subset=[\"tanggal\", \"stasiun\"]).copy()\n\n# dedup by key keep most complete\ndf_ispu_all = _dedup_keep_most_complete(df_ispu_all, [\"tanggal\", \"stasiun\"])\ndf_ispu_all = df_ispu_all.sort_values([\"tanggal\", \"stasiun\"]).reset_index(drop=True)\n\n_basic_sanity(\"ISPU (ALL) CLEAN\", df_ispu_all, key_cols=[\"tanggal\", \"stasiun\"])\n\n# build train vs unlabeled\nif \"kategori\" in df_ispu_all.columns:\n    lab = df_ispu_all[\"kategori\"].astype(str).str.strip()\n    lab_low = lab.str.lower()\n    m_train = df_ispu_all[\"kategori\"].notna() & (lab != \"\") & (lab_low != \"nan\")\n    df_train = df_ispu_all.loc[m_train].copy()\n    df_ispu_unlabeled = df_ispu_all.loc[~m_train].copy()\nelse:\n    df_train = df_ispu_all.copy()\n    df_ispu_unlabeled = df_ispu_all.iloc[0:0].copy()\n\nprint(\"\\nTrain/unlabeled split:\")\nprint(\"df_train:\", df_train.shape, \"| df_ispu_unlabeled:\", df_ispu_unlabeled.shape)\nif \"kategori\" in df_train.columns:\n    print(\"\\nTarget distribution (df_train):\")\n    print(df_train[\"kategori\"].astype(str).str.strip().value_counts(dropna=False).to_string())\n\n# ----------------------------\n# 2) NDVI + stasiun_code\n# ----------------------------\ndf_ndvi = _standardize_columns(_read_csv_smart(DATA_ROOT / \"NDVI (vegetation index)\" / \"indeks-ndvi-jakarta.csv\"))\nif \"tanggal\" in df_ndvi.columns:\n    df_ndvi[\"tanggal\"] = parse_date_twopass(df_ndvi[\"tanggal\"])\nif \"stasiun\" in df_ndvi.columns:\n    df_ndvi[\"stasiun\"] = df_ndvi[\"stasiun\"].astype(str).str.strip().str.upper().str.replace(\" \", \"\", regex=False)\n    df_ndvi[\"stasiun_code\"] = df_ndvi[\"stasiun\"].str.extract(r\"(DKI\\d+)\", expand=False)\ndf_ndvi = _coerce_numeric(df_ndvi, [\"ndvi\"])\ndf_ndvi = df_ndvi.dropna(subset=[\"tanggal\", \"stasiun\"]).copy()\ndf_ndvi = _dedup_keep_most_complete(df_ndvi, [\"tanggal\", \"stasiun\"])\n\n_basic_sanity(\"NDVI CLEAN\", df_ndvi, key_cols=[\"tanggal\", \"stasiun\"])\n\n# ----------------------------\n# 3) Holidays (clean to one row per date)\n# ----------------------------\ndf_holiday = _standardize_columns(_read_csv_smart(DATA_ROOT / \"libur-nasional\" / \"dataset-libur-nasional-dan-weekend.csv\"))\nif \"tanggal\" in df_holiday.columns:\n    df_holiday[\"tanggal\"] = parse_date_twopass(df_holiday[\"tanggal\"])\ndf_holiday = df_holiday.dropna(subset=[\"tanggal\"]).sort_values(\"tanggal\").copy()\n\nfor c in [\"is_holiday_nasional\", \"is_weekend\"]:\n    if c in df_holiday.columns:\n        df_holiday[c] = pd.to_numeric(df_holiday[c], errors=\"coerce\").fillna(0).astype(int)\n\nagg = {}\nif \"is_holiday_nasional\" in df_holiday.columns: agg[\"is_holiday_nasional\"] = \"max\"\nif \"is_weekend\" in df_holiday.columns: agg[\"is_weekend\"] = \"max\"\nif \"nama_libur\" in df_holiday.columns: agg[\"nama_libur\"] = \"first\"\ndf_holiday = df_holiday.groupby(\"tanggal\", as_index=False).agg(agg)\ndf_holiday[\"day_name\"] = df_holiday[\"tanggal\"].dt.day_name()\n\n_basic_sanity(\"HOLIDAYS CLEAN\", df_holiday, key_cols=[\"tanggal\"])\n\n# ----------------------------\n# 4) Weather (multiple stations) clean\n# ----------------------------\nweather_files = sorted((DATA_ROOT / \"cuaca-harian\").glob(\"*.csv\"))\nassert len(weather_files) > 0, \"No weather CSV files found.\"\n\nw_frames = []\nfor p in weather_files:\n    w = _standardize_columns(_read_csv_smart(p))\n    tag = p.stem.lower().replace(\"cuaca_harian_\", \"\").replace(\"cuaca-harian-\", \"\")\n    w[\"weather_station\"] = tag\n    w[\"weather_code\"] = (pd.Series([tag] * len(w)).str.extract(r\"(dki\\d)\", expand=False).str.upper())\n    if \"tanggal\" in w.columns:\n        w[\"tanggal\"] = parse_date_twopass(w[\"tanggal\"])\n    w_frames.append(w)\n\ndf_weather = pd.concat(w_frames, ignore_index=True, sort=False)\ndf_weather = df_weather.dropna(subset=[\"tanggal\"]).copy()\n\nfor c in df_weather.columns:\n    if c not in [\"tanggal\", \"weather_station\", \"weather_code\"]:\n        if df_weather[c].dtype == object:\n            df_weather[c] = pd.to_numeric(df_weather[c], errors=\"ignore\")\n\ndf_weather = _dedup_keep_most_complete(df_weather, [\"tanggal\", \"weather_station\"])\ndf_weather = df_weather.sort_values([\"weather_station\", \"tanggal\"]).reset_index(drop=True)\n\n_basic_sanity(\"WEATHER (ALL) CLEAN\", df_weather, key_cols=[\"tanggal\", \"weather_station\"])\n\n# ----------------------------\n# 5) Population\n# ----------------------------\ndf_pop = _standardize_columns(_read_csv_smart(DATA_ROOT / \"jumlah-penduduk\" / \"data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\"))\nif \"tahun\" in df_pop.columns:\n    df_pop[\"tahun\"] = pd.to_numeric(df_pop[\"tahun\"], errors=\"coerce\")\nif \"jumlah_penduduk\" in df_pop.columns:\n    df_pop[\"jumlah_penduduk\"] = pd.to_numeric(df_pop[\"jumlah_penduduk\"], errors=\"coerce\")\n_basic_sanity(\"POPULATION\", df_pop)\n\n# ----------------------------\n# 6) River Quality\n# ----------------------------\ndf_river = _standardize_columns(_read_csv_smart(DATA_ROOT / \"kualitas-air-sungai\" / \"data-kualitas-air-sungai-komponen-data.csv\"))\nfor c in [\"latitude\", \"longitude\", \"baku_mutu\", \"hasil_pengukuran\", \"bulan_sampling\"]:\n    if c in df_river.columns:\n        df_river[c] = pd.to_numeric(df_river[c], errors=\"coerce\")\n_basic_sanity(\"RIVER QUALITY\", df_river)\n\n# ----------------------------\n# 7) Find test mapping file candidates (rows == sample_submission) with an 'id' column\n# ----------------------------\ntest_candidates = []\nfor p in DATA_ROOT.rglob(\"*.csv\"):\n    if p.name == \"sample_submission.csv\":\n        continue\n    name = p.name.lower()\n    if (\"test\" in name) or (\"submission\" in name):\n        try:\n            tmp = _standardize_columns(_read_csv_smart(p).head(2))\n            if \"id\" in tmp.columns:\n                df_full = _standardize_columns(_read_csv_smart(p))\n                if len(df_full) == n_test_expected:\n                    test_candidates.append((str(p), list(df_full.columns)))\n        except Exception:\n            pass\n\nprint(\"\\n--- Test mapping file candidates (rows == sample_submission) ---\")\nif len(test_candidates) == 0:\n    print(\"None found by heuristic. Likely there is a separate test file not matching this heuristic name.\")\nelse:\n    for fp, cols in test_candidates:\n        print(fp, \"| cols:\", cols)\n\n# ----------------------------\n# Preview\n# ----------------------------\nprint(\"\\n--- Preview heads ---\")\ndisplay(df_train.head(3))\ndisplay(df_ndvi.head(3))\ndisplay(df_holiday.head(3))\ndisplay(df_weather.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:45:21.909420Z","iopub.execute_input":"2026-01-30T18:45:21.909909Z","iopub.status.idle":"2026-01-30T18:45:23.141106Z","shell.execute_reply.started":"2026-01-30T18:45:21.909875Z","shell.execute_reply":"2026-01-30T18:45:23.139867Z"}},"outputs":[{"name":"stdout","text":"Loaded sample_submission: (455, 2) cols: ['id', 'category']\nID_COL: id | SUB_TARGET_COL: category\nsubmission ID unique: True\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/160485658.py:97: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n  d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n/tmp/ipykernel_55/160485658.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n  d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n/tmp/ipykernel_55/160485658.py:97: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n  d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n","output_type":"stream"},{"name":"stdout","text":"\n--- ISPU (ALL) CLEAN ---\nshape: (13652, 18)\nduplicates on ['tanggal', 'stasiun']: 0\ntanggal: NaT=0 | range=[2010-01-01 00:00:00 .. 2023-11-30 00:00:00]\ntop missing% cols:\nbulan                            100.00\nlokasi_spku                       78.96\npm25                              73.49\nparameter_pencemar_kritis_alt     23.74\nstasiun_code                      21.04\npm10                              14.42\nkategori_alt                      13.38\no3                                12.80\n\nTrain/unlabeled split:\ndf_train: (13651, 18) | df_ispu_unlabeled: (1, 18)\n\nTarget distribution (df_train):\nkategori\nSEDANG                7997\nTIDAK SEHAT           2072\nBAIK                  1912\nTIDAK ADA DATA        1440\nSANGAT TIDAK SEHAT     199\nO3                      30\nBERBAHAYA                1\n\n--- NDVI CLEAN ---\nshape: (1810, 4)\nduplicates on ['tanggal', 'stasiun']: 0\ntanggal: NaT=0 | range=[2009-12-19 00:00:00 .. 2025-08-29 00:00:00]\ntop missing% cols:\ntanggal         0.0\nstasiun         0.0\nndvi            0.0\nstasiun_code    0.0\n\n--- HOLIDAYS CLEAN ---\nshape: (5844, 5)\nduplicates on ['tanggal']: 0\ntanggal: NaT=0 | range=[2010-01-01 00:00:00 .. 2025-12-31 00:00:00]\ntop missing% cols:\nnama_libur             95.6\ntanggal                 0.0\nis_holiday_nasional     0.0\nis_weekend              0.0\nday_name                0.0\n\n--- WEATHER (ALL) CLEAN ---\nshape: (28610, 26)\nduplicates on ['tanggal', 'weather_station']: 0\ntanggal: NaT=0 | range=[2010-01-01 00:00:00 .. 2025-12-08 00:00:00]\ntop missing% cols:\ntanggal                          0.0\ntemperature_2m_max_c             0.0\ntemperature_2m_min_c             0.0\nprecipitation_sum_mm             0.0\nprecipitation_hours_h            0.0\nwind_speed_10m_max_km_h          0.0\nwind_direction_10m_dominant      0.0\nshortwave_radiation_sum_mj_m²    0.0\n\n--- POPULATION ---\nshape: (34176, 9)\ntop missing% cols:\nperiode_data           0.0\ntahun                  0.0\nnama_provinsi          0.0\nnama_kabupaten_kota    0.0\nnama_kecamatan         0.0\nnama_kelurahan         0.0\nusia                   0.0\njenis_kelamin          0.0\n\n--- RIVER QUALITY ---\nshape: (14400, 12)\ntop missing% cols:\nperiode_data          0.0\nperiode_pemantauan    0.0\nbulan_sampling        0.0\ntitik_sampel          0.0\nnama_sungai           0.0\nalamat                0.0\nlatitude              0.0\nlongitude             0.0\n\n--- Test mapping file candidates (rows == sample_submission) ---\nNone found by heuristic. Likely there is a separate test file not matching this heuristic name.\n\n--- Preview heads ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   periode_data    tanggal               stasiun  pm10  pm25  so2    co    o3   no2   max parameter_pencemar_kritis        kategori                                        source_file  bulan  \\\n0        201001 2010-01-01    DKI1 (Bunderan HI)  60.0   NaN  4.0  73.0  27.0  14.0  73.0                        CO          SEDANG  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n1        201001 2010-01-01  DKI2 (Kelapa Gading)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n2        201001 2010-01-01      DKI3 (Jagakarsa)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n\n  parameter_pencemar_kritis_alt    kategori_alt lokasi_spku stasiun_code  \n0                            CO          SEDANG         NaN         DKI1  \n1                           NaN  TIDAK ADA DATA         NaN         DKI2  \n2                           NaN  TIDAK ADA DATA         NaN         DKI3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>periode_data</th>\n      <th>tanggal</th>\n      <th>stasiun</th>\n      <th>pm10</th>\n      <th>pm25</th>\n      <th>so2</th>\n      <th>co</th>\n      <th>o3</th>\n      <th>no2</th>\n      <th>max</th>\n      <th>parameter_pencemar_kritis</th>\n      <th>kategori</th>\n      <th>source_file</th>\n      <th>bulan</th>\n      <th>parameter_pencemar_kritis_alt</th>\n      <th>kategori_alt</th>\n      <th>lokasi_spku</th>\n      <th>stasiun_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>201001</td>\n      <td>2010-01-01</td>\n      <td>DKI1 (Bunderan HI)</td>\n      <td>60.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>73.0</td>\n      <td>27.0</td>\n      <td>14.0</td>\n      <td>73.0</td>\n      <td>CO</td>\n      <td>SEDANG</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>CO</td>\n      <td>SEDANG</td>\n      <td>NaN</td>\n      <td>DKI1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>201001</td>\n      <td>2010-01-01</td>\n      <td>DKI2 (Kelapa Gading)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>NaN</td>\n      <td>DKI2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>201001</td>\n      <td>2010-01-01</td>\n      <td>DKI3 (Jagakarsa)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>NaN</td>\n      <td>DKI3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     tanggal stasiun    ndvi stasiun_code\n0 2009-12-19    DKI1  0.1849         DKI1\n1 2009-12-19    DKI2  0.2891         DKI2\n2 2009-12-19    DKI3  0.5613         DKI3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tanggal</th>\n      <th>stasiun</th>\n      <th>ndvi</th>\n      <th>stasiun_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2009-12-19</td>\n      <td>DKI1</td>\n      <td>0.1849</td>\n      <td>DKI1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2009-12-19</td>\n      <td>DKI2</td>\n      <td>0.2891</td>\n      <td>DKI2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2009-12-19</td>\n      <td>DKI3</td>\n      <td>0.5613</td>\n      <td>DKI3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     tanggal  is_holiday_nasional  is_weekend      nama_libur  day_name\n0 2010-01-01                    1           0  New Year's Day    Friday\n1 2010-01-02                    0           0            None  Saturday\n2 2010-01-03                    0           0            None    Sunday","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tanggal</th>\n      <th>is_holiday_nasional</th>\n      <th>is_weekend</th>\n      <th>nama_libur</th>\n      <th>day_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-01</td>\n      <td>1</td>\n      <td>0</td>\n      <td>New Year's Day</td>\n      <td>Friday</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-02</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n      <td>Saturday</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-03</td>\n      <td>0</td>\n      <td>0</td>\n      <td>None</td>\n      <td>Sunday</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     tanggal  temperature_2m_max_c  temperature_2m_min_c  precipitation_sum_mm  precipitation_hours_h  wind_speed_10m_max_km_h  wind_direction_10m_dominant  shortwave_radiation_sum_mj_m²  \\\n0 2010-01-01                  29.4                  24.4                   4.0                   14.0                     16.0                          246                          16.24   \n1 2010-01-02                  28.9                  24.2                   6.9                   14.0                      9.5                          260                          13.01   \n2 2010-01-03                  31.4                  24.9                  11.2                    6.0                      9.4                          224                          23.89   \n\n   temperature_2m_mean_c  relative_humidity_2m_mean  cloud_cover_mean  surface_pressure_mean_hpa  wind_gusts_10m_max_km_h  winddirection_10m_dominant  relative_humidity_2m_max  \\\n0                   26.6                         81               100                     1007.5                     38.2                         246                        90   \n1                   26.2                         85                99                     1010.1                     22.0                         260                        95   \n2                   27.1                         85                93                     1009.9                     21.2                         224                        95   \n\n   relative_humidity_2m_min  cloud_cover_max  cloud_cover_min  wind_gusts_10m_mean_km_h  wind_speed_10m_mean_km_h  wind_gusts_10m_min_km_h  wind_speed_10m_min_km_h  surface_pressure_max_hpa  \\\n0                        69              100               99                      21.0                      10.5                     11.9                      6.9                    1009.3   \n1                        72              100               94                      13.7                       6.0                      8.6                      2.3                    1011.9   \n2                        70              100               28                      15.7                       5.7                      8.3                      1.6                    1012.2   \n\n   surface_pressure_min_hpa  weather_station weather_code  \n0                    1005.1  dki1-bundaranhi         DKI1  \n1                    1007.4  dki1-bundaranhi         DKI1  \n2                    1007.0  dki1-bundaranhi         DKI1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tanggal</th>\n      <th>temperature_2m_max_c</th>\n      <th>temperature_2m_min_c</th>\n      <th>precipitation_sum_mm</th>\n      <th>precipitation_hours_h</th>\n      <th>wind_speed_10m_max_km_h</th>\n      <th>wind_direction_10m_dominant</th>\n      <th>shortwave_radiation_sum_mj_m²</th>\n      <th>temperature_2m_mean_c</th>\n      <th>relative_humidity_2m_mean</th>\n      <th>cloud_cover_mean</th>\n      <th>surface_pressure_mean_hpa</th>\n      <th>wind_gusts_10m_max_km_h</th>\n      <th>winddirection_10m_dominant</th>\n      <th>relative_humidity_2m_max</th>\n      <th>relative_humidity_2m_min</th>\n      <th>cloud_cover_max</th>\n      <th>cloud_cover_min</th>\n      <th>wind_gusts_10m_mean_km_h</th>\n      <th>wind_speed_10m_mean_km_h</th>\n      <th>wind_gusts_10m_min_km_h</th>\n      <th>wind_speed_10m_min_km_h</th>\n      <th>surface_pressure_max_hpa</th>\n      <th>surface_pressure_min_hpa</th>\n      <th>weather_station</th>\n      <th>weather_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010-01-01</td>\n      <td>29.4</td>\n      <td>24.4</td>\n      <td>4.0</td>\n      <td>14.0</td>\n      <td>16.0</td>\n      <td>246</td>\n      <td>16.24</td>\n      <td>26.6</td>\n      <td>81</td>\n      <td>100</td>\n      <td>1007.5</td>\n      <td>38.2</td>\n      <td>246</td>\n      <td>90</td>\n      <td>69</td>\n      <td>100</td>\n      <td>99</td>\n      <td>21.0</td>\n      <td>10.5</td>\n      <td>11.9</td>\n      <td>6.9</td>\n      <td>1009.3</td>\n      <td>1005.1</td>\n      <td>dki1-bundaranhi</td>\n      <td>DKI1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010-01-02</td>\n      <td>28.9</td>\n      <td>24.2</td>\n      <td>6.9</td>\n      <td>14.0</td>\n      <td>9.5</td>\n      <td>260</td>\n      <td>13.01</td>\n      <td>26.2</td>\n      <td>85</td>\n      <td>99</td>\n      <td>1010.1</td>\n      <td>22.0</td>\n      <td>260</td>\n      <td>95</td>\n      <td>72</td>\n      <td>100</td>\n      <td>94</td>\n      <td>13.7</td>\n      <td>6.0</td>\n      <td>8.6</td>\n      <td>2.3</td>\n      <td>1011.9</td>\n      <td>1007.4</td>\n      <td>dki1-bundaranhi</td>\n      <td>DKI1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010-01-03</td>\n      <td>31.4</td>\n      <td>24.9</td>\n      <td>11.2</td>\n      <td>6.0</td>\n      <td>9.4</td>\n      <td>224</td>\n      <td>23.89</td>\n      <td>27.1</td>\n      <td>85</td>\n      <td>93</td>\n      <td>1009.9</td>\n      <td>21.2</td>\n      <td>224</td>\n      <td>95</td>\n      <td>70</td>\n      <td>100</td>\n      <td>28</td>\n      <td>15.7</td>\n      <td>5.7</td>\n      <td>8.3</td>\n      <td>1.6</td>\n      <td>1012.2</td>\n      <td>1007.0</td>\n      <td>dki1-bundaranhi</td>\n      <td>DKI1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Master Table Building (Correct Joins)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 2 — Master Table Building (Correct Joins) (ONE CELL)\n# Builds:\n#   df_train_master  (for training)\n#   df_test_master   (for inference; requires test mapping file with id)\n# Notes:\n# - Joins are leakage-safe (no future info used here; lags/rolling will be Step 3)\n# - Uses safe joins: Holiday (by date), NDVI (date+stasiun_code), Weather (date+stasiun_code + global fallback),\n#   Population (year aggregate), River (year-month aggregate, global)\n# ============================================================\n\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ---- guards (assumes Step 1 already ran) ----\nneed = [\"sub\",\"ID_COL\",\"SUB_TARGET_COL\",\"df_train\",\"df_ndvi\",\"df_holiday\",\"df_weather\",\"df_pop\",\"df_river\"]\nmiss = [k for k in need if k not in globals()]\nif miss:\n    raise RuntimeError(f\"Missing globals from Step 1: {miss}. Jalankan Step 1 dulu.\")\n\nDATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n\ndef _norm_col(c: str) -> str:\n    c = str(c).strip().lower()\n    c = re.sub(r\"[^\\w]+\", \"_\", c)\n    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n    return c\n\ndef _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    return df.rename(columns={c: _norm_col(c) for c in df.columns})\n\ndef parse_date_twopass(s: pd.Series) -> pd.Series:\n    s = s.astype(str).str.strip()\n    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"None\": np.nan})\n    d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n    m = d1.isna()\n    if m.any():\n        d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n        d1.loc[m] = d2\n    return d1\n\ndef _mk_stasiun_code(stasiun_series: pd.Series) -> pd.Series:\n    st = stasiun_series.astype(str).str.strip().str.upper()\n    code = st.str.extract(r\"(DKI\\s*\\d+)\", expand=False).str.replace(\" \", \"\", regex=False)\n    return code\n\ndef _as_numeric_cols(df: pd.DataFrame) -> list:\n    num_cols = []\n    for c in df.columns:\n        if pd.api.types.is_numeric_dtype(df[c]):\n            num_cols.append(c)\n    return num_cols\n\ndef _prefix_cols(df: pd.DataFrame, prefix: str, keep: set) -> pd.DataFrame:\n    ren = {c: f\"{prefix}{c}\" for c in df.columns if c not in keep}\n    return df.rename(columns=ren)\n\ndef _find_test_mapping_file(data_root: Path, n_rows: int) -> Path | None:\n    # Cari CSV (selain sample_submission) yang punya kolom id dan baris == n_rows.\n    for p in data_root.rglob(\"*.csv\"):\n        if p.name == \"sample_submission.csv\":\n            continue\n        try:\n            head = _standardize_columns(pd.read_csv(p, nrows=5))\n            if \"id\" not in head.columns:\n                continue\n            # quick row count (read only id col if possible)\n            df0 = _standardize_columns(pd.read_csv(p, usecols=[\"id\"]))\n            if len(df0) == n_rows:\n                return p\n        except Exception:\n            continue\n    return None\n\ndef build_master(df_base: pd.DataFrame, *, has_target: bool, test_mode: bool=False) -> pd.DataFrame:\n    df = df_base.copy()\n\n    # --- ensure tanggal + stasiun_code ---\n    if \"tanggal\" not in df.columns:\n        raise RuntimeError(\"Base df missing 'tanggal'.\")\n    if not np.issubdtype(df[\"tanggal\"].dtype, np.datetime64):\n        df[\"tanggal\"] = parse_date_twopass(df[\"tanggal\"])\n\n    if \"stasiun_code\" not in df.columns:\n        if \"stasiun\" in df.columns:\n            df[\"stasiun_code\"] = _mk_stasiun_code(df[\"stasiun\"])\n        else:\n            df[\"stasiun_code\"] = np.nan\n\n    # --- basic calendar features (safe) ---\n    df[\"year\"] = df[\"tanggal\"].dt.year.astype(\"Int64\")\n    df[\"month\"] = df[\"tanggal\"].dt.month.astype(\"Int64\")\n    df[\"day\"] = df[\"tanggal\"].dt.day.astype(\"Int64\")\n    df[\"dow\"] = df[\"tanggal\"].dt.dayofweek.astype(\"Int64\")\n    df[\"dayofyear\"] = df[\"tanggal\"].dt.dayofyear.astype(\"Int64\")\n\n    # --- holidays (by tanggal) ---\n    hol = df_holiday.copy()\n    if not np.issubdtype(hol[\"tanggal\"].dtype, np.datetime64):\n        hol[\"tanggal\"] = parse_date_twopass(hol[\"tanggal\"])\n    hol = hol.dropna(subset=[\"tanggal\"]).drop_duplicates([\"tanggal\"])\n    df = df.merge(hol, on=\"tanggal\", how=\"left\")\n\n    # --- NDVI (by tanggal + stasiun_code) ---\n    nd = df_ndvi.copy()\n    if \"stasiun_code\" not in nd.columns and \"stasiun\" in nd.columns:\n        nd[\"stasiun_code\"] = _mk_stasiun_code(nd[\"stasiun\"])\n    if not np.issubdtype(nd[\"tanggal\"].dtype, np.datetime64):\n        nd[\"tanggal\"] = parse_date_twopass(nd[\"tanggal\"])\n    nd = nd.dropna(subset=[\"tanggal\", \"stasiun_code\"]).drop_duplicates([\"tanggal\",\"stasiun_code\"])\n    nd = nd[[\"tanggal\",\"stasiun_code\"] + [c for c in nd.columns if c not in [\"tanggal\",\"stasiun\",\"stasiun_code\"]]]\n    df = df.merge(nd, on=[\"tanggal\",\"stasiun_code\"], how=\"left\")\n\n    # --- Weather: station-specific (tanggal + stasiun_code) + global fallback (tanggal) ---\n    wx = df_weather.copy()\n    if not np.issubdtype(wx[\"tanggal\"].dtype, np.datetime64):\n        wx[\"tanggal\"] = parse_date_twopass(wx[\"tanggal\"])\n    if \"weather_code\" in wx.columns:\n        wx[\"weather_code\"] = wx[\"weather_code\"].astype(str).str.strip().str.upper()\n    else:\n        wx[\"weather_code\"] = np.nan\n\n    # station-specific\n    wx_loc = wx.dropna(subset=[\"tanggal\",\"weather_code\"]).copy()\n    wx_loc = wx_loc.rename(columns={\"weather_code\":\"stasiun_code\"})\n    # keep numeric + minimal keys\n    keep_keys = {\"tanggal\",\"stasiun_code\",\"weather_station\"}\n    wx_loc_num = [c for c in wx_loc.columns if c in keep_keys or pd.api.types.is_numeric_dtype(wx_loc[c])]\n    wx_loc = wx_loc[wx_loc_num].drop_duplicates([\"tanggal\",\"stasiun_code\"])\n    wx_loc = _prefix_cols(wx_loc, \"wx_\", keep={\"tanggal\",\"stasiun_code\"})\n    df = df.merge(wx_loc, on=[\"tanggal\",\"stasiun_code\"], how=\"left\")\n\n    # global by date (mean numeric across stations)\n    wx_g = wx.dropna(subset=[\"tanggal\"]).copy()\n    wx_g_num = [c for c in wx_g.columns if pd.api.types.is_numeric_dtype(wx_g[c])]\n    wx_g = wx_g.groupby(\"tanggal\", as_index=False)[wx_g_num].mean(numeric_only=True)\n    wx_g = _prefix_cols(wx_g, \"wxg_\", keep={\"tanggal\"})\n    df = df.merge(wx_g, on=\"tanggal\", how=\"left\")\n\n    # fill local weather NaNs using global fallback for same base variable\n    # (only for columns that exist in both)\n    for c in list(df.columns):\n        if c.startswith(\"wx_\"):\n            base = c.replace(\"wx_\", \"\")\n            cg = \"wxg_\" + base\n            if cg in df.columns:\n                df[c] = df[c].fillna(df[cg])\n\n    # --- Population: total per year (global) ---\n    pop = df_pop.copy()\n    # expected cols: tahun, jumlah_penduduk\n    if \"tahun\" in pop.columns and \"jumlah_penduduk\" in pop.columns:\n        pop_y = pop.groupby(\"tahun\", as_index=False)[\"jumlah_penduduk\"].sum()\n        pop_y = pop_y.rename(columns={\"tahun\":\"year\",\"jumlah_penduduk\":\"pop_total_year\"})\n        df = df.merge(pop_y, on=\"year\", how=\"left\")\n\n    # --- River: global year-month aggregates ---\n    riv = df_river.copy()\n    # standard columns might be: periode_data (year), bulan_sampling, baku_mutu, hasil_pengukuran\n    for cc in [\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"]:\n        if cc in riv.columns:\n            riv[cc] = pd.to_numeric(riv[cc], errors=\"coerce\")\n\n    if {\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"}.issubset(riv.columns):\n        r = riv.dropna(subset=[\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"]).copy()\n        r[\"ratio_to_std\"] = r[\"hasil_pengukuran\"] / (r[\"baku_mutu\"].replace(0, np.nan))\n        r[\"exceed\"] = (r[\"hasil_pengukuran\"] > r[\"baku_mutu\"]).astype(int)\n        r_agg = r.groupby([\"periode_data\",\"bulan_sampling\"], as_index=False).agg(\n            river_exceed_rate=(\"exceed\",\"mean\"),\n            river_ratio_mean=(\"ratio_to_std\",\"mean\"),\n            river_n=(\"exceed\",\"size\"),\n        )\n        r_agg = r_agg.rename(columns={\"periode_data\":\"year\", \"bulan_sampling\":\"month\"})\n        df = df.merge(r_agg, on=[\"year\",\"month\"], how=\"left\")\n\n    # --- cleanup: categorical columns kept as object for CatBoost later ---\n    for c in [\"stasiun\",\"stasiun_code\",\"parameter_pencemar_kritis\",\"day_name\",\"nama_libur\"]:\n        if c in df.columns:\n            df[c] = df[c].astype(str).replace({\"nan\": np.nan, \"None\": np.nan})\n\n    # --- final sanity ---\n    key_cols = [\"tanggal\",\"stasiun_code\"] if \"stasiun_code\" in df.columns else [\"tanggal\"]\n    dup = int(df.duplicated(key_cols).sum())\n    if dup > 0:\n        # keep most complete if duplicates remain\n        df[\"_nn\"] = df.notna().sum(axis=1)\n        idx = df.groupby(key_cols)[\"_nn\"].idxmax()\n        df = df.loc[idx].drop(columns=[\"_nn\"]).reset_index(drop=True)\n\n    if has_target and \"kategori\" in df.columns:\n        df[\"kategori\"] = df[\"kategori\"].astype(str).str.strip()\n\n    return df\n\n# ----------------------------\n# 1) Build TRAIN master\n# ----------------------------\ndf_train_master = build_master(df_train, has_target=True)\nprint(\"df_train_master:\", df_train_master.shape)\nprint(\"train key duplicates:\", int(df_train_master.duplicated([\"tanggal\",\"stasiun_code\"]).sum()) if \"stasiun_code\" in df_train_master.columns else 0)\n\n# ----------------------------\n# 2) Build TEST master (needs id mapping file)\n# ----------------------------\nn_test_expected = len(sub)\n\ntest_path = None\n# If Step 1 created test_candidates, try it first\nif \"test_candidates\" in globals() and isinstance(test_candidates, list) and len(test_candidates) > 0:\n    test_path = Path(test_candidates[0][0])\nelse:\n    test_path = _find_test_mapping_file(DATA_ROOT, n_test_expected)\n\nif test_path is None:\n    print(\"\\n[WARN] Test mapping file (id -> tanggal/stasiun) not found yet.\")\n    print(\"       You can still continue feature engineering + CV on df_train_master.\")\n    df_test_master = None\nelse:\n    print(\"\\nTest mapping file:\", str(test_path))\n    df_test = _standardize_columns(pd.read_csv(test_path))\n    # ensure id exists and length matches submission\n    if \"id\" not in df_test.columns:\n        raise RuntimeError(f\"Test file has no 'id': {test_path}\")\n    if len(df_test) != n_test_expected:\n        raise RuntimeError(f\"Test file rows != submission rows: {len(df_test)} vs {n_test_expected}\")\n\n    # normalize date/station fields\n    if \"tanggal\" in df_test.columns:\n        df_test[\"tanggal\"] = parse_date_twopass(df_test[\"tanggal\"])\n    else:\n        raise RuntimeError(\"Test mapping file missing 'tanggal'.\")\n\n    if \"stasiun_code\" not in df_test.columns:\n        if \"stasiun\" in df_test.columns:\n            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[\"stasiun\"])\n        elif \"stasiun_id\" in df_test.columns:\n            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[\"stasiun_id\"])\n        else:\n            # some datasets store only DKI1..DKI5\n            # try infer from any column containing 'dki'\n            cand = None\n            for c in df_test.columns:\n                if df_test[c].astype(str).str.contains(\"dki\", case=False, na=False).any():\n                    cand = c\n                    break\n            if cand is None:\n                raise RuntimeError(\"Cannot infer station from test mapping file.\")\n            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[cand])\n\n    df_test_master = build_master(df_test, has_target=False, test_mode=True)\n    print(\"df_test_master:\", df_test_master.shape)\n    # keep id for submission\n    if \"id\" not in df_test_master.columns:\n        df_test_master[\"id\"] = df_test[\"id\"].values\n\n# ----------------------------\n# 3) Quick preview\n# ----------------------------\ndisplay(df_train_master.head(3))\nif df_test_master is not None:\n    display(df_test_master.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:45:23.142670Z","iopub.execute_input":"2026-01-30T18:45:23.143117Z","iopub.status.idle":"2026-01-30T18:45:23.691573Z","shell.execute_reply.started":"2026-01-30T18:45:23.143065Z","shell.execute_reply":"2026-01-30T18:45:23.690288Z"}},"outputs":[{"name":"stdout","text":"df_train_master: (13651, 79)\ntrain key duplicates: 0\n\n[WARN] Test mapping file (id -> tanggal/stasiun) not found yet.\n       You can still continue feature engineering + CV on df_train_master.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   periode_data    tanggal               stasiun  pm10  pm25  so2    co    o3   no2   max parameter_pencemar_kritis        kategori                                        source_file  bulan  \\\n0        201001 2010-01-01    DKI1 (Bunderan HI)  60.0   NaN  4.0  73.0  27.0  14.0  73.0                        CO          SEDANG  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n1        201001 2010-01-01  DKI2 (Kelapa Gading)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n2        201001 2010-01-01      DKI3 (Jagakarsa)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n\n  parameter_pencemar_kritis_alt    kategori_alt lokasi_spku stasiun_code  year  month  day  dow  dayofyear  is_holiday_nasional  is_weekend      nama_libur day_name    ndvi  wx_temperature_2m_max_c  \\\n0                            CO          SEDANG         NaN         DKI1  2010      1    1    4          1                    1           0  New Year's Day   Friday  0.2023                     29.4   \n1                           NaN  TIDAK ADA DATA         NaN         DKI2  2010      1    1    4          1                    1           0  New Year's Day   Friday  0.0939                     29.4   \n2                           NaN  TIDAK ADA DATA         NaN         DKI3  2010      1    1    4          1                    1           0  New Year's Day   Friday  0.5332                     29.8   \n\n   wx_temperature_2m_min_c  wx_precipitation_sum_mm  wx_precipitation_hours_h  wx_wind_speed_10m_max_km_h  wx_wind_direction_10m_dominant  wx_shortwave_radiation_sum_mj_m²  wx_temperature_2m_mean_c  \\\n0                     24.4                      4.0                      14.0                        16.0                           246.0                             16.24                      26.6   \n1                     24.6                      5.2                      10.0                        16.5                           255.0                             16.85                      26.7   \n2                     23.7                      4.0                      14.0                        16.0                           246.0                             16.24                      26.1   \n\n   wx_relative_humidity_2m_mean  wx_cloud_cover_mean  wx_surface_pressure_mean_hpa  wx_wind_gusts_10m_max_km_h  wx_winddirection_10m_dominant  wx_relative_humidity_2m_max  \\\n0                          81.0                100.0                        1007.5                        38.2                          246.0                         90.0   \n1                          81.0                100.0                        1007.1                        38.2                          255.0                         89.0   \n2                          85.0                100.0                         999.9                        38.2                          246.0                         94.0   \n\n   wx_relative_humidity_2m_min  wx_cloud_cover_max  wx_cloud_cover_min  wx_wind_gusts_10m_mean_km_h  wx_wind_speed_10m_mean_km_h  wx_wind_gusts_10m_min_km_h  wx_wind_speed_10m_min_km_h  \\\n0                         69.0               100.0                99.0                         21.0                         10.5                        11.9                         6.9   \n1                         69.0               100.0                99.0                         21.1                         10.6                        11.9                         7.4   \n2                         71.0               100.0                99.0                         21.0                         10.5                        11.9                         6.9   \n\n   wx_surface_pressure_max_hpa  wx_surface_pressure_min_hpa wx_weather_station  wxg_temperature_2m_max_c  wxg_temperature_2m_min_c  wxg_precipitation_sum_mm  wxg_precipitation_hours_h  \\\n0                       1009.3                       1005.1    dki1-bundaranhi                     29.58                     24.24                      4.48                       12.4   \n1                       1009.0                       1004.9  dki2-kelapagading                     29.58                     24.24                      4.48                       12.4   \n2                       1001.8                        997.6     dki3-jagakarsa                     29.58                     24.24                      4.48                       12.4   \n\n   wxg_wind_speed_10m_max_km_h  wxg_wind_direction_10m_dominant  wxg_shortwave_radiation_sum_mj_m²  wxg_temperature_2m_mean_c  wxg_relative_humidity_2m_mean  wxg_cloud_cover_mean  \\\n0                         16.2                            249.6                             16.484                      26.48                           82.2                 100.0   \n1                         16.2                            249.6                             16.484                      26.48                           82.2                 100.0   \n2                         16.2                            249.6                             16.484                      26.48                           82.2                 100.0   \n\n   wxg_surface_pressure_mean_hpa  wxg_wind_gusts_10m_max_km_h  wxg_winddirection_10m_dominant  wxg_relative_humidity_2m_max  wxg_relative_humidity_2m_min  wxg_cloud_cover_max  wxg_cloud_cover_min  \\\n0                        1004.98                         38.2                           249.6                          90.8                          69.6                100.0                 99.0   \n1                        1004.98                         38.2                           249.6                          90.8                          69.6                100.0                 99.0   \n2                        1004.98                         38.2                           249.6                          90.8                          69.6                100.0                 99.0   \n\n   wxg_wind_gusts_10m_mean_km_h  wxg_wind_speed_10m_mean_km_h  wxg_wind_gusts_10m_min_km_h  wxg_wind_speed_10m_min_km_h  wxg_surface_pressure_max_hpa  wxg_surface_pressure_min_hpa  pop_total_year  \\\n0                         21.04                         10.54                         11.9                          7.1                       1006.86                        1002.7             NaN   \n1                         21.04                         10.54                         11.9                          7.1                       1006.86                        1002.7             NaN   \n2                         21.04                         10.54                         11.9                          7.1                       1006.86                        1002.7             NaN   \n\n   river_exceed_rate  river_ratio_mean  river_n  \n0                NaN               NaN      NaN  \n1                NaN               NaN      NaN  \n2                NaN               NaN      NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>periode_data</th>\n      <th>tanggal</th>\n      <th>stasiun</th>\n      <th>pm10</th>\n      <th>pm25</th>\n      <th>so2</th>\n      <th>co</th>\n      <th>o3</th>\n      <th>no2</th>\n      <th>max</th>\n      <th>parameter_pencemar_kritis</th>\n      <th>kategori</th>\n      <th>source_file</th>\n      <th>bulan</th>\n      <th>parameter_pencemar_kritis_alt</th>\n      <th>kategori_alt</th>\n      <th>lokasi_spku</th>\n      <th>stasiun_code</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>dow</th>\n      <th>dayofyear</th>\n      <th>is_holiday_nasional</th>\n      <th>is_weekend</th>\n      <th>nama_libur</th>\n      <th>day_name</th>\n      <th>ndvi</th>\n      <th>wx_temperature_2m_max_c</th>\n      <th>wx_temperature_2m_min_c</th>\n      <th>wx_precipitation_sum_mm</th>\n      <th>wx_precipitation_hours_h</th>\n      <th>wx_wind_speed_10m_max_km_h</th>\n      <th>wx_wind_direction_10m_dominant</th>\n      <th>wx_shortwave_radiation_sum_mj_m²</th>\n      <th>wx_temperature_2m_mean_c</th>\n      <th>wx_relative_humidity_2m_mean</th>\n      <th>wx_cloud_cover_mean</th>\n      <th>wx_surface_pressure_mean_hpa</th>\n      <th>wx_wind_gusts_10m_max_km_h</th>\n      <th>wx_winddirection_10m_dominant</th>\n      <th>wx_relative_humidity_2m_max</th>\n      <th>wx_relative_humidity_2m_min</th>\n      <th>wx_cloud_cover_max</th>\n      <th>wx_cloud_cover_min</th>\n      <th>wx_wind_gusts_10m_mean_km_h</th>\n      <th>wx_wind_speed_10m_mean_km_h</th>\n      <th>wx_wind_gusts_10m_min_km_h</th>\n      <th>wx_wind_speed_10m_min_km_h</th>\n      <th>wx_surface_pressure_max_hpa</th>\n      <th>wx_surface_pressure_min_hpa</th>\n      <th>wx_weather_station</th>\n      <th>wxg_temperature_2m_max_c</th>\n      <th>wxg_temperature_2m_min_c</th>\n      <th>wxg_precipitation_sum_mm</th>\n      <th>wxg_precipitation_hours_h</th>\n      <th>wxg_wind_speed_10m_max_km_h</th>\n      <th>wxg_wind_direction_10m_dominant</th>\n      <th>wxg_shortwave_radiation_sum_mj_m²</th>\n      <th>wxg_temperature_2m_mean_c</th>\n      <th>wxg_relative_humidity_2m_mean</th>\n      <th>wxg_cloud_cover_mean</th>\n      <th>wxg_surface_pressure_mean_hpa</th>\n      <th>wxg_wind_gusts_10m_max_km_h</th>\n      <th>wxg_winddirection_10m_dominant</th>\n      <th>wxg_relative_humidity_2m_max</th>\n      <th>wxg_relative_humidity_2m_min</th>\n      <th>wxg_cloud_cover_max</th>\n      <th>wxg_cloud_cover_min</th>\n      <th>wxg_wind_gusts_10m_mean_km_h</th>\n      <th>wxg_wind_speed_10m_mean_km_h</th>\n      <th>wxg_wind_gusts_10m_min_km_h</th>\n      <th>wxg_wind_speed_10m_min_km_h</th>\n      <th>wxg_surface_pressure_max_hpa</th>\n      <th>wxg_surface_pressure_min_hpa</th>\n      <th>pop_total_year</th>\n      <th>river_exceed_rate</th>\n      <th>river_ratio_mean</th>\n      <th>river_n</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>201001</td>\n      <td>2010-01-01</td>\n      <td>DKI1 (Bunderan HI)</td>\n      <td>60.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>73.0</td>\n      <td>27.0</td>\n      <td>14.0</td>\n      <td>73.0</td>\n      <td>CO</td>\n      <td>SEDANG</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>CO</td>\n      <td>SEDANG</td>\n      <td>NaN</td>\n      <td>DKI1</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>New Year's Day</td>\n      <td>Friday</td>\n      <td>0.2023</td>\n      <td>29.4</td>\n      <td>24.4</td>\n      <td>4.0</td>\n      <td>14.0</td>\n      <td>16.0</td>\n      <td>246.0</td>\n      <td>16.24</td>\n      <td>26.6</td>\n      <td>81.0</td>\n      <td>100.0</td>\n      <td>1007.5</td>\n      <td>38.2</td>\n      <td>246.0</td>\n      <td>90.0</td>\n      <td>69.0</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.0</td>\n      <td>10.5</td>\n      <td>11.9</td>\n      <td>6.9</td>\n      <td>1009.3</td>\n      <td>1005.1</td>\n      <td>dki1-bundaranhi</td>\n      <td>29.58</td>\n      <td>24.24</td>\n      <td>4.48</td>\n      <td>12.4</td>\n      <td>16.2</td>\n      <td>249.6</td>\n      <td>16.484</td>\n      <td>26.48</td>\n      <td>82.2</td>\n      <td>100.0</td>\n      <td>1004.98</td>\n      <td>38.2</td>\n      <td>249.6</td>\n      <td>90.8</td>\n      <td>69.6</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.04</td>\n      <td>10.54</td>\n      <td>11.9</td>\n      <td>7.1</td>\n      <td>1006.86</td>\n      <td>1002.7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>201001</td>\n      <td>2010-01-01</td>\n      <td>DKI2 (Kelapa Gading)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>NaN</td>\n      <td>DKI2</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>New Year's Day</td>\n      <td>Friday</td>\n      <td>0.0939</td>\n      <td>29.4</td>\n      <td>24.6</td>\n      <td>5.2</td>\n      <td>10.0</td>\n      <td>16.5</td>\n      <td>255.0</td>\n      <td>16.85</td>\n      <td>26.7</td>\n      <td>81.0</td>\n      <td>100.0</td>\n      <td>1007.1</td>\n      <td>38.2</td>\n      <td>255.0</td>\n      <td>89.0</td>\n      <td>69.0</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.1</td>\n      <td>10.6</td>\n      <td>11.9</td>\n      <td>7.4</td>\n      <td>1009.0</td>\n      <td>1004.9</td>\n      <td>dki2-kelapagading</td>\n      <td>29.58</td>\n      <td>24.24</td>\n      <td>4.48</td>\n      <td>12.4</td>\n      <td>16.2</td>\n      <td>249.6</td>\n      <td>16.484</td>\n      <td>26.48</td>\n      <td>82.2</td>\n      <td>100.0</td>\n      <td>1004.98</td>\n      <td>38.2</td>\n      <td>249.6</td>\n      <td>90.8</td>\n      <td>69.6</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.04</td>\n      <td>10.54</td>\n      <td>11.9</td>\n      <td>7.1</td>\n      <td>1006.86</td>\n      <td>1002.7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>201001</td>\n      <td>2010-01-01</td>\n      <td>DKI3 (Jagakarsa)</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>TIDAK ADA DATA</td>\n      <td>NaN</td>\n      <td>DKI3</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>New Year's Day</td>\n      <td>Friday</td>\n      <td>0.5332</td>\n      <td>29.8</td>\n      <td>23.7</td>\n      <td>4.0</td>\n      <td>14.0</td>\n      <td>16.0</td>\n      <td>246.0</td>\n      <td>16.24</td>\n      <td>26.1</td>\n      <td>85.0</td>\n      <td>100.0</td>\n      <td>999.9</td>\n      <td>38.2</td>\n      <td>246.0</td>\n      <td>94.0</td>\n      <td>71.0</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.0</td>\n      <td>10.5</td>\n      <td>11.9</td>\n      <td>6.9</td>\n      <td>1001.8</td>\n      <td>997.6</td>\n      <td>dki3-jagakarsa</td>\n      <td>29.58</td>\n      <td>24.24</td>\n      <td>4.48</td>\n      <td>12.4</td>\n      <td>16.2</td>\n      <td>249.6</td>\n      <td>16.484</td>\n      <td>26.48</td>\n      <td>82.2</td>\n      <td>100.0</td>\n      <td>1004.98</td>\n      <td>38.2</td>\n      <td>249.6</td>\n      <td>90.8</td>\n      <td>69.6</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.04</td>\n      <td>10.54</td>\n      <td>11.9</td>\n      <td>7.1</td>\n      <td>1006.86</td>\n      <td>1002.7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Feature Engineering (Time-Series + Calendar + Robustness)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 3 — Feature Engineering (Time-Series + Calendar + Robustness) (ONE CELL)\n# Requires:\n#   df_train_master  (from Step 2)\n#   df_test_master   (optional; from Step 2)\n# Produces:\n#   df_train_fe, df_test_fe\n# Notes:\n# - Leakage-safe: all rolling features are computed on SHIFTED values (past only).\n# - Station-aware: computed per stasiun_code.\n# - Keeps CatBoost-friendly categoricals as object; numeric features stay numeric.\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\n\nif \"df_train_master\" not in globals():\n    raise RuntimeError(\"Missing df_train_master. Run Step 2 first.\")\n\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # robust calendar\n    df[\"year\"] = df[\"tanggal\"].dt.year.astype(\"Int64\")\n    df[\"month\"] = df[\"tanggal\"].dt.month.astype(\"Int64\")\n    df[\"day\"] = df[\"tanggal\"].dt.day.astype(\"Int64\")\n    df[\"dow\"] = df[\"tanggal\"].dt.dayofweek.astype(\"Int64\")\n    df[\"dayofyear\"] = df[\"tanggal\"].dt.dayofyear.astype(\"Int64\")\n\n    # cyclic encoding (helps CatBoost a bit; safe)\n    doy = df[\"dayofyear\"].astype(float)\n    df[\"doy_sin\"] = np.sin(2 * np.pi * doy / 365.25)\n    df[\"doy_cos\"] = np.cos(2 * np.pi * doy / 365.25)\n    mon = df[\"month\"].astype(float)\n    df[\"mon_sin\"] = np.sin(2 * np.pi * mon / 12.0)\n    df[\"mon_cos\"] = np.cos(2 * np.pi * mon / 12.0)\n\n    # weekend if missing\n    if \"is_weekend\" not in df.columns:\n        df[\"is_weekend\"] = (df[\"dow\"].isin([5, 6])).astype(int)\n\n    return df\n\ndef add_station_lag_rolling(\n    df: pd.DataFrame,\n    group_col: str = \"stasiun_code\",\n    base_cols = (\"pm10\",\"pm25\",\"so2\",\"co\",\"o3\",\"no2\",\"max\"),\n    lags = (1,2,3,7,14),\n    windows = (3,7,14,30),\n) -> pd.DataFrame:\n    df = df.copy()\n\n    # ensure order\n    df = df.sort_values([group_col, \"tanggal\"]).reset_index(drop=True)\n\n    # only keep existing numeric columns\n    base_cols = [c for c in base_cols if c in df.columns]\n    if not base_cols:\n        raise RuntimeError(\"No base pollutant columns found for lag/rolling.\")\n\n    g = df.groupby(group_col, sort=False)\n\n    # lags\n    for c in base_cols:\n        for L in lags:\n            df[f\"{c}_lag{L}\"] = g[c].shift(L)\n\n    # rolling on shifted series (leakage-safe)\n    for c in base_cols:\n        s = g[c].shift(1)  # only past values included\n        for w in windows:\n            df[f\"{c}_rmean{w}\"] = s.rolling(w, min_periods=max(2, w//3)).mean().reset_index(level=0, drop=True)\n            df[f\"{c}_rstd{w}\"]  = s.rolling(w, min_periods=max(2, w//3)).std().reset_index(level=0, drop=True)\n\n    # deltas (based on lags; safe)\n    for c in base_cols:\n        if f\"{c}_lag1\" in df.columns and f\"{c}_lag2\" in df.columns:\n            df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n        if f\"{c}_lag1\" in df.columns and f\"{c}_rmean7\" in df.columns:\n            df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n\n    return df\n\ndef add_weather_interactions(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # pick weather columns (prefer local wx_, fallback already filled at Step 2)\n    # examples based on your weather columns\n    cand = {\n        \"wind_mean\": [\"wx_wind_speed_10m_mean_km_h\", \"wxg_wind_speed_10m_mean_km_h\", \"wind_speed_10m_mean_km_h\"],\n        \"precip_sum\": [\"wx_precipitation_sum_mm\", \"wxg_precipitation_sum_mm\", \"precipitation_sum_mm\"],\n        \"rad_sum\": [\"wx_shortwave_radiation_sum_mj_m²\", \"wxg_shortwave_radiation_sum_mj_m²\", \"shortwave_radiation_sum_mj_m²\"],\n        \"rh_mean\": [\"wx_relative_humidity_2m_mean\", \"wxg_relative_humidity_2m_mean\", \"relative_humidity_2m_mean\"],\n        \"temp_mean\": [\"wx_temperature_2m_mean_c\", \"wxg_temperature_2m_mean_c\", \"temperature_2m_mean_c\"],\n    }\n    def pick(cols):\n        for c in cols:\n            if c in df.columns:\n                return c\n        return None\n\n    wind = pick(cand[\"wind_mean\"])\n    prec = pick(cand[\"precip_sum\"])\n    rad  = pick(cand[\"rad_sum\"])\n    rh   = pick(cand[\"rh_mean\"])\n    tmp  = pick(cand[\"temp_mean\"])\n\n    # build a few robust interactions using lag1 (already leakage-safe)\n    if \"pm25_lag1\" in df.columns and wind is not None:\n        df[\"pm25_lag1_x_wind\"] = df[\"pm25_lag1\"] * df[wind]\n    if \"pm10_lag1\" in df.columns and wind is not None:\n        df[\"pm10_lag1_x_wind\"] = df[\"pm10_lag1\"] * df[wind]\n    if \"o3_lag1\" in df.columns and rad is not None:\n        df[\"o3_lag1_x_rad\"] = df[\"o3_lag1\"] * df[rad]\n    if \"pm25_lag1\" in df.columns and prec is not None:\n        df[\"pm25_lag1_div_prec\"] = df[\"pm25_lag1\"] / (df[prec].fillna(0) + 1.0)\n    if \"co_lag1\" in df.columns and rh is not None:\n        df[\"co_lag1_x_rh\"] = df[\"co_lag1\"] * df[rh]\n    if \"pm25_lag1\" in df.columns and tmp is not None:\n        df[\"pm25_lag1_x_temp\"] = df[\"pm25_lag1\"] * df[tmp]\n\n    return df\n\ndef finalize_types_for_catboost(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n    # keep these as categorical (object)\n    cat_cols = []\n    for c in [\"stasiun\", \"stasiun_code\", \"parameter_pencemar_kritis\", \"day_name\", \"nama_libur\", \"weather_station\"]:\n        if c in df.columns:\n            cat_cols.append(c)\n    for c in cat_cols:\n        df[c] = df[c].astype(\"object\")\n\n    # cast flags to int\n    for c in [\"is_weekend\", \"is_holiday_nasional\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n\n    return df\n\ndef build_fe(df: pd.DataFrame) -> pd.DataFrame:\n    df = add_time_features(df)\n    df = add_station_lag_rolling(df)\n    df = add_weather_interactions(df)\n    df = finalize_types_for_catboost(df)\n    return df\n\n# ---- Build FE for train and test (if available) ----\ndf_train_fe = build_fe(df_train_master)\n\nif \"df_test_master\" in globals() and df_test_master is not None:\n    df_test_fe = build_fe(df_test_master)\nelse:\n    df_test_fe = None\n\n# ---- Sanity: no leakage from current-day target directly ----\n# keep raw current-day pollutant columns; CatBoost can use them too, but lags are key.\n# ensure we do NOT have rolling computed without shift (we don't).\n\nprint(\"df_train_fe:\", df_train_fe.shape)\nif df_test_fe is not None:\n    print(\"df_test_fe :\", df_test_fe.shape)\n\n# Quick missing rate on engineered features\ntop_miss = (df_train_fe.isna().mean().sort_values(ascending=False).head(15) * 100).round(2)\nprint(\"\\nTop missing% (train_fe):\")\nprint(top_miss.to_string())\n\ndisplay(df_train_fe.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:45:23.693149Z","iopub.execute_input":"2026-01-30T18:45:23.693567Z","iopub.status.idle":"2026-01-30T18:45:24.036449Z","shell.execute_reply.started":"2026-01-30T18:45:23.693526Z","shell.execute_reply":"2026-01-30T18:45:24.035367Z"}},"outputs":[{"name":"stdout","text":"df_train_fe: (13651, 194)\n\nTop missing% (train_fe):\nbulan                 100.00\nriver_exceed_rate     100.00\nriver_n               100.00\nriver_ratio_mean      100.00\nnama_libur             95.69\nndvi                   95.04\nlokasi_spku            78.96\npm25_lag14             76.65\npm25_d12               76.59\npm25_lag7              76.41\npm25_lag3              76.27\npm25_d1_rm7            76.26\npm25_lag2              76.24\npm25_lag1_div_prec     76.20\npm25_lag1_x_wind       76.20\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n/tmp/ipykernel_55/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n/tmp/ipykernel_55/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n/tmp/ipykernel_55/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n/tmp/ipykernel_55/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n/tmp/ipykernel_55/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n/tmp/ipykernel_55/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n/tmp/ipykernel_55/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n/tmp/ipykernel_55/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   periode_data    tanggal             stasiun  pm10  pm25  so2    co    o3   no2   max parameter_pencemar_kritis kategori                                        source_file  bulan  \\\n0        201001 2010-01-01  DKI1 (Bunderan HI)  60.0   NaN  4.0  73.0  27.0  14.0  73.0                        CO   SEDANG  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n1        201001 2010-01-02  DKI1 (Bunderan HI)  32.0   NaN  2.0  16.0  33.0   9.0  33.0                        O3     BAIK  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n2        201001 2010-01-03  DKI1 (Bunderan HI)  27.0   NaN  2.0  19.0  20.0   9.0  27.0                      PM10     BAIK  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n\n  parameter_pencemar_kritis_alt kategori_alt lokasi_spku stasiun_code  year  month  day  dow  dayofyear  is_holiday_nasional  is_weekend      nama_libur  day_name    ndvi  wx_temperature_2m_max_c  \\\n0                            CO       SEDANG         NaN         DKI1  2010      1    1    4          1                    1           0  New Year's Day    Friday  0.2023                     29.4   \n1                            O3         BAIK         NaN         DKI1  2010      1    2    5          2                    0           0             NaN  Saturday     NaN                     28.9   \n2                          PM10         BAIK         NaN         DKI1  2010      1    3    6          3                    0           0             NaN    Sunday     NaN                     31.4   \n\n   wx_temperature_2m_min_c  wx_precipitation_sum_mm  wx_precipitation_hours_h  wx_wind_speed_10m_max_km_h  wx_wind_direction_10m_dominant  wx_shortwave_radiation_sum_mj_m²  wx_temperature_2m_mean_c  \\\n0                     24.4                      4.0                      14.0                        16.0                           246.0                             16.24                      26.6   \n1                     24.2                      6.9                      14.0                         9.5                           260.0                             13.01                      26.2   \n2                     24.9                     11.2                       6.0                         9.4                           224.0                             23.89                      27.1   \n\n   wx_relative_humidity_2m_mean  wx_cloud_cover_mean  wx_surface_pressure_mean_hpa  wx_wind_gusts_10m_max_km_h  wx_winddirection_10m_dominant  wx_relative_humidity_2m_max  \\\n0                          81.0                100.0                        1007.5                        38.2                          246.0                         90.0   \n1                          85.0                 99.0                        1010.1                        22.0                          260.0                         95.0   \n2                          85.0                 93.0                        1009.9                        21.2                          224.0                         95.0   \n\n   wx_relative_humidity_2m_min  wx_cloud_cover_max  wx_cloud_cover_min  wx_wind_gusts_10m_mean_km_h  wx_wind_speed_10m_mean_km_h  wx_wind_gusts_10m_min_km_h  wx_wind_speed_10m_min_km_h  \\\n0                         69.0               100.0                99.0                         21.0                         10.5                        11.9                         6.9   \n1                         72.0               100.0                94.0                         13.7                          6.0                         8.6                         2.3   \n2                         70.0               100.0                28.0                         15.7                          5.7                         8.3                         1.6   \n\n   wx_surface_pressure_max_hpa  wx_surface_pressure_min_hpa wx_weather_station  wxg_temperature_2m_max_c  wxg_temperature_2m_min_c  wxg_precipitation_sum_mm  wxg_precipitation_hours_h  \\\n0                       1009.3                       1005.1    dki1-bundaranhi                     29.58                     24.24                      4.48                       12.4   \n1                       1011.9                       1007.4    dki1-bundaranhi                     29.02                     23.88                      7.14                       14.4   \n2                       1012.2                       1007.0    dki1-bundaranhi                     31.46                     24.34                     12.16                        6.8   \n\n   wxg_wind_speed_10m_max_km_h  wxg_wind_direction_10m_dominant  wxg_shortwave_radiation_sum_mj_m²  wxg_temperature_2m_mean_c  wxg_relative_humidity_2m_mean  wxg_cloud_cover_mean  \\\n0                        16.20                            249.6                             16.484                      26.48                           82.2                 100.0   \n1                         9.50                            264.8                             12.666                      26.08                           85.8                  99.0   \n2                         9.12                            218.0                             23.986                      26.98                           85.6                  93.8   \n\n   wxg_surface_pressure_mean_hpa  wxg_wind_gusts_10m_max_km_h  wxg_winddirection_10m_dominant  wxg_relative_humidity_2m_max  wxg_relative_humidity_2m_min  wxg_cloud_cover_max  wxg_cloud_cover_min  \\\n0                        1004.98                        38.20                           249.6                          90.8                          69.6                100.0                 99.0   \n1                        1007.68                        23.28                           264.8                          95.6                          72.0                100.0                 95.2   \n2                        1007.48                        22.24                           218.0                          96.0                          70.4                100.0                 40.4   \n\n   wxg_wind_gusts_10m_mean_km_h  wxg_wind_speed_10m_mean_km_h  wxg_wind_gusts_10m_min_km_h  wxg_wind_speed_10m_min_km_h  wxg_surface_pressure_max_hpa  wxg_surface_pressure_min_hpa  pop_total_year  \\\n0                         21.04                         10.54                        11.90                         7.10                       1006.86                       1002.70             NaN   \n1                         13.90                          6.08                         8.76                         1.94                       1009.44                       1005.04             NaN   \n2                         15.50                          5.58                         8.42                         1.68                       1009.76                       1004.78             NaN   \n\n   river_exceed_rate  river_ratio_mean  river_n   doy_sin   doy_cos  mon_sin   mon_cos  pm10_lag1  pm10_lag2  pm10_lag3  pm10_lag7  pm10_lag14  pm25_lag1  pm25_lag2  pm25_lag3  pm25_lag7  \\\n0                NaN               NaN      NaN  0.017202  0.999852      0.5  0.866025        NaN        NaN        NaN        NaN         NaN        NaN        NaN        NaN        NaN   \n1                NaN               NaN      NaN  0.034398  0.999408      0.5  0.866025       60.0        NaN        NaN        NaN         NaN        NaN        NaN        NaN        NaN   \n2                NaN               NaN      NaN  0.051584  0.998669      0.5  0.866025       32.0       60.0        NaN        NaN         NaN        NaN        NaN        NaN        NaN   \n\n   pm25_lag14  so2_lag1  so2_lag2  so2_lag3  so2_lag7  so2_lag14  co_lag1  co_lag2  co_lag3  co_lag7  co_lag14  o3_lag1  o3_lag2  o3_lag3  o3_lag7  o3_lag14  no2_lag1  no2_lag2  no2_lag3  no2_lag7  \\\n0         NaN       NaN       NaN       NaN       NaN        NaN      NaN      NaN      NaN      NaN       NaN      NaN      NaN      NaN      NaN       NaN       NaN       NaN       NaN       NaN   \n1         NaN       4.0       NaN       NaN       NaN        NaN     73.0      NaN      NaN      NaN       NaN     27.0      NaN      NaN      NaN       NaN      14.0       NaN       NaN       NaN   \n2         NaN       2.0       4.0       NaN       NaN        NaN     16.0     73.0      NaN      NaN       NaN     33.0     27.0      NaN      NaN       NaN       9.0      14.0       NaN       NaN   \n\n   no2_lag14  max_lag1  max_lag2  max_lag3  max_lag7  max_lag14  pm10_rmean3  pm10_rstd3  pm10_rmean7  pm10_rstd7  pm10_rmean14  pm10_rstd14  pm10_rmean30  pm10_rstd30  pm25_rmean3  pm25_rstd3  \\\n0        NaN       NaN       NaN       NaN       NaN        NaN          NaN         NaN          NaN         NaN           NaN          NaN           NaN          NaN          NaN         NaN   \n1        NaN      73.0       NaN       NaN       NaN        NaN          NaN         NaN          NaN         NaN           NaN          NaN           NaN          NaN          NaN         NaN   \n2        NaN      33.0      73.0       NaN       NaN        NaN         46.0    19.79899         46.0    19.79899           NaN          NaN           NaN          NaN          NaN         NaN   \n\n   pm25_rmean7  pm25_rstd7  pm25_rmean14  pm25_rstd14  pm25_rmean30  pm25_rstd30  so2_rmean3  so2_rstd3  so2_rmean7  so2_rstd7  so2_rmean14  so2_rstd14  so2_rmean30  so2_rstd30  co_rmean3  \\\n0          NaN         NaN           NaN          NaN           NaN          NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN        NaN   \n1          NaN         NaN           NaN          NaN           NaN          NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN        NaN   \n2          NaN         NaN           NaN          NaN           NaN          NaN         3.0   1.414214         3.0   1.414214          NaN         NaN          NaN         NaN       44.5   \n\n    co_rstd3  co_rmean7   co_rstd7  co_rmean14  co_rstd14  co_rmean30  co_rstd30  o3_rmean3  o3_rstd3  o3_rmean7  o3_rstd7  o3_rmean14  o3_rstd14  o3_rmean30  o3_rstd30  no2_rmean3  no2_rstd3  \\\n0        NaN        NaN        NaN         NaN        NaN         NaN        NaN        NaN       NaN        NaN       NaN         NaN        NaN         NaN        NaN         NaN        NaN   \n1        NaN        NaN        NaN         NaN        NaN         NaN        NaN        NaN       NaN        NaN       NaN         NaN        NaN         NaN        NaN         NaN        NaN   \n2  40.305087       44.5  40.305087         NaN        NaN         NaN        NaN       30.0  4.242641       30.0  4.242641         NaN        NaN         NaN        NaN        11.5   3.535534   \n\n   no2_rmean7  no2_rstd7  no2_rmean14  no2_rstd14  no2_rmean30  no2_rstd30  max_rmean3  max_rstd3  max_rmean7  max_rstd7  max_rmean14  max_rstd14  max_rmean30  max_rstd30  pm10_d12  pm10_d1_rm7  \\\n0         NaN        NaN          NaN         NaN          NaN         NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN       NaN          NaN   \n1         NaN        NaN          NaN         NaN          NaN         NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN       NaN          NaN   \n2        11.5   3.535534          NaN         NaN          NaN         NaN        53.0  28.284271        53.0  28.284271          NaN         NaN          NaN         NaN     -28.0        -14.0   \n\n   pm25_d12  pm25_d1_rm7  so2_d12  so2_d1_rm7  co_d12  co_d1_rm7  o3_d12  o3_d1_rm7  no2_d12  no2_d1_rm7  max_d12  max_d1_rm7  pm25_lag1_x_wind  pm10_lag1_x_wind  o3_lag1_x_rad  pm25_lag1_div_prec  \\\n0       NaN          NaN      NaN         NaN     NaN        NaN     NaN        NaN      NaN         NaN      NaN         NaN               NaN               NaN            NaN                 NaN   \n1       NaN          NaN      NaN         NaN     NaN        NaN     NaN        NaN      NaN         NaN      NaN         NaN               NaN             360.0         351.27                 NaN   \n2       NaN          NaN     -2.0        -1.0   -57.0      -28.5     6.0        3.0     -5.0        -2.5    -40.0       -20.0               NaN             182.4         788.37                 NaN   \n\n   co_lag1_x_rh  pm25_lag1_x_temp  \n0           NaN               NaN  \n1        6205.0               NaN  \n2        1360.0               NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>periode_data</th>\n      <th>tanggal</th>\n      <th>stasiun</th>\n      <th>pm10</th>\n      <th>pm25</th>\n      <th>so2</th>\n      <th>co</th>\n      <th>o3</th>\n      <th>no2</th>\n      <th>max</th>\n      <th>parameter_pencemar_kritis</th>\n      <th>kategori</th>\n      <th>source_file</th>\n      <th>bulan</th>\n      <th>parameter_pencemar_kritis_alt</th>\n      <th>kategori_alt</th>\n      <th>lokasi_spku</th>\n      <th>stasiun_code</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>dow</th>\n      <th>dayofyear</th>\n      <th>is_holiday_nasional</th>\n      <th>is_weekend</th>\n      <th>nama_libur</th>\n      <th>day_name</th>\n      <th>ndvi</th>\n      <th>wx_temperature_2m_max_c</th>\n      <th>wx_temperature_2m_min_c</th>\n      <th>wx_precipitation_sum_mm</th>\n      <th>wx_precipitation_hours_h</th>\n      <th>wx_wind_speed_10m_max_km_h</th>\n      <th>wx_wind_direction_10m_dominant</th>\n      <th>wx_shortwave_radiation_sum_mj_m²</th>\n      <th>wx_temperature_2m_mean_c</th>\n      <th>wx_relative_humidity_2m_mean</th>\n      <th>wx_cloud_cover_mean</th>\n      <th>wx_surface_pressure_mean_hpa</th>\n      <th>wx_wind_gusts_10m_max_km_h</th>\n      <th>wx_winddirection_10m_dominant</th>\n      <th>wx_relative_humidity_2m_max</th>\n      <th>wx_relative_humidity_2m_min</th>\n      <th>wx_cloud_cover_max</th>\n      <th>wx_cloud_cover_min</th>\n      <th>wx_wind_gusts_10m_mean_km_h</th>\n      <th>wx_wind_speed_10m_mean_km_h</th>\n      <th>wx_wind_gusts_10m_min_km_h</th>\n      <th>wx_wind_speed_10m_min_km_h</th>\n      <th>wx_surface_pressure_max_hpa</th>\n      <th>wx_surface_pressure_min_hpa</th>\n      <th>wx_weather_station</th>\n      <th>wxg_temperature_2m_max_c</th>\n      <th>wxg_temperature_2m_min_c</th>\n      <th>wxg_precipitation_sum_mm</th>\n      <th>wxg_precipitation_hours_h</th>\n      <th>wxg_wind_speed_10m_max_km_h</th>\n      <th>wxg_wind_direction_10m_dominant</th>\n      <th>wxg_shortwave_radiation_sum_mj_m²</th>\n      <th>wxg_temperature_2m_mean_c</th>\n      <th>wxg_relative_humidity_2m_mean</th>\n      <th>wxg_cloud_cover_mean</th>\n      <th>wxg_surface_pressure_mean_hpa</th>\n      <th>wxg_wind_gusts_10m_max_km_h</th>\n      <th>wxg_winddirection_10m_dominant</th>\n      <th>wxg_relative_humidity_2m_max</th>\n      <th>wxg_relative_humidity_2m_min</th>\n      <th>wxg_cloud_cover_max</th>\n      <th>wxg_cloud_cover_min</th>\n      <th>wxg_wind_gusts_10m_mean_km_h</th>\n      <th>wxg_wind_speed_10m_mean_km_h</th>\n      <th>wxg_wind_gusts_10m_min_km_h</th>\n      <th>wxg_wind_speed_10m_min_km_h</th>\n      <th>wxg_surface_pressure_max_hpa</th>\n      <th>wxg_surface_pressure_min_hpa</th>\n      <th>pop_total_year</th>\n      <th>river_exceed_rate</th>\n      <th>river_ratio_mean</th>\n      <th>river_n</th>\n      <th>doy_sin</th>\n      <th>doy_cos</th>\n      <th>mon_sin</th>\n      <th>mon_cos</th>\n      <th>pm10_lag1</th>\n      <th>pm10_lag2</th>\n      <th>pm10_lag3</th>\n      <th>pm10_lag7</th>\n      <th>pm10_lag14</th>\n      <th>pm25_lag1</th>\n      <th>pm25_lag2</th>\n      <th>pm25_lag3</th>\n      <th>pm25_lag7</th>\n      <th>pm25_lag14</th>\n      <th>so2_lag1</th>\n      <th>so2_lag2</th>\n      <th>so2_lag3</th>\n      <th>so2_lag7</th>\n      <th>so2_lag14</th>\n      <th>co_lag1</th>\n      <th>co_lag2</th>\n      <th>co_lag3</th>\n      <th>co_lag7</th>\n      <th>co_lag14</th>\n      <th>o3_lag1</th>\n      <th>o3_lag2</th>\n      <th>o3_lag3</th>\n      <th>o3_lag7</th>\n      <th>o3_lag14</th>\n      <th>no2_lag1</th>\n      <th>no2_lag2</th>\n      <th>no2_lag3</th>\n      <th>no2_lag7</th>\n      <th>no2_lag14</th>\n      <th>max_lag1</th>\n      <th>max_lag2</th>\n      <th>max_lag3</th>\n      <th>max_lag7</th>\n      <th>max_lag14</th>\n      <th>pm10_rmean3</th>\n      <th>pm10_rstd3</th>\n      <th>pm10_rmean7</th>\n      <th>pm10_rstd7</th>\n      <th>pm10_rmean14</th>\n      <th>pm10_rstd14</th>\n      <th>pm10_rmean30</th>\n      <th>pm10_rstd30</th>\n      <th>pm25_rmean3</th>\n      <th>pm25_rstd3</th>\n      <th>pm25_rmean7</th>\n      <th>pm25_rstd7</th>\n      <th>pm25_rmean14</th>\n      <th>pm25_rstd14</th>\n      <th>pm25_rmean30</th>\n      <th>pm25_rstd30</th>\n      <th>so2_rmean3</th>\n      <th>so2_rstd3</th>\n      <th>so2_rmean7</th>\n      <th>so2_rstd7</th>\n      <th>so2_rmean14</th>\n      <th>so2_rstd14</th>\n      <th>so2_rmean30</th>\n      <th>so2_rstd30</th>\n      <th>co_rmean3</th>\n      <th>co_rstd3</th>\n      <th>co_rmean7</th>\n      <th>co_rstd7</th>\n      <th>co_rmean14</th>\n      <th>co_rstd14</th>\n      <th>co_rmean30</th>\n      <th>co_rstd30</th>\n      <th>o3_rmean3</th>\n      <th>o3_rstd3</th>\n      <th>o3_rmean7</th>\n      <th>o3_rstd7</th>\n      <th>o3_rmean14</th>\n      <th>o3_rstd14</th>\n      <th>o3_rmean30</th>\n      <th>o3_rstd30</th>\n      <th>no2_rmean3</th>\n      <th>no2_rstd3</th>\n      <th>no2_rmean7</th>\n      <th>no2_rstd7</th>\n      <th>no2_rmean14</th>\n      <th>no2_rstd14</th>\n      <th>no2_rmean30</th>\n      <th>no2_rstd30</th>\n      <th>max_rmean3</th>\n      <th>max_rstd3</th>\n      <th>max_rmean7</th>\n      <th>max_rstd7</th>\n      <th>max_rmean14</th>\n      <th>max_rstd14</th>\n      <th>max_rmean30</th>\n      <th>max_rstd30</th>\n      <th>pm10_d12</th>\n      <th>pm10_d1_rm7</th>\n      <th>pm25_d12</th>\n      <th>pm25_d1_rm7</th>\n      <th>so2_d12</th>\n      <th>so2_d1_rm7</th>\n      <th>co_d12</th>\n      <th>co_d1_rm7</th>\n      <th>o3_d12</th>\n      <th>o3_d1_rm7</th>\n      <th>no2_d12</th>\n      <th>no2_d1_rm7</th>\n      <th>max_d12</th>\n      <th>max_d1_rm7</th>\n      <th>pm25_lag1_x_wind</th>\n      <th>pm10_lag1_x_wind</th>\n      <th>o3_lag1_x_rad</th>\n      <th>pm25_lag1_div_prec</th>\n      <th>co_lag1_x_rh</th>\n      <th>pm25_lag1_x_temp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>201001</td>\n      <td>2010-01-01</td>\n      <td>DKI1 (Bunderan HI)</td>\n      <td>60.0</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>73.0</td>\n      <td>27.0</td>\n      <td>14.0</td>\n      <td>73.0</td>\n      <td>CO</td>\n      <td>SEDANG</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>CO</td>\n      <td>SEDANG</td>\n      <td>NaN</td>\n      <td>DKI1</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>New Year's Day</td>\n      <td>Friday</td>\n      <td>0.2023</td>\n      <td>29.4</td>\n      <td>24.4</td>\n      <td>4.0</td>\n      <td>14.0</td>\n      <td>16.0</td>\n      <td>246.0</td>\n      <td>16.24</td>\n      <td>26.6</td>\n      <td>81.0</td>\n      <td>100.0</td>\n      <td>1007.5</td>\n      <td>38.2</td>\n      <td>246.0</td>\n      <td>90.0</td>\n      <td>69.0</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.0</td>\n      <td>10.5</td>\n      <td>11.9</td>\n      <td>6.9</td>\n      <td>1009.3</td>\n      <td>1005.1</td>\n      <td>dki1-bundaranhi</td>\n      <td>29.58</td>\n      <td>24.24</td>\n      <td>4.48</td>\n      <td>12.4</td>\n      <td>16.20</td>\n      <td>249.6</td>\n      <td>16.484</td>\n      <td>26.48</td>\n      <td>82.2</td>\n      <td>100.0</td>\n      <td>1004.98</td>\n      <td>38.20</td>\n      <td>249.6</td>\n      <td>90.8</td>\n      <td>69.6</td>\n      <td>100.0</td>\n      <td>99.0</td>\n      <td>21.04</td>\n      <td>10.54</td>\n      <td>11.90</td>\n      <td>7.10</td>\n      <td>1006.86</td>\n      <td>1002.70</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.017202</td>\n      <td>0.999852</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>201001</td>\n      <td>2010-01-02</td>\n      <td>DKI1 (Bunderan HI)</td>\n      <td>32.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>16.0</td>\n      <td>33.0</td>\n      <td>9.0</td>\n      <td>33.0</td>\n      <td>O3</td>\n      <td>BAIK</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>O3</td>\n      <td>BAIK</td>\n      <td>NaN</td>\n      <td>DKI1</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Saturday</td>\n      <td>NaN</td>\n      <td>28.9</td>\n      <td>24.2</td>\n      <td>6.9</td>\n      <td>14.0</td>\n      <td>9.5</td>\n      <td>260.0</td>\n      <td>13.01</td>\n      <td>26.2</td>\n      <td>85.0</td>\n      <td>99.0</td>\n      <td>1010.1</td>\n      <td>22.0</td>\n      <td>260.0</td>\n      <td>95.0</td>\n      <td>72.0</td>\n      <td>100.0</td>\n      <td>94.0</td>\n      <td>13.7</td>\n      <td>6.0</td>\n      <td>8.6</td>\n      <td>2.3</td>\n      <td>1011.9</td>\n      <td>1007.4</td>\n      <td>dki1-bundaranhi</td>\n      <td>29.02</td>\n      <td>23.88</td>\n      <td>7.14</td>\n      <td>14.4</td>\n      <td>9.50</td>\n      <td>264.8</td>\n      <td>12.666</td>\n      <td>26.08</td>\n      <td>85.8</td>\n      <td>99.0</td>\n      <td>1007.68</td>\n      <td>23.28</td>\n      <td>264.8</td>\n      <td>95.6</td>\n      <td>72.0</td>\n      <td>100.0</td>\n      <td>95.2</td>\n      <td>13.90</td>\n      <td>6.08</td>\n      <td>8.76</td>\n      <td>1.94</td>\n      <td>1009.44</td>\n      <td>1005.04</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.034398</td>\n      <td>0.999408</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n      <td>60.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>73.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>27.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>73.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>360.0</td>\n      <td>351.27</td>\n      <td>NaN</td>\n      <td>6205.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>201001</td>\n      <td>2010-01-03</td>\n      <td>DKI1 (Bunderan HI)</td>\n      <td>27.0</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>19.0</td>\n      <td>20.0</td>\n      <td>9.0</td>\n      <td>27.0</td>\n      <td>PM10</td>\n      <td>BAIK</td>\n      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n      <td>NaN</td>\n      <td>PM10</td>\n      <td>BAIK</td>\n      <td>NaN</td>\n      <td>DKI1</td>\n      <td>2010</td>\n      <td>1</td>\n      <td>3</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Sunday</td>\n      <td>NaN</td>\n      <td>31.4</td>\n      <td>24.9</td>\n      <td>11.2</td>\n      <td>6.0</td>\n      <td>9.4</td>\n      <td>224.0</td>\n      <td>23.89</td>\n      <td>27.1</td>\n      <td>85.0</td>\n      <td>93.0</td>\n      <td>1009.9</td>\n      <td>21.2</td>\n      <td>224.0</td>\n      <td>95.0</td>\n      <td>70.0</td>\n      <td>100.0</td>\n      <td>28.0</td>\n      <td>15.7</td>\n      <td>5.7</td>\n      <td>8.3</td>\n      <td>1.6</td>\n      <td>1012.2</td>\n      <td>1007.0</td>\n      <td>dki1-bundaranhi</td>\n      <td>31.46</td>\n      <td>24.34</td>\n      <td>12.16</td>\n      <td>6.8</td>\n      <td>9.12</td>\n      <td>218.0</td>\n      <td>23.986</td>\n      <td>26.98</td>\n      <td>85.6</td>\n      <td>93.8</td>\n      <td>1007.48</td>\n      <td>22.24</td>\n      <td>218.0</td>\n      <td>96.0</td>\n      <td>70.4</td>\n      <td>100.0</td>\n      <td>40.4</td>\n      <td>15.50</td>\n      <td>5.58</td>\n      <td>8.42</td>\n      <td>1.68</td>\n      <td>1009.76</td>\n      <td>1004.78</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.051584</td>\n      <td>0.998669</td>\n      <td>0.5</td>\n      <td>0.866025</td>\n      <td>32.0</td>\n      <td>60.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>16.0</td>\n      <td>73.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>33.0</td>\n      <td>27.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9.0</td>\n      <td>14.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>33.0</td>\n      <td>73.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>46.0</td>\n      <td>19.79899</td>\n      <td>46.0</td>\n      <td>19.79899</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.0</td>\n      <td>1.414214</td>\n      <td>3.0</td>\n      <td>1.414214</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>44.5</td>\n      <td>40.305087</td>\n      <td>44.5</td>\n      <td>40.305087</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>30.0</td>\n      <td>4.242641</td>\n      <td>30.0</td>\n      <td>4.242641</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>11.5</td>\n      <td>3.535534</td>\n      <td>11.5</td>\n      <td>3.535534</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>53.0</td>\n      <td>28.284271</td>\n      <td>53.0</td>\n      <td>28.284271</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-28.0</td>\n      <td>-14.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-2.0</td>\n      <td>-1.0</td>\n      <td>-57.0</td>\n      <td>-28.5</td>\n      <td>6.0</td>\n      <td>3.0</td>\n      <td>-5.0</td>\n      <td>-2.5</td>\n      <td>-40.0</td>\n      <td>-20.0</td>\n      <td>NaN</td>\n      <td>182.4</td>\n      <td>788.37</td>\n      <td>NaN</td>\n      <td>1360.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Model Training (Time-Based CV + CatBoost Optimization)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 4 — Model Training (Time-Based CV + CatBoost Optimization) (ONE CELL) — FINAL ROBUST\n# Fixes (compared to your last version):\n# - CPU only (no CUDA probing)\n# - AUTO-detect ALL non-numeric columns as categorical (prevents \"CO to float\" etc)\n# - Sanitize categoricals: NaN/None/\"\" -> \"__MISSING__\" and cast to string\n# - Sanitize numerics: force to numeric (errors->NaN)\n# - Time folds filtered so TRAIN contains ALL classes\n# Outputs:\n# - feature_cols, cat_cols, classes, class_to_id, id_to_class\n# - folds\n# - models\n# - oof_proba, oof_pred, oof_macro_f1\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\n\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import f1_score, classification_report\n\nif \"df_train_fe\" not in globals():\n    raise RuntimeError(\"Missing df_train_fe. Run Step 3 first.\")\n\n# ----------------------------\n# Config\n# ----------------------------\nN_SPLITS = 4\nGAP_DAYS = 0\nSEEDS = [42]\n\nITERATIONS = 8000\nLR = 0.05\nDEPTH = 8\nL2 = 6.0\n\nMISSING_CAT = \"__MISSING__\"\n\n# ----------------------------\n# Prepare data\n# ----------------------------\ndf = df_train_fe.copy()\nif \"tanggal\" not in df.columns or \"kategori\" not in df.columns:\n    raise RuntimeError(\"df_train_fe must contain 'tanggal' and 'kategori'.\")\n\ndf = df.dropna(subset=[\"tanggal\"]).sort_values([\"tanggal\", \"stasiun_code\"] if \"stasiun_code\" in df.columns else [\"tanggal\"]).reset_index(drop=True)\n\ndrop_cols = {\"kategori\", \"tanggal\", \"source_file\", \"periode_data\"}\nif \"id\" in df.columns: drop_cols.add(\"id\")\n\n# align features with test (if exists)\nif \"df_test_fe\" in globals() and df_test_fe is not None:\n    common = [c for c in df.columns if c in df_test_fe.columns]\n    feature_cols = [c for c in common if c not in drop_cols]\nelse:\n    feature_cols = [c for c in df.columns if c not in drop_cols]\n\nX = df[feature_cols].copy()\n\n# clean target\ny_str = df[\"kategori\"].astype(str).str.strip()\ny_str = y_str[y_str.str.lower() != \"nan\"]\ndf = df.loc[y_str.index].reset_index(drop=True)\nX  = X.loc[y_str.index].reset_index(drop=True)\ny_str = y_str.reset_index(drop=True)\n\n# ----------------------------\n# AUTO categorical detection + sanitization (KEY FIX)\n# ----------------------------\n# Any column that is NOT numeric is treated as categorical.\nis_num = X.apply(pd.api.types.is_numeric_dtype)\ncat_cols = X.columns[~is_num].tolist()\n\n# sanitize cat cols\nfor c in cat_cols:\n    X[c] = X[c].where(X[c].notna(), MISSING_CAT).astype(str)\n    X[c] = X[c].replace({\"nan\": MISSING_CAT, \"None\": MISSING_CAT, \"\": MISSING_CAT})\n\n# sanitize numeric cols (coerce)\nnum_cols = X.columns[is_num].tolist()\nfor c in num_cols:\n    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n\n# safety: ensure no object left in numeric cols\nbad_num = [c for c in num_cols if X[c].dtype == object]\nif bad_num:\n    raise RuntimeError(f\"Numeric columns still object after coercion: {bad_num[:10]}\")\n\n# ----------------------------\n# classes + weights\n# ----------------------------\nclasses = sorted(y_str.unique().tolist())\ncounts = y_str.value_counts()\nclass_weights = [float(len(y_str) / (len(classes) * counts[c])) for c in classes]\n\nclass_to_id = {c:i for i,c in enumerate(classes)}\nid_to_class = {i:c for c,i in class_to_id.items()}\ny = y_str.map(class_to_id).astype(int)\n\nprint(\"Train rows:\", len(df), \"| n_features:\", len(feature_cols), \"| n_cat(auto):\", len(cat_cols))\nprint(\"Classes:\", classes)\nprint(\"Class counts:\\n\", counts.to_string())\n\n# ----------------------------\n# Time folds (ensure train has all classes)\n# ----------------------------\ndef make_time_folds_filtered(df_in: pd.DataFrame, y_int: pd.Series, n_splits=4, gap_days=0):\n    d = df_in.copy()\n    d[\"year\"] = d[\"tanggal\"].dt.year\n    years = sorted(d[\"year\"].dropna().unique().tolist())\n\n    all_ids = set(range(int(y_int.nunique())))\n\n    def train_has_all(tr_idx):\n        return set(y_int.iloc[tr_idx].unique().tolist()) == all_ids\n\n    folds = []\n\n    # prefer year-based (newest years as validation)\n    if len(years) >= 2:\n        for vy in years[::-1]:\n            tr_mask = d[\"year\"] < vy\n            va_mask = d[\"year\"] == vy\n            if not va_mask.any():\n                continue\n            if gap_days > 0:\n                va_start = d.loc[va_mask, \"tanggal\"].min()\n                tr_mask = tr_mask & (d[\"tanggal\"] <= (va_start - pd.Timedelta(days=gap_days)))\n            tr_idx = d.index[tr_mask].to_numpy()\n            va_idx = d.index[va_mask].to_numpy()\n            if len(tr_idx) == 0 or len(va_idx) == 0:\n                continue\n            if not train_has_all(tr_idx):\n                continue\n            folds.append((tr_idx, va_idx))\n            if len(folds) >= n_splits:\n                break\n        if folds:\n            return folds\n\n    # fallback blocks by date\n    uniq_dates = np.array(sorted(d[\"tanggal\"].unique()))\n    blocks = np.array_split(uniq_dates, n_splits + 1)\n    for b in blocks[1:][::-1]:\n        va_start, va_end = b.min(), b.max()\n        tr_mask = d[\"tanggal\"] < va_start\n        va_mask = (d[\"tanggal\"] >= va_start) & (d[\"tanggal\"] <= va_end)\n        if gap_days > 0:\n            tr_mask = tr_mask & (d[\"tanggal\"] <= (va_start - pd.Timedelta(days=gap_days)))\n        tr_idx = d.index[tr_mask].to_numpy()\n        va_idx = d.index[va_mask].to_numpy()\n        if len(tr_idx) == 0 or len(va_idx) == 0:\n            continue\n        if not train_has_all(tr_idx):\n            continue\n        folds.append((tr_idx, va_idx))\n        if len(folds) >= n_splits:\n            break\n\n    # final fallback: last 20%\n    if not folds:\n        cut = int(len(d) * 0.8)\n        folds = [(d.index[:cut].to_numpy(), d.index[cut:].to_numpy())]\n    return folds\n\nfolds = make_time_folds_filtered(df, y, n_splits=N_SPLITS, gap_days=GAP_DAYS)\n\nprint(\"\\nFolds:\")\nfor i, (tr, va) in enumerate(folds):\n    dtr = (df.loc[tr, \"tanggal\"].min(), df.loc[tr, \"tanggal\"].max())\n    dva = (df.loc[va, \"tanggal\"].min(), df.loc[va, \"tanggal\"].max())\n    print(f\"fold{i}: train={len(tr)} [{dtr[0]}..{dtr[1]}] | valid={len(va)} [{dva[0]}..{dva[1]}]\")\n\n# ----------------------------\n# Train CV (CPU only)\n# ----------------------------\nK = len(classes)\noof_proba = np.zeros((len(df), K), dtype=np.float32)\nmodels = []\nfold_scores = []\n\nfor seed in SEEDS:\n    print(f\"\\n=== SEED {seed} | task_type=CPU ===\")\n    for fi, (tr_idx, va_idx) in enumerate(folds):\n        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]\n\n        train_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n        valid_pool = Pool(X_va, y_va, cat_features=cat_cols)\n\n        model = CatBoostClassifier(\n            loss_function=\"MultiClass\",\n            eval_metric=\"TotalF1\",\n            classes_count=K,\n            class_weights=class_weights,\n            iterations=ITERATIONS,\n            learning_rate=LR,\n            depth=DEPTH,\n            l2_leaf_reg=L2,\n            random_strength=1.0,\n            bagging_temperature=0.5,\n            border_count=128,\n            random_seed=seed,\n            od_type=\"Iter\",\n            od_wait=400,\n            task_type=\"CPU\",\n            thread_count=-1,\n            verbose=250\n        )\n\n        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n\n        proba = model.predict_proba(X_va)\n        pred_int = np.argmax(proba, axis=1)\n        score = f1_score(y_va, pred_int, average=\"macro\")\n\n        oof_proba[va_idx] += (proba / len(SEEDS))\n        fold_scores.append(score)\n        models.append(model)\n\n        print(f\"[seed {seed} fold {fi}] macroF1={score:.5f} | best_iter={model.get_best_iteration()}\")\n\n# ----------------------------\n# OOF summary\n# ----------------------------\noof_pred_int = np.argmax(oof_proba, axis=1)\noof_pred = np.array([id_to_class[i] for i in oof_pred_int])\noof_macro_f1 = f1_score(y_str, oof_pred, average=\"macro\")\n\nprint(\"\\n=== OOF RESULTS ===\")\nprint(\"Fold macroF1:\", [round(s, 5) for s in fold_scores])\nprint(\"OOF macroF1 :\", round(oof_macro_f1, 6))\nprint(\"\\nOOF classification report:\")\nprint(classification_report(y_str, oof_pred, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:45:24.038035Z","iopub.execute_input":"2026-01-30T18:45:24.038447Z","iopub.status.idle":"2026-01-30T21:16:43.230806Z","shell.execute_reply.started":"2026-01-30T18:45:24.038408Z","shell.execute_reply":"2026-01-30T21:16:43.229586Z"}},"outputs":[{"name":"stdout","text":"Train rows: 13651 | n_features: 190 | n_cat(auto): 9\nClasses: ['BAIK', 'BERBAHAYA', 'O3', 'SANGAT TIDAK SEHAT', 'SEDANG', 'TIDAK ADA DATA', 'TIDAK SEHAT']\nClass counts:\n kategori\nSEDANG                7997\nTIDAK SEHAT           2072\nBAIK                  1912\nTIDAK ADA DATA        1440\nSANGAT TIDAK SEHAT     199\nO3                      30\nBERBAHAYA                1\n\nFolds:\nfold0: train=11981 [2010-01-01 00:00:00..2022-12-31 00:00:00] | valid=1670 [2023-01-01 00:00:00..2023-11-30 00:00:00]\nfold1: train=11490 [2010-01-01 00:00:00..2021-12-31 00:00:00] | valid=491 [2022-01-01 00:00:00..2022-12-31 00:00:00]\nfold2: train=9666 [2010-01-01 00:00:00..2020-12-31 00:00:00] | valid=1824 [2021-01-01 00:00:00..2021-12-31 00:00:00]\n\n=== SEED 42 | task_type=CPU ===\n0:\tlearn: 0.8485167\ttest: 0.1448394\tbest: 0.1448394 (0)\ttotal: 1.25s\tremaining: 2h 46m 53s\n250:\tlearn: 0.9998135\ttest: 0.5368871\tbest: 0.5368871 (249)\ttotal: 3m 59s\tremaining: 2h 2m 58s\n500:\tlearn: 0.9999270\ttest: 0.5437375\tbest: 0.5437375 (495)\ttotal: 8m 3s\tremaining: 2h 37s\n750:\tlearn: 1.0000000\ttest: 0.5526506\tbest: 0.5526506 (693)\ttotal: 12m 5s\tremaining: 1h 56m 44s\n1000:\tlearn: 1.0000000\ttest: 0.5618220\tbest: 0.5618220 (997)\ttotal: 16m 9s\tremaining: 1h 53m\n1250:\tlearn: 1.0000000\ttest: 0.5700503\tbest: 0.5700503 (1247)\ttotal: 20m 16s\tremaining: 1h 49m 23s\n1500:\tlearn: 1.0000000\ttest: 0.5778021\tbest: 0.5803460 (1480)\ttotal: 24m 26s\tremaining: 1h 45m 48s\n1750:\tlearn: 1.0000000\ttest: 0.5903294\tbest: 0.5927785 (1728)\ttotal: 28m 33s\tremaining: 1h 41m 56s\n2000:\tlearn: 1.0000000\ttest: 0.6163127\tbest: 0.6163127 (1998)\ttotal: 32m 41s\tremaining: 1h 38m\n2250:\tlearn: 1.0000000\ttest: 0.6367172\tbest: 0.6387963 (2127)\ttotal: 36m 50s\tremaining: 1h 34m 6s\n2500:\tlearn: 1.0000000\ttest: 0.6589184\tbest: 0.6628047 (2494)\ttotal: 41m 1s\tremaining: 1h 30m 11s\n2750:\tlearn: 1.0000000\ttest: 0.6779274\tbest: 0.6779274 (2678)\ttotal: 45m 10s\tremaining: 1h 26m 12s\n3000:\tlearn: 1.0000000\ttest: 0.6845341\tbest: 0.6845341 (2992)\ttotal: 49m 19s\tremaining: 1h 22m 9s\n3250:\tlearn: 1.0000000\ttest: 0.6953007\tbest: 0.6953007 (3203)\ttotal: 53m 27s\tremaining: 1h 18m 5s\n3500:\tlearn: 1.0000000\ttest: 0.7057519\tbest: 0.7057519 (3471)\ttotal: 57m 38s\tremaining: 1h 14m 4s\n3750:\tlearn: 1.0000000\ttest: 0.7091695\tbest: 0.7091695 (3718)\ttotal: 1h 1m 48s\tremaining: 1h 10m 1s\n4000:\tlearn: 1.0000000\ttest: 0.7108663\tbest: 0.7108663 (3764)\ttotal: 1h 5m 57s\tremaining: 1h 5m 55s\n4250:\tlearn: 1.0000000\ttest: 0.7241667\tbest: 0.7241667 (4221)\ttotal: 1h 10m 5s\tremaining: 1h 1m 49s\n4500:\tlearn: 1.0000000\ttest: 0.7392173\tbest: 0.7397223 (4489)\ttotal: 1h 14m 16s\tremaining: 57m 44s\n4750:\tlearn: 1.0000000\ttest: 0.7500220\tbest: 0.7500220 (4725)\ttotal: 1h 18m 25s\tremaining: 53m 38s\n5000:\tlearn: 1.0000000\ttest: 0.7560746\tbest: 0.7560746 (4982)\ttotal: 1h 22m 34s\tremaining: 49m 31s\n5250:\tlearn: 1.0000000\ttest: 0.7649987\tbest: 0.7649987 (5210)\ttotal: 1h 26m 45s\tremaining: 45m 25s\n5500:\tlearn: 1.0000000\ttest: 0.7708508\tbest: 0.7708508 (5493)\ttotal: 1h 30m 55s\tremaining: 41m 18s\n5750:\tlearn: 1.0000000\ttest: 0.7780628\tbest: 0.7780628 (5735)\ttotal: 1h 35m 5s\tremaining: 37m 11s\n6000:\tlearn: 1.0000000\ttest: 0.7907768\tbest: 0.7907768 (5985)\ttotal: 1h 39m 15s\tremaining: 33m 3s\n6250:\tlearn: 1.0000000\ttest: 0.7935585\tbest: 0.7935585 (6163)\ttotal: 1h 43m 26s\tremaining: 28m 56s\n6500:\tlearn: 1.0000000\ttest: 0.7963251\tbest: 0.7977029 (6475)\ttotal: 1h 47m 35s\tremaining: 24m 48s\n6750:\tlearn: 1.0000000\ttest: 0.8004478\tbest: 0.8004478 (6741)\ttotal: 1h 51m 46s\tremaining: 20m 40s\n7000:\tlearn: 1.0000000\ttest: 0.8031786\tbest: 0.8031786 (6986)\ttotal: 1h 55m 55s\tremaining: 16m 32s\n7250:\tlearn: 1.0000000\ttest: 0.8058957\tbest: 0.8058957 (7186)\ttotal: 2h 4s\tremaining: 12m 24s\n7500:\tlearn: 1.0000000\ttest: 0.8085995\tbest: 0.8085995 (7275)\ttotal: 2h 4m 15s\tremaining: 8m 15s\n7750:\tlearn: 1.0000000\ttest: 0.8112903\tbest: 0.8112903 (7608)\ttotal: 2h 8m 22s\tremaining: 4m 7s\n7999:\tlearn: 1.0000000\ttest: 0.8179012\tbest: 0.8179012 (7921)\ttotal: 2h 12m 32s\tremaining: 0us\n\nbestTest = 0.8179012488\nbestIteration = 7921\n\nShrink model to first 7922 iterations.\n[seed 42 fold 0] macroF1=0.55306 | best_iter=7921\n0:\tlearn: 0.8821136\ttest: 0.5229408\tbest: 0.5229408 (0)\ttotal: 823ms\tremaining: 1h 49m 43s\n250:\tlearn: 0.9999807\ttest: 0.8746219\tbest: 0.8746219 (245)\ttotal: 3m 38s\tremaining: 1h 52m 32s\n500:\tlearn: 1.0000000\ttest: 0.8834959\tbest: 0.8834959 (462)\ttotal: 7m 34s\tremaining: 1h 53m 16s\n750:\tlearn: 1.0000000\ttest: 0.8834959\tbest: 0.8834959 (462)\ttotal: 11m 32s\tremaining: 1h 51m 27s\nStopped by overfitting detector  (400 iterations wait)\n\nbestTest = 0.8834958666\nbestIteration = 462\n\nShrink model to first 463 iterations.\n[seed 42 fold 1] macroF1=0.71460 | best_iter=462\n0:\tlearn: 0.8147977\ttest: 0.2955386\tbest: 0.2955386 (0)\ttotal: 829ms\tremaining: 1h 50m 28s\n250:\tlearn: 0.9999589\ttest: 1.0000000\tbest: 1.0000000 (13)\ttotal: 3m 12s\tremaining: 1h 39m 8s\nStopped by overfitting detector  (400 iterations wait)\n\nbestTest = 1\nbestIteration = 13\n\nShrink model to first 14 iterations.\n[seed 42 fold 2] macroF1=1.00000 | best_iter=13\n\n=== OOF RESULTS ===\nFold macroF1: [0.55306, 0.7146, 1.0]\nOOF macroF1 : 0.181812\n\nOOF classification report:\n                    precision    recall  f1-score   support\n\n              BAIK     0.1869    1.0000    0.3150      1912\n         BERBAHAYA     0.0000    0.0000    0.0000         1\n                O3     0.0000    0.0000    0.0000        30\nSANGAT TIDAK SEHAT     0.0000    0.0000    0.0000       199\n            SEDANG     0.9807    0.3496    0.5155      7997\n    TIDAK ADA DATA     1.0000    0.0111    0.0220      1440\n       TIDAK SEHAT     0.9946    0.2664    0.4203      2072\n\n          accuracy                         0.3865     13651\n         macro avg     0.4517    0.2325    0.1818     13651\n      weighted avg     0.8571    0.3865    0.4122     13651\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Inference, Ensembling, Submission & QA","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 5 — Inference, Ensembling, Submission & QA (ONE CELL) — FINAL (REAL)\n# Uses REAL df_test_fe from STEP 3 (NOT dummy from sub)\n#\n# REQUIRE:\n# - STEP 1: sub, ID_COL\n# - STEP 3: df_test_fe  (engineered test features)\n# - STEP 4: models, feature_cols, cat_cols, id_to_class\n#\n# OUTPUT:\n# - /kaggle/working/submission.csv\n# - /kaggle/working/qa_submission.json\n# ============================================================\n\nimport json\nimport numpy as np\nimport pandas as pd\nfrom catboost import Pool\n\n# ----------------------------\n# HARD GUARDS\n# ----------------------------\nneed = [\"sub\", \"ID_COL\", \"df_test_fe\", \"models\", \"feature_cols\", \"cat_cols\", \"id_to_class\"]\nmissing = [k for k in need if k not in globals() or globals()[k] is None]\nif missing:\n    raise RuntimeError(f\"Missing required objects: {missing}. Jalankan step yang kurang.\")\n\nSUB_TARGET_COL = \"category\" if \"category\" in sub.columns else sub.columns[-1]\nMISSING_CAT = \"__MISSING__\"\n\n# ----------------------------\n# PREP TEST FEATURES (REAL)\n# ----------------------------\ntest = df_test_fe.copy()\n\n# ID must exist in df_test_fe\nif ID_COL not in test.columns:\n    raise RuntimeError(f\"{ID_COL} not found in df_test_fe. Pastikan Step 2/3 membawa kolom ID ke test.\")\n\n# align columns to training features in ONE SHOT (no fragmentation)\n# - reindex will create missing cols as NaN efficiently\nX_test = test.reindex(columns=feature_cols)\n\n# ----------------------------\n# SANITIZATION (IDENTICAL TO STEP 4)\n# ----------------------------\n# categorical\nfor c in cat_cols:\n    if c not in X_test.columns:\n        # if cat col missing, create as missing category\n        X_test[c] = MISSING_CAT\n    X_test[c] = X_test[c].where(X_test[c].notna(), MISSING_CAT).astype(str)\n    X_test[c] = X_test[c].replace({\"nan\": MISSING_CAT, \"None\": MISSING_CAT, \"\": MISSING_CAT})\n\n# numeric\nnum_cols = [c for c in X_test.columns if c not in cat_cols]\nX_test[num_cols] = X_test[num_cols].apply(pd.to_numeric, errors=\"coerce\")\n\n# safety check: numeric must not be object\nbad_num = [c for c in num_cols if X_test[c].dtype == object]\nif bad_num:\n    raise RuntimeError(f\"Numeric columns still object after coercion: {bad_num[:20]}\")\n\nprint(f\"[OK] REAL test features ready | rows={len(X_test)} | features={len(feature_cols)} | cat={len(cat_cols)}\")\n\n# ----------------------------\n# INFERENCE + ENSEMBLE (mean prob)\n# ----------------------------\nK = len(id_to_class)\nproba_ens = np.zeros((len(X_test), K), dtype=np.float32)\n\ntest_pool = Pool(X_test, cat_features=cat_cols)\n\nfor mi, model in enumerate(models):\n    p = model.predict_proba(test_pool)\n    if p.shape[1] != K:\n        raise RuntimeError(f\"Class mismatch: model proba has {p.shape[1]} cols, expected {K}\")\n    proba_ens += p / len(models)\n    print(f\"[OK] model {mi} contributed\")\n\npred_int = np.argmax(proba_ens, axis=1)\npred_label = np.array([id_to_class[i] for i in pred_int], dtype=object)\n\n# ----------------------------\n# BUILD SUBMISSION (ORDER = sample_submission)\n# ----------------------------\nsubmission = sub[[ID_COL]].copy()\n\n# map id -> prediction via merge (safe)\npred_df = pd.DataFrame({ID_COL: test[ID_COL].values, \"_pred\": pred_label})\nsubmission = submission.merge(pred_df, on=ID_COL, how=\"left\")\n\n# fill any missing prediction (should not happen) with majority class from preds\nif submission[\"_pred\"].isna().any():\n    maj = pd.Series(pred_label).value_counts().idxmax()\n    submission[\"_pred\"] = submission[\"_pred\"].fillna(maj)\n\nsubmission[SUB_TARGET_COL] = submission[\"_pred\"]\nsubmission = submission.drop(columns=[\"_pred\"])\n\n# ----------------------------\n# QA — FAIL FAST\n# ----------------------------\nqa = {\n    \"rows_submission\": int(len(submission)),\n    \"rows_sample\": int(len(sub)),\n    \"id_unique\": bool(submission[ID_COL].is_unique),\n    \"missing_pred\": int(submission[SUB_TARGET_COL].isna().sum()),\n    \"labels_predicted_top\": submission[SUB_TARGET_COL].value_counts().head(20).to_dict(),\n    \"n_unique_labels_pred\": int(submission[SUB_TARGET_COL].nunique()),\n    \"labels_expected\": sorted(list(id_to_class.values())),\n}\n\nprint(\"\\n=== QA REPORT ===\")\nfor k, v in qa.items():\n    print(f\"{k}: {v}\")\n\nassert qa[\"rows_submission\"] == qa[\"rows_sample\"], \"Row count mismatch with sample_submission\"\nassert qa[\"id_unique\"], \"Duplicate IDs in submission\"\nassert qa[\"missing_pred\"] == 0, \"Missing predictions found\"\n\n# ----------------------------\n# SAVE\n# ----------------------------\nOUT_PATH = \"/kaggle/working/submission.csv\"\nQA_PATH  = \"/kaggle/working/qa_submission.json\"\n\nsubmission.to_csv(OUT_PATH, index=False)\nwith open(QA_PATH, \"w\") as f:\n    json.dump(qa, f, indent=2)\n\nprint(f\"\\n[OK] submission saved to {OUT_PATH}\")\nprint(f\"[OK] qa saved to {QA_PATH}\")\ndisplay(submission.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T21:33:05.758977Z","iopub.execute_input":"2026-01-30T21:33:05.759310Z","iopub.status.idle":"2026-01-30T21:33:05.977809Z","shell.execute_reply.started":"2026-01-30T21:33:05.759282Z","shell.execute_reply":"2026-01-30T21:33:05.976904Z"}},"outputs":[{"name":"stdout","text":"[OK] STEP 5 starting — REAL inference with df_test_fe\n[OK] Test matrix ready | rows=455 | features=190\n[OK] model 0 inferred\n[OK] model 1 inferred\n[OK] model 2 inferred\n\n=== QA REPORT ===\nrows_submission: 455\nrows_sample: 455\nid_unique: True\nmissing_pred: 0\nlabel_distribution: {'TIDAK ADA DATA': 455}\n\n[OK] submission saved to /kaggle/working/submission.csv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                id        category\n0  2025-09-01_DKI1  TIDAK ADA DATA\n1  2025-09-01_DKI2  TIDAK ADA DATA\n2  2025-09-01_DKI3  TIDAK ADA DATA\n3  2025-09-01_DKI4  TIDAK ADA DATA\n4  2025-09-01_DKI5  TIDAK ADA DATA","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-09-01_DKI1</td>\n      <td>TIDAK ADA DATA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-09-01_DKI2</td>\n      <td>TIDAK ADA DATA</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-09-01_DKI3</td>\n      <td>TIDAK ADA DATA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025-09-01_DKI4</td>\n      <td>TIDAK ADA DATA</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025-09-01_DKI5</td>\n      <td>TIDAK ADA DATA</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11}]}