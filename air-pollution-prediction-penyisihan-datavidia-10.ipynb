{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa63cf3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-30T15:42:48.110173Z",
     "iopub.status.busy": "2026-01-30T15:42:48.109824Z",
     "iopub.status.idle": "2026-01-30T15:42:49.124178Z",
     "shell.execute_reply": "2026-01-30T15:42:49.123240Z"
    },
    "papermill": {
     "duration": 1.025057,
     "end_time": "2026-01-30T15:42:49.126344",
     "exception": false,
     "start_time": "2026-01-30T15:42:48.101287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/penyisihan-datavidia-10/sample_submission.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2025.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/NDVI (vegetation index)/indeks-ndvi-jakarta.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/kualitas-air-sungai/data-kualitas-air-sungai-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/libur-nasional/dataset-libur-nasional-dan-weekend.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki3-jagakarsa.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki2-kelapagading.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki1-bundaranhi.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki5-kebonjeruk.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki4-lubangbuaya.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/jumlah-penduduk/data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9091538f",
   "metadata": {
    "papermill": {
     "duration": 0.00337,
     "end_time": "2026-01-30T15:42:49.134404",
     "exception": false,
     "start_time": "2026-01-30T15:42:49.131034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading & Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca38021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T15:42:49.143028Z",
     "iopub.status.busy": "2026-01-30T15:42:49.142592Z",
     "iopub.status.idle": "2026-01-30T15:42:50.305609Z",
     "shell.execute_reply": "2026-01-30T15:42:50.304664Z"
    },
    "papermill": {
     "duration": 1.16962,
     "end_time": "2026-01-30T15:42:50.307308",
     "exception": false,
     "start_time": "2026-01-30T15:42:49.137688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample_submission: (455, 2) cols: ['id', 'category']\n",
      "ID_COL: id | SUB_TARGET_COL: category\n",
      "submission ID unique: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/160485658.py:97: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
      "/tmp/ipykernel_24/160485658.py:100: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n",
      "/tmp/ipykernel_24/160485658.py:97: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ISPU (ALL) CLEAN ---\n",
      "shape: (13652, 18)\n",
      "duplicates on ['tanggal', 'stasiun']: 0\n",
      "tanggal: NaT=0 | range=[2010-01-01 00:00:00 .. 2023-11-30 00:00:00]\n",
      "top missing% cols:\n",
      "bulan                            100.00\n",
      "lokasi_spku                       78.96\n",
      "pm25                              73.49\n",
      "parameter_pencemar_kritis_alt     23.74\n",
      "stasiun_code                      21.04\n",
      "pm10                              14.42\n",
      "kategori_alt                      13.38\n",
      "o3                                12.80\n",
      "\n",
      "Train/unlabeled split:\n",
      "df_train: (13651, 18) | df_ispu_unlabeled: (1, 18)\n",
      "\n",
      "Target distribution (df_train):\n",
      "kategori\n",
      "SEDANG                7997\n",
      "TIDAK SEHAT           2072\n",
      "BAIK                  1912\n",
      "TIDAK ADA DATA        1440\n",
      "SANGAT TIDAK SEHAT     199\n",
      "O3                      30\n",
      "BERBAHAYA                1\n",
      "\n",
      "--- NDVI CLEAN ---\n",
      "shape: (1810, 4)\n",
      "duplicates on ['tanggal', 'stasiun']: 0\n",
      "tanggal: NaT=0 | range=[2009-12-19 00:00:00 .. 2025-08-29 00:00:00]\n",
      "top missing% cols:\n",
      "tanggal         0.0\n",
      "stasiun         0.0\n",
      "ndvi            0.0\n",
      "stasiun_code    0.0\n",
      "\n",
      "--- HOLIDAYS CLEAN ---\n",
      "shape: (5844, 5)\n",
      "duplicates on ['tanggal']: 0\n",
      "tanggal: NaT=0 | range=[2010-01-01 00:00:00 .. 2025-12-31 00:00:00]\n",
      "top missing% cols:\n",
      "nama_libur             95.6\n",
      "tanggal                 0.0\n",
      "is_holiday_nasional     0.0\n",
      "is_weekend              0.0\n",
      "day_name                0.0\n",
      "\n",
      "--- WEATHER (ALL) CLEAN ---\n",
      "shape: (28610, 26)\n",
      "duplicates on ['tanggal', 'weather_station']: 0\n",
      "tanggal: NaT=0 | range=[2010-01-01 00:00:00 .. 2025-12-08 00:00:00]\n",
      "top missing% cols:\n",
      "tanggal                          0.0\n",
      "temperature_2m_max_c             0.0\n",
      "temperature_2m_min_c             0.0\n",
      "precipitation_sum_mm             0.0\n",
      "precipitation_hours_h            0.0\n",
      "wind_speed_10m_max_km_h          0.0\n",
      "wind_direction_10m_dominant      0.0\n",
      "shortwave_radiation_sum_mj_m²    0.0\n",
      "\n",
      "--- POPULATION ---\n",
      "shape: (34176, 9)\n",
      "top missing% cols:\n",
      "periode_data           0.0\n",
      "tahun                  0.0\n",
      "nama_provinsi          0.0\n",
      "nama_kabupaten_kota    0.0\n",
      "nama_kecamatan         0.0\n",
      "nama_kelurahan         0.0\n",
      "usia                   0.0\n",
      "jenis_kelamin          0.0\n",
      "\n",
      "--- RIVER QUALITY ---\n",
      "shape: (14400, 12)\n",
      "top missing% cols:\n",
      "periode_data          0.0\n",
      "periode_pemantauan    0.0\n",
      "bulan_sampling        0.0\n",
      "titik_sampel          0.0\n",
      "nama_sungai           0.0\n",
      "alamat                0.0\n",
      "latitude              0.0\n",
      "longitude             0.0\n",
      "\n",
      "--- Test mapping file candidates (rows == sample_submission) ---\n",
      "None found by heuristic. Likely there is a separate test file not matching this heuristic name.\n",
      "\n",
      "--- Preview heads ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periode_data</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>stasiun</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>so2</th>\n",
       "      <th>co</th>\n",
       "      <th>o3</th>\n",
       "      <th>no2</th>\n",
       "      <th>max</th>\n",
       "      <th>parameter_pencemar_kritis</th>\n",
       "      <th>kategori</th>\n",
       "      <th>source_file</th>\n",
       "      <th>bulan</th>\n",
       "      <th>parameter_pencemar_kritis_alt</th>\n",
       "      <th>kategori_alt</th>\n",
       "      <th>lokasi_spku</th>\n",
       "      <th>stasiun_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CO</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI2 (Kelapa Gading)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI3 (Jagakarsa)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periode_data    tanggal               stasiun  pm10  pm25  so2    co    o3   no2   max parameter_pencemar_kritis        kategori                                        source_file  bulan  \\\n",
       "0        201001 2010-01-01    DKI1 (Bunderan HI)  60.0   NaN  4.0  73.0  27.0  14.0  73.0                        CO          SEDANG  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "1        201001 2010-01-01  DKI2 (Kelapa Gading)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "2        201001 2010-01-01      DKI3 (Jagakarsa)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "\n",
       "  parameter_pencemar_kritis_alt    kategori_alt lokasi_spku stasiun_code  \n",
       "0                            CO          SEDANG         NaN         DKI1  \n",
       "1                           NaN  TIDAK ADA DATA         NaN         DKI2  \n",
       "2                           NaN  TIDAK ADA DATA         NaN         DKI3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tanggal</th>\n",
       "      <th>stasiun</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>stasiun_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-19</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>0.1849</td>\n",
       "      <td>DKI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-12-19</td>\n",
       "      <td>DKI2</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>DKI2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-12-19</td>\n",
       "      <td>DKI3</td>\n",
       "      <td>0.5613</td>\n",
       "      <td>DKI3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tanggal stasiun    ndvi stasiun_code\n",
       "0 2009-12-19    DKI1  0.1849         DKI1\n",
       "1 2009-12-19    DKI2  0.2891         DKI2\n",
       "2 2009-12-19    DKI3  0.5613         DKI3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tanggal</th>\n",
       "      <th>is_holiday_nasional</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>nama_libur</th>\n",
       "      <th>day_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tanggal  is_holiday_nasional  is_weekend      nama_libur  day_name\n",
       "0 2010-01-01                    1           0  New Year's Day    Friday\n",
       "1 2010-01-02                    0           0            None  Saturday\n",
       "2 2010-01-03                    0           0            None    Sunday"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tanggal</th>\n",
       "      <th>temperature_2m_max_c</th>\n",
       "      <th>temperature_2m_min_c</th>\n",
       "      <th>precipitation_sum_mm</th>\n",
       "      <th>precipitation_hours_h</th>\n",
       "      <th>wind_speed_10m_max_km_h</th>\n",
       "      <th>wind_direction_10m_dominant</th>\n",
       "      <th>shortwave_radiation_sum_mj_m²</th>\n",
       "      <th>temperature_2m_mean_c</th>\n",
       "      <th>relative_humidity_2m_mean</th>\n",
       "      <th>cloud_cover_mean</th>\n",
       "      <th>surface_pressure_mean_hpa</th>\n",
       "      <th>wind_gusts_10m_max_km_h</th>\n",
       "      <th>winddirection_10m_dominant</th>\n",
       "      <th>relative_humidity_2m_max</th>\n",
       "      <th>relative_humidity_2m_min</th>\n",
       "      <th>cloud_cover_max</th>\n",
       "      <th>cloud_cover_min</th>\n",
       "      <th>wind_gusts_10m_mean_km_h</th>\n",
       "      <th>wind_speed_10m_mean_km_h</th>\n",
       "      <th>wind_gusts_10m_min_km_h</th>\n",
       "      <th>wind_speed_10m_min_km_h</th>\n",
       "      <th>surface_pressure_max_hpa</th>\n",
       "      <th>surface_pressure_min_hpa</th>\n",
       "      <th>weather_station</th>\n",
       "      <th>weather_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>29.4</td>\n",
       "      <td>24.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>246</td>\n",
       "      <td>16.24</td>\n",
       "      <td>26.6</td>\n",
       "      <td>81</td>\n",
       "      <td>100</td>\n",
       "      <td>1007.5</td>\n",
       "      <td>38.2</td>\n",
       "      <td>246</td>\n",
       "      <td>90</td>\n",
       "      <td>69</td>\n",
       "      <td>100</td>\n",
       "      <td>99</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>1005.1</td>\n",
       "      <td>dki1-bundaranhi</td>\n",
       "      <td>DKI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>28.9</td>\n",
       "      <td>24.2</td>\n",
       "      <td>6.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>260</td>\n",
       "      <td>13.01</td>\n",
       "      <td>26.2</td>\n",
       "      <td>85</td>\n",
       "      <td>99</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>260</td>\n",
       "      <td>95</td>\n",
       "      <td>72</td>\n",
       "      <td>100</td>\n",
       "      <td>94</td>\n",
       "      <td>13.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>1007.4</td>\n",
       "      <td>dki1-bundaranhi</td>\n",
       "      <td>DKI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>31.4</td>\n",
       "      <td>24.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>224</td>\n",
       "      <td>23.89</td>\n",
       "      <td>27.1</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "      <td>1009.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>224</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>15.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1012.2</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>dki1-bundaranhi</td>\n",
       "      <td>DKI1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tanggal  temperature_2m_max_c  temperature_2m_min_c  precipitation_sum_mm  precipitation_hours_h  wind_speed_10m_max_km_h  wind_direction_10m_dominant  shortwave_radiation_sum_mj_m²  \\\n",
       "0 2010-01-01                  29.4                  24.4                   4.0                   14.0                     16.0                          246                          16.24   \n",
       "1 2010-01-02                  28.9                  24.2                   6.9                   14.0                      9.5                          260                          13.01   \n",
       "2 2010-01-03                  31.4                  24.9                  11.2                    6.0                      9.4                          224                          23.89   \n",
       "\n",
       "   temperature_2m_mean_c  relative_humidity_2m_mean  cloud_cover_mean  surface_pressure_mean_hpa  wind_gusts_10m_max_km_h  winddirection_10m_dominant  relative_humidity_2m_max  \\\n",
       "0                   26.6                         81               100                     1007.5                     38.2                         246                        90   \n",
       "1                   26.2                         85                99                     1010.1                     22.0                         260                        95   \n",
       "2                   27.1                         85                93                     1009.9                     21.2                         224                        95   \n",
       "\n",
       "   relative_humidity_2m_min  cloud_cover_max  cloud_cover_min  wind_gusts_10m_mean_km_h  wind_speed_10m_mean_km_h  wind_gusts_10m_min_km_h  wind_speed_10m_min_km_h  surface_pressure_max_hpa  \\\n",
       "0                        69              100               99                      21.0                      10.5                     11.9                      6.9                    1009.3   \n",
       "1                        72              100               94                      13.7                       6.0                      8.6                      2.3                    1011.9   \n",
       "2                        70              100               28                      15.7                       5.7                      8.3                      1.6                    1012.2   \n",
       "\n",
       "   surface_pressure_min_hpa  weather_station weather_code  \n",
       "0                    1005.1  dki1-bundaranhi         DKI1  \n",
       "1                    1007.4  dki1-bundaranhi         DKI1  \n",
       "2                    1007.0  dki1-bundaranhi         DKI1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1 — Data Loading & Sanity Checks (ONE CELL) — CatBoost Track (REVISED v2)\n",
    "# Fixes:\n",
    "# - Series.lower() bug -> use .str.lower()\n",
    "# - Keeps robust parsing, dedup, and label/critical unification\n",
    "# Outputs (globals):\n",
    "#   sub, ID_COL, SUB_TARGET_COL\n",
    "#   df_ispu_all, df_train, df_ispu_unlabeled\n",
    "#   df_ndvi, df_holiday, df_weather, df_pop, df_river\n",
    "#   test_candidates\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n",
    "assert DATA_ROOT.exists(), f\"DATA_ROOT not found: {DATA_ROOT}\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _read_csv_smart(path: Path) -> pd.DataFrame:\n",
    "    seps = [\",\", \";\", \"\\t\", \"|\"]\n",
    "    encs = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n",
    "    last_err = None\n",
    "    for sep in seps:\n",
    "        for enc in encs:\n",
    "            try:\n",
    "                df = pd.read_csv(path, sep=sep, encoding=enc, low_memory=False)\n",
    "                if df.shape[1] >= 2:\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "    raise RuntimeError(f\"Failed to read: {path}\\nLast error: {last_err}\")\n",
    "\n",
    "def _norm_col(c: str) -> str:\n",
    "    c = str(c).strip().lower()\n",
    "    c = re.sub(r\"[^\\w]+\", \"_\", c)\n",
    "    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "    return c\n",
    "\n",
    "def _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = {c: _norm_col(c) for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "\n",
    "    rename = {}\n",
    "    for c in df.columns:\n",
    "        if c in [\"tanggal\", \"date\", \"time\", \"waktu\"]:\n",
    "            rename[c] = \"tanggal\"\n",
    "        elif c in [\"stasiun\", \"station\", \"stasiun_id\", \"id_stasiun\"]:\n",
    "            rename[c] = \"stasiun\"\n",
    "        elif c in [\"periode_data\", \"periode\"]:\n",
    "            rename[c] = \"periode_data\"\n",
    "        elif c in [\"pm_sepuluh\", \"pm10\", \"pm_10\"]:\n",
    "            rename[c] = \"pm10\"\n",
    "        elif c in [\"pm_duakomalima\", \"pm2_5\", \"pm25\", \"pm_2_5\", \"pm2_5_\"]:\n",
    "            rename[c] = \"pm25\"\n",
    "        elif c in [\"sulfur_dioksida\", \"so2\"]:\n",
    "            rename[c] = \"so2\"\n",
    "        elif c in [\"karbon_monoksida\", \"co\"]:\n",
    "            rename[c] = \"co\"\n",
    "        elif c in [\"ozon\", \"o3\"]:\n",
    "            rename[c] = \"o3\"\n",
    "        elif c in [\"nitrogen_dioksida\", \"no2\"]:\n",
    "            rename[c] = \"no2\"\n",
    "        elif c in [\"parameter_pencemar_kritis\", \"parameter_pencemar\", \"pencemar_kritis\"]:\n",
    "            rename[c] = \"parameter_pencemar_kritis\"\n",
    "        elif c in [\"max\", \"maks\", \"nilai_maks\", \"indeks_maks\"]:\n",
    "            rename[c] = \"max\"\n",
    "        elif c in [\"ndvi\", \"vegetation_index\"]:\n",
    "            rename[c] = \"ndvi\"\n",
    "        elif c in [\"is_holiday_nasional\", \"holiday_nasional\", \"is_holiday\"]:\n",
    "            rename[c] = \"is_holiday_nasional\"\n",
    "        elif c in [\"is_weekend\", \"weekend\"]:\n",
    "            rename[c] = \"is_weekend\"\n",
    "        elif c in [\"day_name\", \"nama_hari\"]:\n",
    "            rename[c] = \"day_name\"\n",
    "        elif c in [\"nama_libur\", \"holiday_name\"]:\n",
    "            rename[c] = \"nama_libur\"\n",
    "\n",
    "        # keep common typos as *_alt\n",
    "        elif c == \"categori\":\n",
    "            rename[c] = \"kategori_alt\"\n",
    "        elif c == \"critical\":\n",
    "            rename[c] = \"parameter_pencemar_kritis_alt\"\n",
    "\n",
    "    return df.rename(columns=rename)\n",
    "\n",
    "def parse_date_twopass(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"None\": np.nan})\n",
    "    d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    m = d1.isna()\n",
    "    if m.any():\n",
    "        d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n",
    "        d1.loc[m] = d2\n",
    "    return d1\n",
    "\n",
    "def _coerce_numeric(df: pd.DataFrame, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _dedup_keep_most_complete(df: pd.DataFrame, key_cols):\n",
    "    if not all(k in df.columns for k in key_cols):\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"_nn\"] = df.notna().sum(axis=1)\n",
    "    idx = df.groupby(key_cols)[\"_nn\"].idxmax()\n",
    "    df = df.loc[idx].drop(columns=[\"_nn\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _basic_sanity(name: str, df: pd.DataFrame, key_cols=None, date_col=\"tanggal\"):\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"shape:\", df.shape)\n",
    "    if key_cols is not None and all(k in df.columns for k in key_cols):\n",
    "        print(f\"duplicates on {key_cols}:\", int(df.duplicated(key_cols).sum()))\n",
    "    if date_col in df.columns:\n",
    "        print(f\"{date_col}: NaT={int(df[date_col].isna().sum())} | range=[{df[date_col].min()} .. {df[date_col].max()}]\")\n",
    "    miss = (df.isna().mean().sort_values(ascending=False).head(8) * 100).round(2)\n",
    "    print(\"top missing% cols:\")\n",
    "    print(miss.to_string())\n",
    "\n",
    "# ----------------------------\n",
    "# 0) sample_submission\n",
    "# ----------------------------\n",
    "sub = _standardize_columns(_read_csv_smart(DATA_ROOT / \"sample_submission.csv\"))\n",
    "ID_COL = \"id\" if \"id\" in sub.columns else sub.columns[0]\n",
    "SUB_TARGET_COL = \"category\" if \"category\" in sub.columns else sub.columns[-1]\n",
    "n_test_expected = len(sub)\n",
    "\n",
    "print(\"Loaded sample_submission:\", sub.shape, \"cols:\", list(sub.columns))\n",
    "print(\"ID_COL:\", ID_COL, \"| SUB_TARGET_COL:\", SUB_TARGET_COL)\n",
    "print(\"submission ID unique:\", bool(sub[ID_COL].is_unique))\n",
    "\n",
    "# ----------------------------\n",
    "# 1) ISPU (concat all years) + CLEAN\n",
    "# ----------------------------\n",
    "ispu_files = sorted((DATA_ROOT / \"ISPU\").glob(\"*.csv\"))\n",
    "assert len(ispu_files) > 0, \"No ISPU CSV files found.\"\n",
    "\n",
    "frames = []\n",
    "for p in ispu_files:\n",
    "    df = _standardize_columns(_read_csv_smart(p))\n",
    "    df[\"source_file\"] = p.name\n",
    "    frames.append(df)\n",
    "\n",
    "df_ispu_all = pd.concat(frames, ignore_index=True, sort=False)\n",
    "\n",
    "# unify label + critical columns into canonical names\n",
    "if \"kategori_alt\" in df_ispu_all.columns:\n",
    "    if \"kategori\" not in df_ispu_all.columns:\n",
    "        df_ispu_all[\"kategori\"] = df_ispu_all[\"kategori_alt\"]\n",
    "    else:\n",
    "        df_ispu_all[\"kategori\"] = df_ispu_all[\"kategori\"].fillna(df_ispu_all[\"kategori_alt\"])\n",
    "\n",
    "if \"parameter_pencemar_kritis_alt\" in df_ispu_all.columns:\n",
    "    if \"parameter_pencemar_kritis\" not in df_ispu_all.columns:\n",
    "        df_ispu_all[\"parameter_pencemar_kritis\"] = df_ispu_all[\"parameter_pencemar_kritis_alt\"]\n",
    "    else:\n",
    "        df_ispu_all[\"parameter_pencemar_kritis\"] = df_ispu_all[\"parameter_pencemar_kritis\"].fillna(df_ispu_all[\"parameter_pencemar_kritis_alt\"])\n",
    "\n",
    "# robust date parse\n",
    "if \"tanggal\" in df_ispu_all.columns:\n",
    "    df_ispu_all[\"tanggal\"] = parse_date_twopass(df_ispu_all[\"tanggal\"])\n",
    "\n",
    "# stasiun cleanup + code\n",
    "if \"stasiun\" in df_ispu_all.columns:\n",
    "    df_ispu_all[\"stasiun\"] = df_ispu_all[\"stasiun\"].astype(str).str.strip()\n",
    "    df_ispu_all[\"stasiun_code\"] = (\n",
    "        df_ispu_all[\"stasiun\"]\n",
    "        .str.upper()\n",
    "        .str.extract(r\"(DKI\\s*\\d+)\", expand=False)\n",
    "        .str.replace(\" \", \"\", regex=False)\n",
    "    )\n",
    "else:\n",
    "    df_ispu_all[\"stasiun_code\"] = np.nan\n",
    "\n",
    "# numeric casts\n",
    "df_ispu_all = _coerce_numeric(df_ispu_all, [\"pm10\", \"pm25\", \"so2\", \"co\", \"o3\", \"no2\", \"max\"])\n",
    "\n",
    "# drop rows missing key fields\n",
    "df_ispu_all = df_ispu_all.dropna(subset=[\"tanggal\", \"stasiun\"]).copy()\n",
    "\n",
    "# dedup by key keep most complete\n",
    "df_ispu_all = _dedup_keep_most_complete(df_ispu_all, [\"tanggal\", \"stasiun\"])\n",
    "df_ispu_all = df_ispu_all.sort_values([\"tanggal\", \"stasiun\"]).reset_index(drop=True)\n",
    "\n",
    "_basic_sanity(\"ISPU (ALL) CLEAN\", df_ispu_all, key_cols=[\"tanggal\", \"stasiun\"])\n",
    "\n",
    "# build train vs unlabeled\n",
    "if \"kategori\" in df_ispu_all.columns:\n",
    "    lab = df_ispu_all[\"kategori\"].astype(str).str.strip()\n",
    "    lab_low = lab.str.lower()\n",
    "    m_train = df_ispu_all[\"kategori\"].notna() & (lab != \"\") & (lab_low != \"nan\")\n",
    "    df_train = df_ispu_all.loc[m_train].copy()\n",
    "    df_ispu_unlabeled = df_ispu_all.loc[~m_train].copy()\n",
    "else:\n",
    "    df_train = df_ispu_all.copy()\n",
    "    df_ispu_unlabeled = df_ispu_all.iloc[0:0].copy()\n",
    "\n",
    "print(\"\\nTrain/unlabeled split:\")\n",
    "print(\"df_train:\", df_train.shape, \"| df_ispu_unlabeled:\", df_ispu_unlabeled.shape)\n",
    "if \"kategori\" in df_train.columns:\n",
    "    print(\"\\nTarget distribution (df_train):\")\n",
    "    print(df_train[\"kategori\"].astype(str).str.strip().value_counts(dropna=False).to_string())\n",
    "\n",
    "# ----------------------------\n",
    "# 2) NDVI + stasiun_code\n",
    "# ----------------------------\n",
    "df_ndvi = _standardize_columns(_read_csv_smart(DATA_ROOT / \"NDVI (vegetation index)\" / \"indeks-ndvi-jakarta.csv\"))\n",
    "if \"tanggal\" in df_ndvi.columns:\n",
    "    df_ndvi[\"tanggal\"] = parse_date_twopass(df_ndvi[\"tanggal\"])\n",
    "if \"stasiun\" in df_ndvi.columns:\n",
    "    df_ndvi[\"stasiun\"] = df_ndvi[\"stasiun\"].astype(str).str.strip().str.upper().str.replace(\" \", \"\", regex=False)\n",
    "    df_ndvi[\"stasiun_code\"] = df_ndvi[\"stasiun\"].str.extract(r\"(DKI\\d+)\", expand=False)\n",
    "df_ndvi = _coerce_numeric(df_ndvi, [\"ndvi\"])\n",
    "df_ndvi = df_ndvi.dropna(subset=[\"tanggal\", \"stasiun\"]).copy()\n",
    "df_ndvi = _dedup_keep_most_complete(df_ndvi, [\"tanggal\", \"stasiun\"])\n",
    "\n",
    "_basic_sanity(\"NDVI CLEAN\", df_ndvi, key_cols=[\"tanggal\", \"stasiun\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Holidays (clean to one row per date)\n",
    "# ----------------------------\n",
    "df_holiday = _standardize_columns(_read_csv_smart(DATA_ROOT / \"libur-nasional\" / \"dataset-libur-nasional-dan-weekend.csv\"))\n",
    "if \"tanggal\" in df_holiday.columns:\n",
    "    df_holiday[\"tanggal\"] = parse_date_twopass(df_holiday[\"tanggal\"])\n",
    "df_holiday = df_holiday.dropna(subset=[\"tanggal\"]).sort_values(\"tanggal\").copy()\n",
    "\n",
    "for c in [\"is_holiday_nasional\", \"is_weekend\"]:\n",
    "    if c in df_holiday.columns:\n",
    "        df_holiday[c] = pd.to_numeric(df_holiday[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "agg = {}\n",
    "if \"is_holiday_nasional\" in df_holiday.columns: agg[\"is_holiday_nasional\"] = \"max\"\n",
    "if \"is_weekend\" in df_holiday.columns: agg[\"is_weekend\"] = \"max\"\n",
    "if \"nama_libur\" in df_holiday.columns: agg[\"nama_libur\"] = \"first\"\n",
    "df_holiday = df_holiday.groupby(\"tanggal\", as_index=False).agg(agg)\n",
    "df_holiday[\"day_name\"] = df_holiday[\"tanggal\"].dt.day_name()\n",
    "\n",
    "_basic_sanity(\"HOLIDAYS CLEAN\", df_holiday, key_cols=[\"tanggal\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Weather (multiple stations) clean\n",
    "# ----------------------------\n",
    "weather_files = sorted((DATA_ROOT / \"cuaca-harian\").glob(\"*.csv\"))\n",
    "assert len(weather_files) > 0, \"No weather CSV files found.\"\n",
    "\n",
    "w_frames = []\n",
    "for p in weather_files:\n",
    "    w = _standardize_columns(_read_csv_smart(p))\n",
    "    tag = p.stem.lower().replace(\"cuaca_harian_\", \"\").replace(\"cuaca-harian-\", \"\")\n",
    "    w[\"weather_station\"] = tag\n",
    "    w[\"weather_code\"] = (pd.Series([tag] * len(w)).str.extract(r\"(dki\\d)\", expand=False).str.upper())\n",
    "    if \"tanggal\" in w.columns:\n",
    "        w[\"tanggal\"] = parse_date_twopass(w[\"tanggal\"])\n",
    "    w_frames.append(w)\n",
    "\n",
    "df_weather = pd.concat(w_frames, ignore_index=True, sort=False)\n",
    "df_weather = df_weather.dropna(subset=[\"tanggal\"]).copy()\n",
    "\n",
    "for c in df_weather.columns:\n",
    "    if c not in [\"tanggal\", \"weather_station\", \"weather_code\"]:\n",
    "        if df_weather[c].dtype == object:\n",
    "            df_weather[c] = pd.to_numeric(df_weather[c], errors=\"ignore\")\n",
    "\n",
    "df_weather = _dedup_keep_most_complete(df_weather, [\"tanggal\", \"weather_station\"])\n",
    "df_weather = df_weather.sort_values([\"weather_station\", \"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "_basic_sanity(\"WEATHER (ALL) CLEAN\", df_weather, key_cols=[\"tanggal\", \"weather_station\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Population\n",
    "# ----------------------------\n",
    "df_pop = _standardize_columns(_read_csv_smart(DATA_ROOT / \"jumlah-penduduk\" / \"data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\"))\n",
    "if \"tahun\" in df_pop.columns:\n",
    "    df_pop[\"tahun\"] = pd.to_numeric(df_pop[\"tahun\"], errors=\"coerce\")\n",
    "if \"jumlah_penduduk\" in df_pop.columns:\n",
    "    df_pop[\"jumlah_penduduk\"] = pd.to_numeric(df_pop[\"jumlah_penduduk\"], errors=\"coerce\")\n",
    "_basic_sanity(\"POPULATION\", df_pop)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) River Quality\n",
    "# ----------------------------\n",
    "df_river = _standardize_columns(_read_csv_smart(DATA_ROOT / \"kualitas-air-sungai\" / \"data-kualitas-air-sungai-komponen-data.csv\"))\n",
    "for c in [\"latitude\", \"longitude\", \"baku_mutu\", \"hasil_pengukuran\", \"bulan_sampling\"]:\n",
    "    if c in df_river.columns:\n",
    "        df_river[c] = pd.to_numeric(df_river[c], errors=\"coerce\")\n",
    "_basic_sanity(\"RIVER QUALITY\", df_river)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Find test mapping file candidates (rows == sample_submission) with an 'id' column\n",
    "# ----------------------------\n",
    "test_candidates = []\n",
    "for p in DATA_ROOT.rglob(\"*.csv\"):\n",
    "    if p.name == \"sample_submission.csv\":\n",
    "        continue\n",
    "    name = p.name.lower()\n",
    "    if (\"test\" in name) or (\"submission\" in name):\n",
    "        try:\n",
    "            tmp = _standardize_columns(_read_csv_smart(p).head(2))\n",
    "            if \"id\" in tmp.columns:\n",
    "                df_full = _standardize_columns(_read_csv_smart(p))\n",
    "                if len(df_full) == n_test_expected:\n",
    "                    test_candidates.append((str(p), list(df_full.columns)))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"\\n--- Test mapping file candidates (rows == sample_submission) ---\")\n",
    "if len(test_candidates) == 0:\n",
    "    print(\"None found by heuristic. Likely there is a separate test file not matching this heuristic name.\")\n",
    "else:\n",
    "    for fp, cols in test_candidates:\n",
    "        print(fp, \"| cols:\", cols)\n",
    "\n",
    "# ----------------------------\n",
    "# Preview\n",
    "# ----------------------------\n",
    "print(\"\\n--- Preview heads ---\")\n",
    "display(df_train.head(3))\n",
    "display(df_ndvi.head(3))\n",
    "display(df_holiday.head(3))\n",
    "display(df_weather.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a263004",
   "metadata": {
    "papermill": {
     "duration": 0.003975,
     "end_time": "2026-01-30T15:42:50.315544",
     "exception": false,
     "start_time": "2026-01-30T15:42:50.311569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Master Table Building (Correct Joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8175f21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T15:42:50.325606Z",
     "iopub.status.busy": "2026-01-30T15:42:50.325265Z",
     "iopub.status.idle": "2026-01-30T15:42:50.749770Z",
     "shell.execute_reply": "2026-01-30T15:42:50.748880Z"
    },
    "papermill": {
     "duration": 0.43204,
     "end_time": "2026-01-30T15:42:50.751453",
     "exception": false,
     "start_time": "2026-01-30T15:42:50.319413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_master: (13651, 79)\n",
      "train key duplicates: 0\n",
      "\n",
      "[WARN] Test mapping file (id -> tanggal/stasiun) not found yet.\n",
      "       You can still continue feature engineering + CV on df_train_master.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periode_data</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>stasiun</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>so2</th>\n",
       "      <th>co</th>\n",
       "      <th>o3</th>\n",
       "      <th>no2</th>\n",
       "      <th>max</th>\n",
       "      <th>parameter_pencemar_kritis</th>\n",
       "      <th>kategori</th>\n",
       "      <th>source_file</th>\n",
       "      <th>bulan</th>\n",
       "      <th>parameter_pencemar_kritis_alt</th>\n",
       "      <th>kategori_alt</th>\n",
       "      <th>lokasi_spku</th>\n",
       "      <th>stasiun_code</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dow</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>is_holiday_nasional</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>nama_libur</th>\n",
       "      <th>day_name</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>wx_temperature_2m_max_c</th>\n",
       "      <th>wx_temperature_2m_min_c</th>\n",
       "      <th>wx_precipitation_sum_mm</th>\n",
       "      <th>wx_precipitation_hours_h</th>\n",
       "      <th>wx_wind_speed_10m_max_km_h</th>\n",
       "      <th>wx_wind_direction_10m_dominant</th>\n",
       "      <th>wx_shortwave_radiation_sum_mj_m²</th>\n",
       "      <th>wx_temperature_2m_mean_c</th>\n",
       "      <th>wx_relative_humidity_2m_mean</th>\n",
       "      <th>wx_cloud_cover_mean</th>\n",
       "      <th>wx_surface_pressure_mean_hpa</th>\n",
       "      <th>wx_wind_gusts_10m_max_km_h</th>\n",
       "      <th>wx_winddirection_10m_dominant</th>\n",
       "      <th>wx_relative_humidity_2m_max</th>\n",
       "      <th>wx_relative_humidity_2m_min</th>\n",
       "      <th>wx_cloud_cover_max</th>\n",
       "      <th>wx_cloud_cover_min</th>\n",
       "      <th>wx_wind_gusts_10m_mean_km_h</th>\n",
       "      <th>wx_wind_speed_10m_mean_km_h</th>\n",
       "      <th>wx_wind_gusts_10m_min_km_h</th>\n",
       "      <th>wx_wind_speed_10m_min_km_h</th>\n",
       "      <th>wx_surface_pressure_max_hpa</th>\n",
       "      <th>wx_surface_pressure_min_hpa</th>\n",
       "      <th>wx_weather_station</th>\n",
       "      <th>wxg_temperature_2m_max_c</th>\n",
       "      <th>wxg_temperature_2m_min_c</th>\n",
       "      <th>wxg_precipitation_sum_mm</th>\n",
       "      <th>wxg_precipitation_hours_h</th>\n",
       "      <th>wxg_wind_speed_10m_max_km_h</th>\n",
       "      <th>wxg_wind_direction_10m_dominant</th>\n",
       "      <th>wxg_shortwave_radiation_sum_mj_m²</th>\n",
       "      <th>wxg_temperature_2m_mean_c</th>\n",
       "      <th>wxg_relative_humidity_2m_mean</th>\n",
       "      <th>wxg_cloud_cover_mean</th>\n",
       "      <th>wxg_surface_pressure_mean_hpa</th>\n",
       "      <th>wxg_wind_gusts_10m_max_km_h</th>\n",
       "      <th>wxg_winddirection_10m_dominant</th>\n",
       "      <th>wxg_relative_humidity_2m_max</th>\n",
       "      <th>wxg_relative_humidity_2m_min</th>\n",
       "      <th>wxg_cloud_cover_max</th>\n",
       "      <th>wxg_cloud_cover_min</th>\n",
       "      <th>wxg_wind_gusts_10m_mean_km_h</th>\n",
       "      <th>wxg_wind_speed_10m_mean_km_h</th>\n",
       "      <th>wxg_wind_gusts_10m_min_km_h</th>\n",
       "      <th>wxg_wind_speed_10m_min_km_h</th>\n",
       "      <th>wxg_surface_pressure_max_hpa</th>\n",
       "      <th>wxg_surface_pressure_min_hpa</th>\n",
       "      <th>pop_total_year</th>\n",
       "      <th>river_exceed_rate</th>\n",
       "      <th>river_ratio_mean</th>\n",
       "      <th>river_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CO</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>29.4</td>\n",
       "      <td>24.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>16.24</td>\n",
       "      <td>26.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1007.5</td>\n",
       "      <td>38.2</td>\n",
       "      <td>246.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>1005.1</td>\n",
       "      <td>dki1-bundaranhi</td>\n",
       "      <td>29.58</td>\n",
       "      <td>24.24</td>\n",
       "      <td>4.48</td>\n",
       "      <td>12.4</td>\n",
       "      <td>16.2</td>\n",
       "      <td>249.6</td>\n",
       "      <td>16.484</td>\n",
       "      <td>26.48</td>\n",
       "      <td>82.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1004.98</td>\n",
       "      <td>38.2</td>\n",
       "      <td>249.6</td>\n",
       "      <td>90.8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.04</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1006.86</td>\n",
       "      <td>1002.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI2 (Kelapa Gading)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI2</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0.0939</td>\n",
       "      <td>29.4</td>\n",
       "      <td>24.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>255.0</td>\n",
       "      <td>16.85</td>\n",
       "      <td>26.7</td>\n",
       "      <td>81.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>38.2</td>\n",
       "      <td>255.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>10.6</td>\n",
       "      <td>11.9</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>1004.9</td>\n",
       "      <td>dki2-kelapagading</td>\n",
       "      <td>29.58</td>\n",
       "      <td>24.24</td>\n",
       "      <td>4.48</td>\n",
       "      <td>12.4</td>\n",
       "      <td>16.2</td>\n",
       "      <td>249.6</td>\n",
       "      <td>16.484</td>\n",
       "      <td>26.48</td>\n",
       "      <td>82.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1004.98</td>\n",
       "      <td>38.2</td>\n",
       "      <td>249.6</td>\n",
       "      <td>90.8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.04</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1006.86</td>\n",
       "      <td>1002.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI3 (Jagakarsa)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TIDAK ADA DATA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI3</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0.5332</td>\n",
       "      <td>29.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>16.24</td>\n",
       "      <td>26.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>999.9</td>\n",
       "      <td>38.2</td>\n",
       "      <td>246.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1001.8</td>\n",
       "      <td>997.6</td>\n",
       "      <td>dki3-jagakarsa</td>\n",
       "      <td>29.58</td>\n",
       "      <td>24.24</td>\n",
       "      <td>4.48</td>\n",
       "      <td>12.4</td>\n",
       "      <td>16.2</td>\n",
       "      <td>249.6</td>\n",
       "      <td>16.484</td>\n",
       "      <td>26.48</td>\n",
       "      <td>82.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1004.98</td>\n",
       "      <td>38.2</td>\n",
       "      <td>249.6</td>\n",
       "      <td>90.8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.04</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>1006.86</td>\n",
       "      <td>1002.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periode_data    tanggal               stasiun  pm10  pm25  so2    co    o3   no2   max parameter_pencemar_kritis        kategori                                        source_file  bulan  \\\n",
       "0        201001 2010-01-01    DKI1 (Bunderan HI)  60.0   NaN  4.0  73.0  27.0  14.0  73.0                        CO          SEDANG  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "1        201001 2010-01-01  DKI2 (Kelapa Gading)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "2        201001 2010-01-01      DKI3 (Jagakarsa)   NaN   NaN  NaN   NaN   NaN   NaN   0.0                       NaN  TIDAK ADA DATA  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "\n",
       "  parameter_pencemar_kritis_alt    kategori_alt lokasi_spku stasiun_code  year  month  day  dow  dayofyear  is_holiday_nasional  is_weekend      nama_libur day_name    ndvi  wx_temperature_2m_max_c  \\\n",
       "0                            CO          SEDANG         NaN         DKI1  2010      1    1    4          1                    1           0  New Year's Day   Friday  0.2023                     29.4   \n",
       "1                           NaN  TIDAK ADA DATA         NaN         DKI2  2010      1    1    4          1                    1           0  New Year's Day   Friday  0.0939                     29.4   \n",
       "2                           NaN  TIDAK ADA DATA         NaN         DKI3  2010      1    1    4          1                    1           0  New Year's Day   Friday  0.5332                     29.8   \n",
       "\n",
       "   wx_temperature_2m_min_c  wx_precipitation_sum_mm  wx_precipitation_hours_h  wx_wind_speed_10m_max_km_h  wx_wind_direction_10m_dominant  wx_shortwave_radiation_sum_mj_m²  wx_temperature_2m_mean_c  \\\n",
       "0                     24.4                      4.0                      14.0                        16.0                           246.0                             16.24                      26.6   \n",
       "1                     24.6                      5.2                      10.0                        16.5                           255.0                             16.85                      26.7   \n",
       "2                     23.7                      4.0                      14.0                        16.0                           246.0                             16.24                      26.1   \n",
       "\n",
       "   wx_relative_humidity_2m_mean  wx_cloud_cover_mean  wx_surface_pressure_mean_hpa  wx_wind_gusts_10m_max_km_h  wx_winddirection_10m_dominant  wx_relative_humidity_2m_max  \\\n",
       "0                          81.0                100.0                        1007.5                        38.2                          246.0                         90.0   \n",
       "1                          81.0                100.0                        1007.1                        38.2                          255.0                         89.0   \n",
       "2                          85.0                100.0                         999.9                        38.2                          246.0                         94.0   \n",
       "\n",
       "   wx_relative_humidity_2m_min  wx_cloud_cover_max  wx_cloud_cover_min  wx_wind_gusts_10m_mean_km_h  wx_wind_speed_10m_mean_km_h  wx_wind_gusts_10m_min_km_h  wx_wind_speed_10m_min_km_h  \\\n",
       "0                         69.0               100.0                99.0                         21.0                         10.5                        11.9                         6.9   \n",
       "1                         69.0               100.0                99.0                         21.1                         10.6                        11.9                         7.4   \n",
       "2                         71.0               100.0                99.0                         21.0                         10.5                        11.9                         6.9   \n",
       "\n",
       "   wx_surface_pressure_max_hpa  wx_surface_pressure_min_hpa wx_weather_station  wxg_temperature_2m_max_c  wxg_temperature_2m_min_c  wxg_precipitation_sum_mm  wxg_precipitation_hours_h  \\\n",
       "0                       1009.3                       1005.1    dki1-bundaranhi                     29.58                     24.24                      4.48                       12.4   \n",
       "1                       1009.0                       1004.9  dki2-kelapagading                     29.58                     24.24                      4.48                       12.4   \n",
       "2                       1001.8                        997.6     dki3-jagakarsa                     29.58                     24.24                      4.48                       12.4   \n",
       "\n",
       "   wxg_wind_speed_10m_max_km_h  wxg_wind_direction_10m_dominant  wxg_shortwave_radiation_sum_mj_m²  wxg_temperature_2m_mean_c  wxg_relative_humidity_2m_mean  wxg_cloud_cover_mean  \\\n",
       "0                         16.2                            249.6                             16.484                      26.48                           82.2                 100.0   \n",
       "1                         16.2                            249.6                             16.484                      26.48                           82.2                 100.0   \n",
       "2                         16.2                            249.6                             16.484                      26.48                           82.2                 100.0   \n",
       "\n",
       "   wxg_surface_pressure_mean_hpa  wxg_wind_gusts_10m_max_km_h  wxg_winddirection_10m_dominant  wxg_relative_humidity_2m_max  wxg_relative_humidity_2m_min  wxg_cloud_cover_max  wxg_cloud_cover_min  \\\n",
       "0                        1004.98                         38.2                           249.6                          90.8                          69.6                100.0                 99.0   \n",
       "1                        1004.98                         38.2                           249.6                          90.8                          69.6                100.0                 99.0   \n",
       "2                        1004.98                         38.2                           249.6                          90.8                          69.6                100.0                 99.0   \n",
       "\n",
       "   wxg_wind_gusts_10m_mean_km_h  wxg_wind_speed_10m_mean_km_h  wxg_wind_gusts_10m_min_km_h  wxg_wind_speed_10m_min_km_h  wxg_surface_pressure_max_hpa  wxg_surface_pressure_min_hpa  pop_total_year  \\\n",
       "0                         21.04                         10.54                         11.9                          7.1                       1006.86                        1002.7             NaN   \n",
       "1                         21.04                         10.54                         11.9                          7.1                       1006.86                        1002.7             NaN   \n",
       "2                         21.04                         10.54                         11.9                          7.1                       1006.86                        1002.7             NaN   \n",
       "\n",
       "   river_exceed_rate  river_ratio_mean  river_n  \n",
       "0                NaN               NaN      NaN  \n",
       "1                NaN               NaN      NaN  \n",
       "2                NaN               NaN      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2 — Master Table Building (Correct Joins) (ONE CELL)\n",
    "# Builds:\n",
    "#   df_train_master  (for training)\n",
    "#   df_test_master   (for inference; requires test mapping file with id)\n",
    "# Notes:\n",
    "# - Joins are leakage-safe (no future info used here; lags/rolling will be Step 3)\n",
    "# - Uses safe joins: Holiday (by date), NDVI (date+stasiun_code), Weather (date+stasiun_code + global fallback),\n",
    "#   Population (year aggregate), River (year-month aggregate, global)\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- guards (assumes Step 1 already ran) ----\n",
    "need = [\"sub\",\"ID_COL\",\"SUB_TARGET_COL\",\"df_train\",\"df_ndvi\",\"df_holiday\",\"df_weather\",\"df_pop\",\"df_river\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from Step 1: {miss}. Jalankan Step 1 dulu.\")\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n",
    "\n",
    "def _norm_col(c: str) -> str:\n",
    "    c = str(c).strip().lower()\n",
    "    c = re.sub(r\"[^\\w]+\", \"_\", c)\n",
    "    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "    return c\n",
    "\n",
    "def _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.rename(columns={c: _norm_col(c) for c in df.columns})\n",
    "\n",
    "def parse_date_twopass(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"None\": np.nan})\n",
    "    d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    m = d1.isna()\n",
    "    if m.any():\n",
    "        d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n",
    "        d1.loc[m] = d2\n",
    "    return d1\n",
    "\n",
    "def _mk_stasiun_code(stasiun_series: pd.Series) -> pd.Series:\n",
    "    st = stasiun_series.astype(str).str.strip().str.upper()\n",
    "    code = st.str.extract(r\"(DKI\\s*\\d+)\", expand=False).str.replace(\" \", \"\", regex=False)\n",
    "    return code\n",
    "\n",
    "def _as_numeric_cols(df: pd.DataFrame) -> list:\n",
    "    num_cols = []\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            num_cols.append(c)\n",
    "    return num_cols\n",
    "\n",
    "def _prefix_cols(df: pd.DataFrame, prefix: str, keep: set) -> pd.DataFrame:\n",
    "    ren = {c: f\"{prefix}{c}\" for c in df.columns if c not in keep}\n",
    "    return df.rename(columns=ren)\n",
    "\n",
    "def _find_test_mapping_file(data_root: Path, n_rows: int) -> Path | None:\n",
    "    # Cari CSV (selain sample_submission) yang punya kolom id dan baris == n_rows.\n",
    "    for p in data_root.rglob(\"*.csv\"):\n",
    "        if p.name == \"sample_submission.csv\":\n",
    "            continue\n",
    "        try:\n",
    "            head = _standardize_columns(pd.read_csv(p, nrows=5))\n",
    "            if \"id\" not in head.columns:\n",
    "                continue\n",
    "            # quick row count (read only id col if possible)\n",
    "            df0 = _standardize_columns(pd.read_csv(p, usecols=[\"id\"]))\n",
    "            if len(df0) == n_rows:\n",
    "                return p\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def build_master(df_base: pd.DataFrame, *, has_target: bool, test_mode: bool=False) -> pd.DataFrame:\n",
    "    df = df_base.copy()\n",
    "\n",
    "    # --- ensure tanggal + stasiun_code ---\n",
    "    if \"tanggal\" not in df.columns:\n",
    "        raise RuntimeError(\"Base df missing 'tanggal'.\")\n",
    "    if not np.issubdtype(df[\"tanggal\"].dtype, np.datetime64):\n",
    "        df[\"tanggal\"] = parse_date_twopass(df[\"tanggal\"])\n",
    "\n",
    "    if \"stasiun_code\" not in df.columns:\n",
    "        if \"stasiun\" in df.columns:\n",
    "            df[\"stasiun_code\"] = _mk_stasiun_code(df[\"stasiun\"])\n",
    "        else:\n",
    "            df[\"stasiun_code\"] = np.nan\n",
    "\n",
    "    # --- basic calendar features (safe) ---\n",
    "    df[\"year\"] = df[\"tanggal\"].dt.year.astype(\"Int64\")\n",
    "    df[\"month\"] = df[\"tanggal\"].dt.month.astype(\"Int64\")\n",
    "    df[\"day\"] = df[\"tanggal\"].dt.day.astype(\"Int64\")\n",
    "    df[\"dow\"] = df[\"tanggal\"].dt.dayofweek.astype(\"Int64\")\n",
    "    df[\"dayofyear\"] = df[\"tanggal\"].dt.dayofyear.astype(\"Int64\")\n",
    "\n",
    "    # --- holidays (by tanggal) ---\n",
    "    hol = df_holiday.copy()\n",
    "    if not np.issubdtype(hol[\"tanggal\"].dtype, np.datetime64):\n",
    "        hol[\"tanggal\"] = parse_date_twopass(hol[\"tanggal\"])\n",
    "    hol = hol.dropna(subset=[\"tanggal\"]).drop_duplicates([\"tanggal\"])\n",
    "    df = df.merge(hol, on=\"tanggal\", how=\"left\")\n",
    "\n",
    "    # --- NDVI (by tanggal + stasiun_code) ---\n",
    "    nd = df_ndvi.copy()\n",
    "    if \"stasiun_code\" not in nd.columns and \"stasiun\" in nd.columns:\n",
    "        nd[\"stasiun_code\"] = _mk_stasiun_code(nd[\"stasiun\"])\n",
    "    if not np.issubdtype(nd[\"tanggal\"].dtype, np.datetime64):\n",
    "        nd[\"tanggal\"] = parse_date_twopass(nd[\"tanggal\"])\n",
    "    nd = nd.dropna(subset=[\"tanggal\", \"stasiun_code\"]).drop_duplicates([\"tanggal\",\"stasiun_code\"])\n",
    "    nd = nd[[\"tanggal\",\"stasiun_code\"] + [c for c in nd.columns if c not in [\"tanggal\",\"stasiun\",\"stasiun_code\"]]]\n",
    "    df = df.merge(nd, on=[\"tanggal\",\"stasiun_code\"], how=\"left\")\n",
    "\n",
    "    # --- Weather: station-specific (tanggal + stasiun_code) + global fallback (tanggal) ---\n",
    "    wx = df_weather.copy()\n",
    "    if not np.issubdtype(wx[\"tanggal\"].dtype, np.datetime64):\n",
    "        wx[\"tanggal\"] = parse_date_twopass(wx[\"tanggal\"])\n",
    "    if \"weather_code\" in wx.columns:\n",
    "        wx[\"weather_code\"] = wx[\"weather_code\"].astype(str).str.strip().str.upper()\n",
    "    else:\n",
    "        wx[\"weather_code\"] = np.nan\n",
    "\n",
    "    # station-specific\n",
    "    wx_loc = wx.dropna(subset=[\"tanggal\",\"weather_code\"]).copy()\n",
    "    wx_loc = wx_loc.rename(columns={\"weather_code\":\"stasiun_code\"})\n",
    "    # keep numeric + minimal keys\n",
    "    keep_keys = {\"tanggal\",\"stasiun_code\",\"weather_station\"}\n",
    "    wx_loc_num = [c for c in wx_loc.columns if c in keep_keys or pd.api.types.is_numeric_dtype(wx_loc[c])]\n",
    "    wx_loc = wx_loc[wx_loc_num].drop_duplicates([\"tanggal\",\"stasiun_code\"])\n",
    "    wx_loc = _prefix_cols(wx_loc, \"wx_\", keep={\"tanggal\",\"stasiun_code\"})\n",
    "    df = df.merge(wx_loc, on=[\"tanggal\",\"stasiun_code\"], how=\"left\")\n",
    "\n",
    "    # global by date (mean numeric across stations)\n",
    "    wx_g = wx.dropna(subset=[\"tanggal\"]).copy()\n",
    "    wx_g_num = [c for c in wx_g.columns if pd.api.types.is_numeric_dtype(wx_g[c])]\n",
    "    wx_g = wx_g.groupby(\"tanggal\", as_index=False)[wx_g_num].mean(numeric_only=True)\n",
    "    wx_g = _prefix_cols(wx_g, \"wxg_\", keep={\"tanggal\"})\n",
    "    df = df.merge(wx_g, on=\"tanggal\", how=\"left\")\n",
    "\n",
    "    # fill local weather NaNs using global fallback for same base variable\n",
    "    # (only for columns that exist in both)\n",
    "    for c in list(df.columns):\n",
    "        if c.startswith(\"wx_\"):\n",
    "            base = c.replace(\"wx_\", \"\")\n",
    "            cg = \"wxg_\" + base\n",
    "            if cg in df.columns:\n",
    "                df[c] = df[c].fillna(df[cg])\n",
    "\n",
    "    # --- Population: total per year (global) ---\n",
    "    pop = df_pop.copy()\n",
    "    # expected cols: tahun, jumlah_penduduk\n",
    "    if \"tahun\" in pop.columns and \"jumlah_penduduk\" in pop.columns:\n",
    "        pop_y = pop.groupby(\"tahun\", as_index=False)[\"jumlah_penduduk\"].sum()\n",
    "        pop_y = pop_y.rename(columns={\"tahun\":\"year\",\"jumlah_penduduk\":\"pop_total_year\"})\n",
    "        df = df.merge(pop_y, on=\"year\", how=\"left\")\n",
    "\n",
    "    # --- River: global year-month aggregates ---\n",
    "    riv = df_river.copy()\n",
    "    # standard columns might be: periode_data (year), bulan_sampling, baku_mutu, hasil_pengukuran\n",
    "    for cc in [\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"]:\n",
    "        if cc in riv.columns:\n",
    "            riv[cc] = pd.to_numeric(riv[cc], errors=\"coerce\")\n",
    "\n",
    "    if {\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"}.issubset(riv.columns):\n",
    "        r = riv.dropna(subset=[\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"]).copy()\n",
    "        r[\"ratio_to_std\"] = r[\"hasil_pengukuran\"] / (r[\"baku_mutu\"].replace(0, np.nan))\n",
    "        r[\"exceed\"] = (r[\"hasil_pengukuran\"] > r[\"baku_mutu\"]).astype(int)\n",
    "        r_agg = r.groupby([\"periode_data\",\"bulan_sampling\"], as_index=False).agg(\n",
    "            river_exceed_rate=(\"exceed\",\"mean\"),\n",
    "            river_ratio_mean=(\"ratio_to_std\",\"mean\"),\n",
    "            river_n=(\"exceed\",\"size\"),\n",
    "        )\n",
    "        r_agg = r_agg.rename(columns={\"periode_data\":\"year\", \"bulan_sampling\":\"month\"})\n",
    "        df = df.merge(r_agg, on=[\"year\",\"month\"], how=\"left\")\n",
    "\n",
    "    # --- cleanup: categorical columns kept as object for CatBoost later ---\n",
    "    for c in [\"stasiun\",\"stasiun_code\",\"parameter_pencemar_kritis\",\"day_name\",\"nama_libur\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "\n",
    "    # --- final sanity ---\n",
    "    key_cols = [\"tanggal\",\"stasiun_code\"] if \"stasiun_code\" in df.columns else [\"tanggal\"]\n",
    "    dup = int(df.duplicated(key_cols).sum())\n",
    "    if dup > 0:\n",
    "        # keep most complete if duplicates remain\n",
    "        df[\"_nn\"] = df.notna().sum(axis=1)\n",
    "        idx = df.groupby(key_cols)[\"_nn\"].idxmax()\n",
    "        df = df.loc[idx].drop(columns=[\"_nn\"]).reset_index(drop=True)\n",
    "\n",
    "    if has_target and \"kategori\" in df.columns:\n",
    "        df[\"kategori\"] = df[\"kategori\"].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Build TRAIN master\n",
    "# ----------------------------\n",
    "df_train_master = build_master(df_train, has_target=True)\n",
    "print(\"df_train_master:\", df_train_master.shape)\n",
    "print(\"train key duplicates:\", int(df_train_master.duplicated([\"tanggal\",\"stasiun_code\"]).sum()) if \"stasiun_code\" in df_train_master.columns else 0)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Build TEST master (needs id mapping file)\n",
    "# ----------------------------\n",
    "n_test_expected = len(sub)\n",
    "\n",
    "test_path = None\n",
    "# If Step 1 created test_candidates, try it first\n",
    "if \"test_candidates\" in globals() and isinstance(test_candidates, list) and len(test_candidates) > 0:\n",
    "    test_path = Path(test_candidates[0][0])\n",
    "else:\n",
    "    test_path = _find_test_mapping_file(DATA_ROOT, n_test_expected)\n",
    "\n",
    "if test_path is None:\n",
    "    print(\"\\n[WARN] Test mapping file (id -> tanggal/stasiun) not found yet.\")\n",
    "    print(\"       You can still continue feature engineering + CV on df_train_master.\")\n",
    "    df_test_master = None\n",
    "else:\n",
    "    print(\"\\nTest mapping file:\", str(test_path))\n",
    "    df_test = _standardize_columns(pd.read_csv(test_path))\n",
    "    # ensure id exists and length matches submission\n",
    "    if \"id\" not in df_test.columns:\n",
    "        raise RuntimeError(f\"Test file has no 'id': {test_path}\")\n",
    "    if len(df_test) != n_test_expected:\n",
    "        raise RuntimeError(f\"Test file rows != submission rows: {len(df_test)} vs {n_test_expected}\")\n",
    "\n",
    "    # normalize date/station fields\n",
    "    if \"tanggal\" in df_test.columns:\n",
    "        df_test[\"tanggal\"] = parse_date_twopass(df_test[\"tanggal\"])\n",
    "    else:\n",
    "        raise RuntimeError(\"Test mapping file missing 'tanggal'.\")\n",
    "\n",
    "    if \"stasiun_code\" not in df_test.columns:\n",
    "        if \"stasiun\" in df_test.columns:\n",
    "            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[\"stasiun\"])\n",
    "        elif \"stasiun_id\" in df_test.columns:\n",
    "            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[\"stasiun_id\"])\n",
    "        else:\n",
    "            # some datasets store only DKI1..DKI5\n",
    "            # try infer from any column containing 'dki'\n",
    "            cand = None\n",
    "            for c in df_test.columns:\n",
    "                if df_test[c].astype(str).str.contains(\"dki\", case=False, na=False).any():\n",
    "                    cand = c\n",
    "                    break\n",
    "            if cand is None:\n",
    "                raise RuntimeError(\"Cannot infer station from test mapping file.\")\n",
    "            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[cand])\n",
    "\n",
    "    df_test_master = build_master(df_test, has_target=False, test_mode=True)\n",
    "    print(\"df_test_master:\", df_test_master.shape)\n",
    "    # keep id for submission\n",
    "    if \"id\" not in df_test_master.columns:\n",
    "        df_test_master[\"id\"] = df_test[\"id\"].values\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Quick preview\n",
    "# ----------------------------\n",
    "display(df_train_master.head(3))\n",
    "if df_test_master is not None:\n",
    "    display(df_test_master.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8138e",
   "metadata": {
    "papermill": {
     "duration": 0.004531,
     "end_time": "2026-01-30T15:42:50.760748",
     "exception": false,
     "start_time": "2026-01-30T15:42:50.756217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering (Time-Series + Calendar + Robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd556af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T15:42:50.771879Z",
     "iopub.status.busy": "2026-01-30T15:42:50.771554Z",
     "iopub.status.idle": "2026-01-30T15:42:51.086911Z",
     "shell.execute_reply": "2026-01-30T15:42:51.085900Z"
    },
    "papermill": {
     "duration": 0.323787,
     "end_time": "2026-01-30T15:42:51.089055",
     "exception": false,
     "start_time": "2026-01-30T15:42:50.765268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_fe: (13651, 194)\n",
      "\n",
      "Top missing% (train_fe):\n",
      "bulan                 100.00\n",
      "river_exceed_rate     100.00\n",
      "river_n               100.00\n",
      "river_ratio_mean      100.00\n",
      "nama_libur             95.69\n",
      "ndvi                   95.04\n",
      "lokasi_spku            78.96\n",
      "pm25_lag14             76.65\n",
      "pm25_d12               76.59\n",
      "pm25_lag7              76.41\n",
      "pm25_lag3              76.27\n",
      "pm25_d1_rm7            76.26\n",
      "pm25_lag2              76.24\n",
      "pm25_lag1_div_prec     76.20\n",
      "pm25_lag1_x_wind       76.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n",
      "/tmp/ipykernel_24/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n",
      "/tmp/ipykernel_24/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n",
      "/tmp/ipykernel_24/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n",
      "/tmp/ipykernel_24/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n",
      "/tmp/ipykernel_24/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n",
      "/tmp/ipykernel_24/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n",
      "/tmp/ipykernel_24/761311308.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n",
      "/tmp/ipykernel_24/761311308.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periode_data</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>stasiun</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>so2</th>\n",
       "      <th>co</th>\n",
       "      <th>o3</th>\n",
       "      <th>no2</th>\n",
       "      <th>max</th>\n",
       "      <th>parameter_pencemar_kritis</th>\n",
       "      <th>kategori</th>\n",
       "      <th>source_file</th>\n",
       "      <th>bulan</th>\n",
       "      <th>parameter_pencemar_kritis_alt</th>\n",
       "      <th>kategori_alt</th>\n",
       "      <th>lokasi_spku</th>\n",
       "      <th>stasiun_code</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dow</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>is_holiday_nasional</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>nama_libur</th>\n",
       "      <th>day_name</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>wx_temperature_2m_max_c</th>\n",
       "      <th>wx_temperature_2m_min_c</th>\n",
       "      <th>wx_precipitation_sum_mm</th>\n",
       "      <th>wx_precipitation_hours_h</th>\n",
       "      <th>wx_wind_speed_10m_max_km_h</th>\n",
       "      <th>wx_wind_direction_10m_dominant</th>\n",
       "      <th>wx_shortwave_radiation_sum_mj_m²</th>\n",
       "      <th>wx_temperature_2m_mean_c</th>\n",
       "      <th>wx_relative_humidity_2m_mean</th>\n",
       "      <th>wx_cloud_cover_mean</th>\n",
       "      <th>wx_surface_pressure_mean_hpa</th>\n",
       "      <th>wx_wind_gusts_10m_max_km_h</th>\n",
       "      <th>wx_winddirection_10m_dominant</th>\n",
       "      <th>wx_relative_humidity_2m_max</th>\n",
       "      <th>wx_relative_humidity_2m_min</th>\n",
       "      <th>wx_cloud_cover_max</th>\n",
       "      <th>wx_cloud_cover_min</th>\n",
       "      <th>wx_wind_gusts_10m_mean_km_h</th>\n",
       "      <th>wx_wind_speed_10m_mean_km_h</th>\n",
       "      <th>wx_wind_gusts_10m_min_km_h</th>\n",
       "      <th>wx_wind_speed_10m_min_km_h</th>\n",
       "      <th>wx_surface_pressure_max_hpa</th>\n",
       "      <th>wx_surface_pressure_min_hpa</th>\n",
       "      <th>wx_weather_station</th>\n",
       "      <th>wxg_temperature_2m_max_c</th>\n",
       "      <th>wxg_temperature_2m_min_c</th>\n",
       "      <th>wxg_precipitation_sum_mm</th>\n",
       "      <th>wxg_precipitation_hours_h</th>\n",
       "      <th>wxg_wind_speed_10m_max_km_h</th>\n",
       "      <th>wxg_wind_direction_10m_dominant</th>\n",
       "      <th>wxg_shortwave_radiation_sum_mj_m²</th>\n",
       "      <th>wxg_temperature_2m_mean_c</th>\n",
       "      <th>wxg_relative_humidity_2m_mean</th>\n",
       "      <th>wxg_cloud_cover_mean</th>\n",
       "      <th>wxg_surface_pressure_mean_hpa</th>\n",
       "      <th>wxg_wind_gusts_10m_max_km_h</th>\n",
       "      <th>wxg_winddirection_10m_dominant</th>\n",
       "      <th>wxg_relative_humidity_2m_max</th>\n",
       "      <th>wxg_relative_humidity_2m_min</th>\n",
       "      <th>wxg_cloud_cover_max</th>\n",
       "      <th>wxg_cloud_cover_min</th>\n",
       "      <th>wxg_wind_gusts_10m_mean_km_h</th>\n",
       "      <th>wxg_wind_speed_10m_mean_km_h</th>\n",
       "      <th>wxg_wind_gusts_10m_min_km_h</th>\n",
       "      <th>wxg_wind_speed_10m_min_km_h</th>\n",
       "      <th>wxg_surface_pressure_max_hpa</th>\n",
       "      <th>wxg_surface_pressure_min_hpa</th>\n",
       "      <th>pop_total_year</th>\n",
       "      <th>river_exceed_rate</th>\n",
       "      <th>river_ratio_mean</th>\n",
       "      <th>river_n</th>\n",
       "      <th>doy_sin</th>\n",
       "      <th>doy_cos</th>\n",
       "      <th>mon_sin</th>\n",
       "      <th>mon_cos</th>\n",
       "      <th>pm10_lag1</th>\n",
       "      <th>pm10_lag2</th>\n",
       "      <th>pm10_lag3</th>\n",
       "      <th>pm10_lag7</th>\n",
       "      <th>pm10_lag14</th>\n",
       "      <th>pm25_lag1</th>\n",
       "      <th>pm25_lag2</th>\n",
       "      <th>pm25_lag3</th>\n",
       "      <th>pm25_lag7</th>\n",
       "      <th>pm25_lag14</th>\n",
       "      <th>so2_lag1</th>\n",
       "      <th>so2_lag2</th>\n",
       "      <th>so2_lag3</th>\n",
       "      <th>so2_lag7</th>\n",
       "      <th>so2_lag14</th>\n",
       "      <th>co_lag1</th>\n",
       "      <th>co_lag2</th>\n",
       "      <th>co_lag3</th>\n",
       "      <th>co_lag7</th>\n",
       "      <th>co_lag14</th>\n",
       "      <th>o3_lag1</th>\n",
       "      <th>o3_lag2</th>\n",
       "      <th>o3_lag3</th>\n",
       "      <th>o3_lag7</th>\n",
       "      <th>o3_lag14</th>\n",
       "      <th>no2_lag1</th>\n",
       "      <th>no2_lag2</th>\n",
       "      <th>no2_lag3</th>\n",
       "      <th>no2_lag7</th>\n",
       "      <th>no2_lag14</th>\n",
       "      <th>max_lag1</th>\n",
       "      <th>max_lag2</th>\n",
       "      <th>max_lag3</th>\n",
       "      <th>max_lag7</th>\n",
       "      <th>max_lag14</th>\n",
       "      <th>pm10_rmean3</th>\n",
       "      <th>pm10_rstd3</th>\n",
       "      <th>pm10_rmean7</th>\n",
       "      <th>pm10_rstd7</th>\n",
       "      <th>pm10_rmean14</th>\n",
       "      <th>pm10_rstd14</th>\n",
       "      <th>pm10_rmean30</th>\n",
       "      <th>pm10_rstd30</th>\n",
       "      <th>pm25_rmean3</th>\n",
       "      <th>pm25_rstd3</th>\n",
       "      <th>pm25_rmean7</th>\n",
       "      <th>pm25_rstd7</th>\n",
       "      <th>pm25_rmean14</th>\n",
       "      <th>pm25_rstd14</th>\n",
       "      <th>pm25_rmean30</th>\n",
       "      <th>pm25_rstd30</th>\n",
       "      <th>so2_rmean3</th>\n",
       "      <th>so2_rstd3</th>\n",
       "      <th>so2_rmean7</th>\n",
       "      <th>so2_rstd7</th>\n",
       "      <th>so2_rmean14</th>\n",
       "      <th>so2_rstd14</th>\n",
       "      <th>so2_rmean30</th>\n",
       "      <th>so2_rstd30</th>\n",
       "      <th>co_rmean3</th>\n",
       "      <th>co_rstd3</th>\n",
       "      <th>co_rmean7</th>\n",
       "      <th>co_rstd7</th>\n",
       "      <th>co_rmean14</th>\n",
       "      <th>co_rstd14</th>\n",
       "      <th>co_rmean30</th>\n",
       "      <th>co_rstd30</th>\n",
       "      <th>o3_rmean3</th>\n",
       "      <th>o3_rstd3</th>\n",
       "      <th>o3_rmean7</th>\n",
       "      <th>o3_rstd7</th>\n",
       "      <th>o3_rmean14</th>\n",
       "      <th>o3_rstd14</th>\n",
       "      <th>o3_rmean30</th>\n",
       "      <th>o3_rstd30</th>\n",
       "      <th>no2_rmean3</th>\n",
       "      <th>no2_rstd3</th>\n",
       "      <th>no2_rmean7</th>\n",
       "      <th>no2_rstd7</th>\n",
       "      <th>no2_rmean14</th>\n",
       "      <th>no2_rstd14</th>\n",
       "      <th>no2_rmean30</th>\n",
       "      <th>no2_rstd30</th>\n",
       "      <th>max_rmean3</th>\n",
       "      <th>max_rstd3</th>\n",
       "      <th>max_rmean7</th>\n",
       "      <th>max_rstd7</th>\n",
       "      <th>max_rmean14</th>\n",
       "      <th>max_rstd14</th>\n",
       "      <th>max_rmean30</th>\n",
       "      <th>max_rstd30</th>\n",
       "      <th>pm10_d12</th>\n",
       "      <th>pm10_d1_rm7</th>\n",
       "      <th>pm25_d12</th>\n",
       "      <th>pm25_d1_rm7</th>\n",
       "      <th>so2_d12</th>\n",
       "      <th>so2_d1_rm7</th>\n",
       "      <th>co_d12</th>\n",
       "      <th>co_d1_rm7</th>\n",
       "      <th>o3_d12</th>\n",
       "      <th>o3_d1_rm7</th>\n",
       "      <th>no2_d12</th>\n",
       "      <th>no2_d1_rm7</th>\n",
       "      <th>max_d12</th>\n",
       "      <th>max_d1_rm7</th>\n",
       "      <th>pm25_lag1_x_wind</th>\n",
       "      <th>pm10_lag1_x_wind</th>\n",
       "      <th>o3_lag1_x_rad</th>\n",
       "      <th>pm25_lag1_div_prec</th>\n",
       "      <th>co_lag1_x_rh</th>\n",
       "      <th>pm25_lag1_x_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CO</td>\n",
       "      <td>SEDANG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>29.4</td>\n",
       "      <td>24.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>16.24</td>\n",
       "      <td>26.6</td>\n",
       "      <td>81.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1007.5</td>\n",
       "      <td>38.2</td>\n",
       "      <td>246.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1009.3</td>\n",
       "      <td>1005.1</td>\n",
       "      <td>dki1-bundaranhi</td>\n",
       "      <td>29.58</td>\n",
       "      <td>24.24</td>\n",
       "      <td>4.48</td>\n",
       "      <td>12.4</td>\n",
       "      <td>16.20</td>\n",
       "      <td>249.6</td>\n",
       "      <td>16.484</td>\n",
       "      <td>26.48</td>\n",
       "      <td>82.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1004.98</td>\n",
       "      <td>38.20</td>\n",
       "      <td>249.6</td>\n",
       "      <td>90.8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>21.04</td>\n",
       "      <td>10.54</td>\n",
       "      <td>11.90</td>\n",
       "      <td>7.10</td>\n",
       "      <td>1006.86</td>\n",
       "      <td>1002.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>O3</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O3</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.9</td>\n",
       "      <td>24.2</td>\n",
       "      <td>6.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>260.0</td>\n",
       "      <td>13.01</td>\n",
       "      <td>26.2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1011.9</td>\n",
       "      <td>1007.4</td>\n",
       "      <td>dki1-bundaranhi</td>\n",
       "      <td>29.02</td>\n",
       "      <td>23.88</td>\n",
       "      <td>7.14</td>\n",
       "      <td>14.4</td>\n",
       "      <td>9.50</td>\n",
       "      <td>264.8</td>\n",
       "      <td>12.666</td>\n",
       "      <td>26.08</td>\n",
       "      <td>85.8</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1007.68</td>\n",
       "      <td>23.28</td>\n",
       "      <td>264.8</td>\n",
       "      <td>95.6</td>\n",
       "      <td>72.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.2</td>\n",
       "      <td>13.90</td>\n",
       "      <td>6.08</td>\n",
       "      <td>8.76</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1009.44</td>\n",
       "      <td>1005.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034398</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>351.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6205.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201001</td>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>DKI1 (Bunderan HI)</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>PM10</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>indeks-standar-pencemaran-udara-(ispu)-tahun-2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PM10</td>\n",
       "      <td>BAIK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DKI1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.4</td>\n",
       "      <td>24.9</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>224.0</td>\n",
       "      <td>23.89</td>\n",
       "      <td>27.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1009.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>224.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1012.2</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>dki1-bundaranhi</td>\n",
       "      <td>31.46</td>\n",
       "      <td>24.34</td>\n",
       "      <td>12.16</td>\n",
       "      <td>6.8</td>\n",
       "      <td>9.12</td>\n",
       "      <td>218.0</td>\n",
       "      <td>23.986</td>\n",
       "      <td>26.98</td>\n",
       "      <td>85.6</td>\n",
       "      <td>93.8</td>\n",
       "      <td>1007.48</td>\n",
       "      <td>22.24</td>\n",
       "      <td>218.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>70.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.4</td>\n",
       "      <td>15.50</td>\n",
       "      <td>5.58</td>\n",
       "      <td>8.42</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1009.76</td>\n",
       "      <td>1004.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051584</td>\n",
       "      <td>0.998669</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>32.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>19.79899</td>\n",
       "      <td>46.0</td>\n",
       "      <td>19.79899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.5</td>\n",
       "      <td>40.305087</td>\n",
       "      <td>44.5</td>\n",
       "      <td>40.305087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.242641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>11.5</td>\n",
       "      <td>3.535534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>28.284271</td>\n",
       "      <td>53.0</td>\n",
       "      <td>28.284271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>-28.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.4</td>\n",
       "      <td>788.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   periode_data    tanggal             stasiun  pm10  pm25  so2    co    o3   no2   max parameter_pencemar_kritis kategori                                        source_file  bulan  \\\n",
       "0        201001 2010-01-01  DKI1 (Bunderan HI)  60.0   NaN  4.0  73.0  27.0  14.0  73.0                        CO   SEDANG  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "1        201001 2010-01-02  DKI1 (Bunderan HI)  32.0   NaN  2.0  16.0  33.0   9.0  33.0                        O3     BAIK  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "2        201001 2010-01-03  DKI1 (Bunderan HI)  27.0   NaN  2.0  19.0  20.0   9.0  27.0                      PM10     BAIK  indeks-standar-pencemaran-udara-(ispu)-tahun-2...    NaN   \n",
       "\n",
       "  parameter_pencemar_kritis_alt kategori_alt lokasi_spku stasiun_code  year  month  day  dow  dayofyear  is_holiday_nasional  is_weekend      nama_libur  day_name    ndvi  wx_temperature_2m_max_c  \\\n",
       "0                            CO       SEDANG         NaN         DKI1  2010      1    1    4          1                    1           0  New Year's Day    Friday  0.2023                     29.4   \n",
       "1                            O3         BAIK         NaN         DKI1  2010      1    2    5          2                    0           0             NaN  Saturday     NaN                     28.9   \n",
       "2                          PM10         BAIK         NaN         DKI1  2010      1    3    6          3                    0           0             NaN    Sunday     NaN                     31.4   \n",
       "\n",
       "   wx_temperature_2m_min_c  wx_precipitation_sum_mm  wx_precipitation_hours_h  wx_wind_speed_10m_max_km_h  wx_wind_direction_10m_dominant  wx_shortwave_radiation_sum_mj_m²  wx_temperature_2m_mean_c  \\\n",
       "0                     24.4                      4.0                      14.0                        16.0                           246.0                             16.24                      26.6   \n",
       "1                     24.2                      6.9                      14.0                         9.5                           260.0                             13.01                      26.2   \n",
       "2                     24.9                     11.2                       6.0                         9.4                           224.0                             23.89                      27.1   \n",
       "\n",
       "   wx_relative_humidity_2m_mean  wx_cloud_cover_mean  wx_surface_pressure_mean_hpa  wx_wind_gusts_10m_max_km_h  wx_winddirection_10m_dominant  wx_relative_humidity_2m_max  \\\n",
       "0                          81.0                100.0                        1007.5                        38.2                          246.0                         90.0   \n",
       "1                          85.0                 99.0                        1010.1                        22.0                          260.0                         95.0   \n",
       "2                          85.0                 93.0                        1009.9                        21.2                          224.0                         95.0   \n",
       "\n",
       "   wx_relative_humidity_2m_min  wx_cloud_cover_max  wx_cloud_cover_min  wx_wind_gusts_10m_mean_km_h  wx_wind_speed_10m_mean_km_h  wx_wind_gusts_10m_min_km_h  wx_wind_speed_10m_min_km_h  \\\n",
       "0                         69.0               100.0                99.0                         21.0                         10.5                        11.9                         6.9   \n",
       "1                         72.0               100.0                94.0                         13.7                          6.0                         8.6                         2.3   \n",
       "2                         70.0               100.0                28.0                         15.7                          5.7                         8.3                         1.6   \n",
       "\n",
       "   wx_surface_pressure_max_hpa  wx_surface_pressure_min_hpa wx_weather_station  wxg_temperature_2m_max_c  wxg_temperature_2m_min_c  wxg_precipitation_sum_mm  wxg_precipitation_hours_h  \\\n",
       "0                       1009.3                       1005.1    dki1-bundaranhi                     29.58                     24.24                      4.48                       12.4   \n",
       "1                       1011.9                       1007.4    dki1-bundaranhi                     29.02                     23.88                      7.14                       14.4   \n",
       "2                       1012.2                       1007.0    dki1-bundaranhi                     31.46                     24.34                     12.16                        6.8   \n",
       "\n",
       "   wxg_wind_speed_10m_max_km_h  wxg_wind_direction_10m_dominant  wxg_shortwave_radiation_sum_mj_m²  wxg_temperature_2m_mean_c  wxg_relative_humidity_2m_mean  wxg_cloud_cover_mean  \\\n",
       "0                        16.20                            249.6                             16.484                      26.48                           82.2                 100.0   \n",
       "1                         9.50                            264.8                             12.666                      26.08                           85.8                  99.0   \n",
       "2                         9.12                            218.0                             23.986                      26.98                           85.6                  93.8   \n",
       "\n",
       "   wxg_surface_pressure_mean_hpa  wxg_wind_gusts_10m_max_km_h  wxg_winddirection_10m_dominant  wxg_relative_humidity_2m_max  wxg_relative_humidity_2m_min  wxg_cloud_cover_max  wxg_cloud_cover_min  \\\n",
       "0                        1004.98                        38.20                           249.6                          90.8                          69.6                100.0                 99.0   \n",
       "1                        1007.68                        23.28                           264.8                          95.6                          72.0                100.0                 95.2   \n",
       "2                        1007.48                        22.24                           218.0                          96.0                          70.4                100.0                 40.4   \n",
       "\n",
       "   wxg_wind_gusts_10m_mean_km_h  wxg_wind_speed_10m_mean_km_h  wxg_wind_gusts_10m_min_km_h  wxg_wind_speed_10m_min_km_h  wxg_surface_pressure_max_hpa  wxg_surface_pressure_min_hpa  pop_total_year  \\\n",
       "0                         21.04                         10.54                        11.90                         7.10                       1006.86                       1002.70             NaN   \n",
       "1                         13.90                          6.08                         8.76                         1.94                       1009.44                       1005.04             NaN   \n",
       "2                         15.50                          5.58                         8.42                         1.68                       1009.76                       1004.78             NaN   \n",
       "\n",
       "   river_exceed_rate  river_ratio_mean  river_n   doy_sin   doy_cos  mon_sin   mon_cos  pm10_lag1  pm10_lag2  pm10_lag3  pm10_lag7  pm10_lag14  pm25_lag1  pm25_lag2  pm25_lag3  pm25_lag7  \\\n",
       "0                NaN               NaN      NaN  0.017202  0.999852      0.5  0.866025        NaN        NaN        NaN        NaN         NaN        NaN        NaN        NaN        NaN   \n",
       "1                NaN               NaN      NaN  0.034398  0.999408      0.5  0.866025       60.0        NaN        NaN        NaN         NaN        NaN        NaN        NaN        NaN   \n",
       "2                NaN               NaN      NaN  0.051584  0.998669      0.5  0.866025       32.0       60.0        NaN        NaN         NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "   pm25_lag14  so2_lag1  so2_lag2  so2_lag3  so2_lag7  so2_lag14  co_lag1  co_lag2  co_lag3  co_lag7  co_lag14  o3_lag1  o3_lag2  o3_lag3  o3_lag7  o3_lag14  no2_lag1  no2_lag2  no2_lag3  no2_lag7  \\\n",
       "0         NaN       NaN       NaN       NaN       NaN        NaN      NaN      NaN      NaN      NaN       NaN      NaN      NaN      NaN      NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1         NaN       4.0       NaN       NaN       NaN        NaN     73.0      NaN      NaN      NaN       NaN     27.0      NaN      NaN      NaN       NaN      14.0       NaN       NaN       NaN   \n",
       "2         NaN       2.0       4.0       NaN       NaN        NaN     16.0     73.0      NaN      NaN       NaN     33.0     27.0      NaN      NaN       NaN       9.0      14.0       NaN       NaN   \n",
       "\n",
       "   no2_lag14  max_lag1  max_lag2  max_lag3  max_lag7  max_lag14  pm10_rmean3  pm10_rstd3  pm10_rmean7  pm10_rstd7  pm10_rmean14  pm10_rstd14  pm10_rmean30  pm10_rstd30  pm25_rmean3  pm25_rstd3  \\\n",
       "0        NaN       NaN       NaN       NaN       NaN        NaN          NaN         NaN          NaN         NaN           NaN          NaN           NaN          NaN          NaN         NaN   \n",
       "1        NaN      73.0       NaN       NaN       NaN        NaN          NaN         NaN          NaN         NaN           NaN          NaN           NaN          NaN          NaN         NaN   \n",
       "2        NaN      33.0      73.0       NaN       NaN        NaN         46.0    19.79899         46.0    19.79899           NaN          NaN           NaN          NaN          NaN         NaN   \n",
       "\n",
       "   pm25_rmean7  pm25_rstd7  pm25_rmean14  pm25_rstd14  pm25_rmean30  pm25_rstd30  so2_rmean3  so2_rstd3  so2_rmean7  so2_rstd7  so2_rmean14  so2_rstd14  so2_rmean30  so2_rstd30  co_rmean3  \\\n",
       "0          NaN         NaN           NaN          NaN           NaN          NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN        NaN   \n",
       "1          NaN         NaN           NaN          NaN           NaN          NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN        NaN   \n",
       "2          NaN         NaN           NaN          NaN           NaN          NaN         3.0   1.414214         3.0   1.414214          NaN         NaN          NaN         NaN       44.5   \n",
       "\n",
       "    co_rstd3  co_rmean7   co_rstd7  co_rmean14  co_rstd14  co_rmean30  co_rstd30  o3_rmean3  o3_rstd3  o3_rmean7  o3_rstd7  o3_rmean14  o3_rstd14  o3_rmean30  o3_rstd30  no2_rmean3  no2_rstd3  \\\n",
       "0        NaN        NaN        NaN         NaN        NaN         NaN        NaN        NaN       NaN        NaN       NaN         NaN        NaN         NaN        NaN         NaN        NaN   \n",
       "1        NaN        NaN        NaN         NaN        NaN         NaN        NaN        NaN       NaN        NaN       NaN         NaN        NaN         NaN        NaN         NaN        NaN   \n",
       "2  40.305087       44.5  40.305087         NaN        NaN         NaN        NaN       30.0  4.242641       30.0  4.242641         NaN        NaN         NaN        NaN        11.5   3.535534   \n",
       "\n",
       "   no2_rmean7  no2_rstd7  no2_rmean14  no2_rstd14  no2_rmean30  no2_rstd30  max_rmean3  max_rstd3  max_rmean7  max_rstd7  max_rmean14  max_rstd14  max_rmean30  max_rstd30  pm10_d12  pm10_d1_rm7  \\\n",
       "0         NaN        NaN          NaN         NaN          NaN         NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN       NaN          NaN   \n",
       "1         NaN        NaN          NaN         NaN          NaN         NaN         NaN        NaN         NaN        NaN          NaN         NaN          NaN         NaN       NaN          NaN   \n",
       "2        11.5   3.535534          NaN         NaN          NaN         NaN        53.0  28.284271        53.0  28.284271          NaN         NaN          NaN         NaN     -28.0        -14.0   \n",
       "\n",
       "   pm25_d12  pm25_d1_rm7  so2_d12  so2_d1_rm7  co_d12  co_d1_rm7  o3_d12  o3_d1_rm7  no2_d12  no2_d1_rm7  max_d12  max_d1_rm7  pm25_lag1_x_wind  pm10_lag1_x_wind  o3_lag1_x_rad  pm25_lag1_div_prec  \\\n",
       "0       NaN          NaN      NaN         NaN     NaN        NaN     NaN        NaN      NaN         NaN      NaN         NaN               NaN               NaN            NaN                 NaN   \n",
       "1       NaN          NaN      NaN         NaN     NaN        NaN     NaN        NaN      NaN         NaN      NaN         NaN               NaN             360.0         351.27                 NaN   \n",
       "2       NaN          NaN     -2.0        -1.0   -57.0      -28.5     6.0        3.0     -5.0        -2.5    -40.0       -20.0               NaN             182.4         788.37                 NaN   \n",
       "\n",
       "   co_lag1_x_rh  pm25_lag1_x_temp  \n",
       "0           NaN               NaN  \n",
       "1        6205.0               NaN  \n",
       "2        1360.0               NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 3 — Feature Engineering (Time-Series + Calendar + Robustness) (ONE CELL)\n",
    "# Requires:\n",
    "#   df_train_master  (from Step 2)\n",
    "#   df_test_master   (optional; from Step 2)\n",
    "# Produces:\n",
    "#   df_train_fe, df_test_fe\n",
    "# Notes:\n",
    "# - Leakage-safe: all rolling features are computed on SHIFTED values (past only).\n",
    "# - Station-aware: computed per stasiun_code.\n",
    "# - Keeps CatBoost-friendly categoricals as object; numeric features stay numeric.\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"df_train_master\" not in globals():\n",
    "    raise RuntimeError(\"Missing df_train_master. Run Step 2 first.\")\n",
    "\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # robust calendar\n",
    "    df[\"year\"] = df[\"tanggal\"].dt.year.astype(\"Int64\")\n",
    "    df[\"month\"] = df[\"tanggal\"].dt.month.astype(\"Int64\")\n",
    "    df[\"day\"] = df[\"tanggal\"].dt.day.astype(\"Int64\")\n",
    "    df[\"dow\"] = df[\"tanggal\"].dt.dayofweek.astype(\"Int64\")\n",
    "    df[\"dayofyear\"] = df[\"tanggal\"].dt.dayofyear.astype(\"Int64\")\n",
    "\n",
    "    # cyclic encoding (helps CatBoost a bit; safe)\n",
    "    doy = df[\"dayofyear\"].astype(float)\n",
    "    df[\"doy_sin\"] = np.sin(2 * np.pi * doy / 365.25)\n",
    "    df[\"doy_cos\"] = np.cos(2 * np.pi * doy / 365.25)\n",
    "    mon = df[\"month\"].astype(float)\n",
    "    df[\"mon_sin\"] = np.sin(2 * np.pi * mon / 12.0)\n",
    "    df[\"mon_cos\"] = np.cos(2 * np.pi * mon / 12.0)\n",
    "\n",
    "    # weekend if missing\n",
    "    if \"is_weekend\" not in df.columns:\n",
    "        df[\"is_weekend\"] = (df[\"dow\"].isin([5, 6])).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_station_lag_rolling(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str = \"stasiun_code\",\n",
    "    base_cols = (\"pm10\",\"pm25\",\"so2\",\"co\",\"o3\",\"no2\",\"max\"),\n",
    "    lags = (1,2,3,7,14),\n",
    "    windows = (3,7,14,30),\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # ensure order\n",
    "    df = df.sort_values([group_col, \"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "    # only keep existing numeric columns\n",
    "    base_cols = [c for c in base_cols if c in df.columns]\n",
    "    if not base_cols:\n",
    "        raise RuntimeError(\"No base pollutant columns found for lag/rolling.\")\n",
    "\n",
    "    g = df.groupby(group_col, sort=False)\n",
    "\n",
    "    # lags\n",
    "    for c in base_cols:\n",
    "        for L in lags:\n",
    "            df[f\"{c}_lag{L}\"] = g[c].shift(L)\n",
    "\n",
    "    # rolling on shifted series (leakage-safe)\n",
    "    for c in base_cols:\n",
    "        s = g[c].shift(1)  # only past values included\n",
    "        for w in windows:\n",
    "            df[f\"{c}_rmean{w}\"] = s.rolling(w, min_periods=max(2, w//3)).mean().reset_index(level=0, drop=True)\n",
    "            df[f\"{c}_rstd{w}\"]  = s.rolling(w, min_periods=max(2, w//3)).std().reset_index(level=0, drop=True)\n",
    "\n",
    "    # deltas (based on lags; safe)\n",
    "    for c in base_cols:\n",
    "        if f\"{c}_lag1\" in df.columns and f\"{c}_lag2\" in df.columns:\n",
    "            df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n",
    "        if f\"{c}_lag1\" in df.columns and f\"{c}_rmean7\" in df.columns:\n",
    "            df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_weather_interactions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # pick weather columns (prefer local wx_, fallback already filled at Step 2)\n",
    "    # examples based on your weather columns\n",
    "    cand = {\n",
    "        \"wind_mean\": [\"wx_wind_speed_10m_mean_km_h\", \"wxg_wind_speed_10m_mean_km_h\", \"wind_speed_10m_mean_km_h\"],\n",
    "        \"precip_sum\": [\"wx_precipitation_sum_mm\", \"wxg_precipitation_sum_mm\", \"precipitation_sum_mm\"],\n",
    "        \"rad_sum\": [\"wx_shortwave_radiation_sum_mj_m²\", \"wxg_shortwave_radiation_sum_mj_m²\", \"shortwave_radiation_sum_mj_m²\"],\n",
    "        \"rh_mean\": [\"wx_relative_humidity_2m_mean\", \"wxg_relative_humidity_2m_mean\", \"relative_humidity_2m_mean\"],\n",
    "        \"temp_mean\": [\"wx_temperature_2m_mean_c\", \"wxg_temperature_2m_mean_c\", \"temperature_2m_mean_c\"],\n",
    "    }\n",
    "    def pick(cols):\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    wind = pick(cand[\"wind_mean\"])\n",
    "    prec = pick(cand[\"precip_sum\"])\n",
    "    rad  = pick(cand[\"rad_sum\"])\n",
    "    rh   = pick(cand[\"rh_mean\"])\n",
    "    tmp  = pick(cand[\"temp_mean\"])\n",
    "\n",
    "    # build a few robust interactions using lag1 (already leakage-safe)\n",
    "    if \"pm25_lag1\" in df.columns and wind is not None:\n",
    "        df[\"pm25_lag1_x_wind\"] = df[\"pm25_lag1\"] * df[wind]\n",
    "    if \"pm10_lag1\" in df.columns and wind is not None:\n",
    "        df[\"pm10_lag1_x_wind\"] = df[\"pm10_lag1\"] * df[wind]\n",
    "    if \"o3_lag1\" in df.columns and rad is not None:\n",
    "        df[\"o3_lag1_x_rad\"] = df[\"o3_lag1\"] * df[rad]\n",
    "    if \"pm25_lag1\" in df.columns and prec is not None:\n",
    "        df[\"pm25_lag1_div_prec\"] = df[\"pm25_lag1\"] / (df[prec].fillna(0) + 1.0)\n",
    "    if \"co_lag1\" in df.columns and rh is not None:\n",
    "        df[\"co_lag1_x_rh\"] = df[\"co_lag1\"] * df[rh]\n",
    "    if \"pm25_lag1\" in df.columns and tmp is not None:\n",
    "        df[\"pm25_lag1_x_temp\"] = df[\"pm25_lag1\"] * df[tmp]\n",
    "\n",
    "    return df\n",
    "\n",
    "def finalize_types_for_catboost(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # keep these as categorical (object)\n",
    "    cat_cols = []\n",
    "    for c in [\"stasiun\", \"stasiun_code\", \"parameter_pencemar_kritis\", \"day_name\", \"nama_libur\", \"weather_station\"]:\n",
    "        if c in df.columns:\n",
    "            cat_cols.append(c)\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype(\"object\")\n",
    "\n",
    "    # cast flags to int\n",
    "    for c in [\"is_weekend\", \"is_holiday_nasional\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_fe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = add_time_features(df)\n",
    "    df = add_station_lag_rolling(df)\n",
    "    df = add_weather_interactions(df)\n",
    "    df = finalize_types_for_catboost(df)\n",
    "    return df\n",
    "\n",
    "# ---- Build FE for train and test (if available) ----\n",
    "df_train_fe = build_fe(df_train_master)\n",
    "\n",
    "if \"df_test_master\" in globals() and df_test_master is not None:\n",
    "    df_test_fe = build_fe(df_test_master)\n",
    "else:\n",
    "    df_test_fe = None\n",
    "\n",
    "# ---- Sanity: no leakage from current-day target directly ----\n",
    "# keep raw current-day pollutant columns; CatBoost can use them too, but lags are key.\n",
    "# ensure we do NOT have rolling computed without shift (we don't).\n",
    "\n",
    "print(\"df_train_fe:\", df_train_fe.shape)\n",
    "if df_test_fe is not None:\n",
    "    print(\"df_test_fe :\", df_test_fe.shape)\n",
    "\n",
    "# Quick missing rate on engineered features\n",
    "top_miss = (df_train_fe.isna().mean().sort_values(ascending=False).head(15) * 100).round(2)\n",
    "print(\"\\nTop missing% (train_fe):\")\n",
    "print(top_miss.to_string())\n",
    "\n",
    "display(df_train_fe.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c5efb",
   "metadata": {
    "papermill": {
     "duration": 0.00556,
     "end_time": "2026-01-30T15:42:51.100561",
     "exception": false,
     "start_time": "2026-01-30T15:42:51.095001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training (Time-Based CV + CatBoost Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07410250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T15:42:51.113865Z",
     "iopub.status.busy": "2026-01-30T15:42:51.113547Z",
     "iopub.status.idle": "2026-01-30T17:31:01.633205Z",
     "shell.execute_reply": "2026-01-30T17:31:01.632145Z"
    },
    "papermill": {
     "duration": 6490.529054,
     "end_time": "2026-01-30T17:31:01.635174",
     "exception": false,
     "start_time": "2026-01-30T15:42:51.106120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 13651 | n_features: 190 | n_cat(auto): 9\n",
      "Classes: ['BAIK', 'BERBAHAYA', 'O3', 'SANGAT TIDAK SEHAT', 'SEDANG', 'TIDAK ADA DATA', 'TIDAK SEHAT']\n",
      "Class counts:\n",
      " kategori\n",
      "SEDANG                7997\n",
      "TIDAK SEHAT           2072\n",
      "BAIK                  1912\n",
      "TIDAK ADA DATA        1440\n",
      "SANGAT TIDAK SEHAT     199\n",
      "O3                      30\n",
      "BERBAHAYA                1\n",
      "\n",
      "Folds:\n",
      "fold0: train=11981 [2010-01-01 00:00:00..2022-12-31 00:00:00] | valid=1670 [2023-01-01 00:00:00..2023-11-30 00:00:00]\n",
      "fold1: train=11490 [2010-01-01 00:00:00..2021-12-31 00:00:00] | valid=491 [2022-01-01 00:00:00..2022-12-31 00:00:00]\n",
      "fold2: train=9666 [2010-01-01 00:00:00..2020-12-31 00:00:00] | valid=1824 [2021-01-01 00:00:00..2021-12-31 00:00:00]\n",
      "\n",
      "=== SEED 42 | task_type=CPU ===\n",
      "0:\tlearn: 0.8485167\ttest: 0.1448394\tbest: 0.1448394 (0)\ttotal: 753ms\tremaining: 1h 40m 21s\n",
      "250:\tlearn: 0.9998135\ttest: 0.5368871\tbest: 0.5368871 (249)\ttotal: 2m 51s\tremaining: 1h 28m\n",
      "500:\tlearn: 0.9999270\ttest: 0.5437375\tbest: 0.5437375 (495)\ttotal: 5m 47s\tremaining: 1h 26m 40s\n",
      "750:\tlearn: 1.0000000\ttest: 0.5526506\tbest: 0.5526506 (693)\ttotal: 8m 39s\tremaining: 1h 23m 30s\n",
      "1000:\tlearn: 1.0000000\ttest: 0.5618220\tbest: 0.5618220 (997)\ttotal: 11m 32s\tremaining: 1h 20m 41s\n",
      "1250:\tlearn: 1.0000000\ttest: 0.5700503\tbest: 0.5700503 (1247)\ttotal: 14m 28s\tremaining: 1h 18m 5s\n",
      "1500:\tlearn: 1.0000000\ttest: 0.5778021\tbest: 0.5803460 (1480)\ttotal: 17m 27s\tremaining: 1h 15m 37s\n",
      "1750:\tlearn: 1.0000000\ttest: 0.5903294\tbest: 0.5927785 (1728)\ttotal: 20m 25s\tremaining: 1h 12m 53s\n",
      "2000:\tlearn: 1.0000000\ttest: 0.6163127\tbest: 0.6163127 (1998)\ttotal: 23m 22s\tremaining: 1h 10m 6s\n",
      "2250:\tlearn: 1.0000000\ttest: 0.6367172\tbest: 0.6387963 (2127)\ttotal: 26m 20s\tremaining: 1h 7m 17s\n",
      "2500:\tlearn: 1.0000000\ttest: 0.6589184\tbest: 0.6628047 (2494)\ttotal: 29m 17s\tremaining: 1h 4m 23s\n",
      "2750:\tlearn: 1.0000000\ttest: 0.6779274\tbest: 0.6779274 (2678)\ttotal: 32m 15s\tremaining: 1h 1m 33s\n",
      "3000:\tlearn: 1.0000000\ttest: 0.6845341\tbest: 0.6845341 (2992)\ttotal: 35m 17s\tremaining: 58m 46s\n",
      "3250:\tlearn: 1.0000000\ttest: 0.6953007\tbest: 0.6953007 (3203)\ttotal: 38m 17s\tremaining: 55m 55s\n",
      "3500:\tlearn: 1.0000000\ttest: 0.7057519\tbest: 0.7057519 (3471)\ttotal: 41m 20s\tremaining: 53m 7s\n",
      "3750:\tlearn: 1.0000000\ttest: 0.7091695\tbest: 0.7091695 (3718)\ttotal: 44m 22s\tremaining: 50m 15s\n",
      "4000:\tlearn: 1.0000000\ttest: 0.7108663\tbest: 0.7108663 (3764)\ttotal: 47m 21s\tremaining: 47m 20s\n",
      "4250:\tlearn: 1.0000000\ttest: 0.7241667\tbest: 0.7241667 (4221)\ttotal: 50m 23s\tremaining: 44m 26s\n",
      "4500:\tlearn: 1.0000000\ttest: 0.7392173\tbest: 0.7397223 (4489)\ttotal: 53m 24s\tremaining: 41m 31s\n",
      "4750:\tlearn: 1.0000000\ttest: 0.7500220\tbest: 0.7500220 (4725)\ttotal: 56m 23s\tremaining: 38m 33s\n",
      "5000:\tlearn: 1.0000000\ttest: 0.7560746\tbest: 0.7560746 (4982)\ttotal: 59m 23s\tremaining: 35m 37s\n",
      "5250:\tlearn: 1.0000000\ttest: 0.7649987\tbest: 0.7649987 (5210)\ttotal: 1h 2m 21s\tremaining: 32m 38s\n",
      "5500:\tlearn: 1.0000000\ttest: 0.7708508\tbest: 0.7708508 (5493)\ttotal: 1h 5m 18s\tremaining: 29m 40s\n",
      "5750:\tlearn: 1.0000000\ttest: 0.7780628\tbest: 0.7780628 (5735)\ttotal: 1h 8m 17s\tremaining: 26m 42s\n",
      "6000:\tlearn: 1.0000000\ttest: 0.7907768\tbest: 0.7907768 (5985)\ttotal: 1h 11m 14s\tremaining: 23m 43s\n",
      "6250:\tlearn: 1.0000000\ttest: 0.7935585\tbest: 0.7935585 (6163)\ttotal: 1h 14m 12s\tremaining: 20m 45s\n",
      "6500:\tlearn: 1.0000000\ttest: 0.7963251\tbest: 0.7977029 (6475)\ttotal: 1h 17m 9s\tremaining: 17m 47s\n",
      "6750:\tlearn: 1.0000000\ttest: 0.8004478\tbest: 0.8004478 (6741)\ttotal: 1h 20m 7s\tremaining: 14m 49s\n",
      "7000:\tlearn: 1.0000000\ttest: 0.8031786\tbest: 0.8031786 (6986)\ttotal: 1h 23m 4s\tremaining: 11m 51s\n",
      "7250:\tlearn: 1.0000000\ttest: 0.8058957\tbest: 0.8058957 (7186)\ttotal: 1h 26m 1s\tremaining: 8m 53s\n",
      "7500:\tlearn: 1.0000000\ttest: 0.8085995\tbest: 0.8085995 (7275)\ttotal: 1h 28m 58s\tremaining: 5m 55s\n",
      "7750:\tlearn: 1.0000000\ttest: 0.8112903\tbest: 0.8112903 (7608)\ttotal: 1h 31m 52s\tremaining: 2m 57s\n",
      "7999:\tlearn: 1.0000000\ttest: 0.8179012\tbest: 0.8179012 (7921)\ttotal: 1h 34m 48s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8179012488\n",
      "bestIteration = 7921\n",
      "\n",
      "Shrink model to first 7922 iterations.\n",
      "[seed 42 fold 0] macroF1=0.55306 | best_iter=7921\n",
      "0:\tlearn: 0.8821136\ttest: 0.5229408\tbest: 0.5229408 (0)\ttotal: 581ms\tremaining: 1h 17m 30s\n",
      "250:\tlearn: 0.9999807\ttest: 0.8746219\tbest: 0.8746219 (245)\ttotal: 2m 34s\tremaining: 1h 19m 35s\n",
      "500:\tlearn: 1.0000000\ttest: 0.8834959\tbest: 0.8834959 (462)\ttotal: 5m 21s\tremaining: 1h 20m 7s\n",
      "750:\tlearn: 1.0000000\ttest: 0.8834959\tbest: 0.8834959 (462)\ttotal: 8m 8s\tremaining: 1h 18m 36s\n",
      "Stopped by overfitting detector  (400 iterations wait)\n",
      "\n",
      "bestTest = 0.8834958666\n",
      "bestIteration = 462\n",
      "\n",
      "Shrink model to first 463 iterations.\n",
      "[seed 42 fold 1] macroF1=0.71460 | best_iter=462\n",
      "0:\tlearn: 0.8147977\ttest: 0.2955386\tbest: 0.2955386 (0)\ttotal: 613ms\tremaining: 1h 21m 46s\n",
      "250:\tlearn: 0.9999589\ttest: 1.0000000\tbest: 1.0000000 (13)\ttotal: 2m 18s\tremaining: 1h 11m 4s\n",
      "Stopped by overfitting detector  (400 iterations wait)\n",
      "\n",
      "bestTest = 1\n",
      "bestIteration = 13\n",
      "\n",
      "Shrink model to first 14 iterations.\n",
      "[seed 42 fold 2] macroF1=1.00000 | best_iter=13\n",
      "\n",
      "=== OOF RESULTS ===\n",
      "Fold macroF1: [0.55306, 0.7146, 1.0]\n",
      "OOF macroF1 : 0.181812\n",
      "\n",
      "OOF classification report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              BAIK     0.1869    1.0000    0.3150      1912\n",
      "         BERBAHAYA     0.0000    0.0000    0.0000         1\n",
      "                O3     0.0000    0.0000    0.0000        30\n",
      "SANGAT TIDAK SEHAT     0.0000    0.0000    0.0000       199\n",
      "            SEDANG     0.9807    0.3496    0.5155      7997\n",
      "    TIDAK ADA DATA     1.0000    0.0111    0.0220      1440\n",
      "       TIDAK SEHAT     0.9946    0.2664    0.4203      2072\n",
      "\n",
      "          accuracy                         0.3865     13651\n",
      "         macro avg     0.4517    0.2325    0.1818     13651\n",
      "      weighted avg     0.8571    0.3865    0.4122     13651\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 4 — Model Training (Time-Based CV + CatBoost Optimization) (ONE CELL) — FINAL ROBUST\n",
    "# Fixes (compared to your last version):\n",
    "# - CPU only (no CUDA probing)\n",
    "# - AUTO-detect ALL non-numeric columns as categorical (prevents \"CO to float\" etc)\n",
    "# - Sanitize categoricals: NaN/None/\"\" -> \"__MISSING__\" and cast to string\n",
    "# - Sanitize numerics: force to numeric (errors->NaN)\n",
    "# - Time folds filtered so TRAIN contains ALL classes\n",
    "# Outputs:\n",
    "# - feature_cols, cat_cols, classes, class_to_id, id_to_class\n",
    "# - folds\n",
    "# - models\n",
    "# - oof_proba, oof_pred, oof_macro_f1\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "if \"df_train_fe\" not in globals():\n",
    "    raise RuntimeError(\"Missing df_train_fe. Run Step 3 first.\")\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "N_SPLITS = 4\n",
    "GAP_DAYS = 0\n",
    "SEEDS = [42]\n",
    "\n",
    "ITERATIONS = 8000\n",
    "LR = 0.05\n",
    "DEPTH = 8\n",
    "L2 = 6.0\n",
    "\n",
    "MISSING_CAT = \"__MISSING__\"\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare data\n",
    "# ----------------------------\n",
    "df = df_train_fe.copy()\n",
    "if \"tanggal\" not in df.columns or \"kategori\" not in df.columns:\n",
    "    raise RuntimeError(\"df_train_fe must contain 'tanggal' and 'kategori'.\")\n",
    "\n",
    "df = df.dropna(subset=[\"tanggal\"]).sort_values([\"tanggal\", \"stasiun_code\"] if \"stasiun_code\" in df.columns else [\"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "drop_cols = {\"kategori\", \"tanggal\", \"source_file\", \"periode_data\"}\n",
    "if \"id\" in df.columns: drop_cols.add(\"id\")\n",
    "\n",
    "# align features with test (if exists)\n",
    "if \"df_test_fe\" in globals() and df_test_fe is not None:\n",
    "    common = [c for c in df.columns if c in df_test_fe.columns]\n",
    "    feature_cols = [c for c in common if c not in drop_cols]\n",
    "else:\n",
    "    feature_cols = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "# clean target\n",
    "y_str = df[\"kategori\"].astype(str).str.strip()\n",
    "y_str = y_str[y_str.str.lower() != \"nan\"]\n",
    "df = df.loc[y_str.index].reset_index(drop=True)\n",
    "X  = X.loc[y_str.index].reset_index(drop=True)\n",
    "y_str = y_str.reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# AUTO categorical detection + sanitization (KEY FIX)\n",
    "# ----------------------------\n",
    "# Any column that is NOT numeric is treated as categorical.\n",
    "is_num = X.apply(pd.api.types.is_numeric_dtype)\n",
    "cat_cols = X.columns[~is_num].tolist()\n",
    "\n",
    "# sanitize cat cols\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].where(X[c].notna(), MISSING_CAT).astype(str)\n",
    "    X[c] = X[c].replace({\"nan\": MISSING_CAT, \"None\": MISSING_CAT, \"\": MISSING_CAT})\n",
    "\n",
    "# sanitize numeric cols (coerce)\n",
    "num_cols = X.columns[is_num].tolist()\n",
    "for c in num_cols:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "# safety: ensure no object left in numeric cols\n",
    "bad_num = [c for c in num_cols if X[c].dtype == object]\n",
    "if bad_num:\n",
    "    raise RuntimeError(f\"Numeric columns still object after coercion: {bad_num[:10]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# classes + weights\n",
    "# ----------------------------\n",
    "classes = sorted(y_str.unique().tolist())\n",
    "counts = y_str.value_counts()\n",
    "class_weights = [float(len(y_str) / (len(classes) * counts[c])) for c in classes]\n",
    "\n",
    "class_to_id = {c:i for i,c in enumerate(classes)}\n",
    "id_to_class = {i:c for c,i in class_to_id.items()}\n",
    "y = y_str.map(class_to_id).astype(int)\n",
    "\n",
    "print(\"Train rows:\", len(df), \"| n_features:\", len(feature_cols), \"| n_cat(auto):\", len(cat_cols))\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Class counts:\\n\", counts.to_string())\n",
    "\n",
    "# ----------------------------\n",
    "# Time folds (ensure train has all classes)\n",
    "# ----------------------------\n",
    "def make_time_folds_filtered(df_in: pd.DataFrame, y_int: pd.Series, n_splits=4, gap_days=0):\n",
    "    d = df_in.copy()\n",
    "    d[\"year\"] = d[\"tanggal\"].dt.year\n",
    "    years = sorted(d[\"year\"].dropna().unique().tolist())\n",
    "\n",
    "    all_ids = set(range(int(y_int.nunique())))\n",
    "\n",
    "    def train_has_all(tr_idx):\n",
    "        return set(y_int.iloc[tr_idx].unique().tolist()) == all_ids\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    # prefer year-based (newest years as validation)\n",
    "    if len(years) >= 2:\n",
    "        for vy in years[::-1]:\n",
    "            tr_mask = d[\"year\"] < vy\n",
    "            va_mask = d[\"year\"] == vy\n",
    "            if not va_mask.any():\n",
    "                continue\n",
    "            if gap_days > 0:\n",
    "                va_start = d.loc[va_mask, \"tanggal\"].min()\n",
    "                tr_mask = tr_mask & (d[\"tanggal\"] <= (va_start - pd.Timedelta(days=gap_days)))\n",
    "            tr_idx = d.index[tr_mask].to_numpy()\n",
    "            va_idx = d.index[va_mask].to_numpy()\n",
    "            if len(tr_idx) == 0 or len(va_idx) == 0:\n",
    "                continue\n",
    "            if not train_has_all(tr_idx):\n",
    "                continue\n",
    "            folds.append((tr_idx, va_idx))\n",
    "            if len(folds) >= n_splits:\n",
    "                break\n",
    "        if folds:\n",
    "            return folds\n",
    "\n",
    "    # fallback blocks by date\n",
    "    uniq_dates = np.array(sorted(d[\"tanggal\"].unique()))\n",
    "    blocks = np.array_split(uniq_dates, n_splits + 1)\n",
    "    for b in blocks[1:][::-1]:\n",
    "        va_start, va_end = b.min(), b.max()\n",
    "        tr_mask = d[\"tanggal\"] < va_start\n",
    "        va_mask = (d[\"tanggal\"] >= va_start) & (d[\"tanggal\"] <= va_end)\n",
    "        if gap_days > 0:\n",
    "            tr_mask = tr_mask & (d[\"tanggal\"] <= (va_start - pd.Timedelta(days=gap_days)))\n",
    "        tr_idx = d.index[tr_mask].to_numpy()\n",
    "        va_idx = d.index[va_mask].to_numpy()\n",
    "        if len(tr_idx) == 0 or len(va_idx) == 0:\n",
    "            continue\n",
    "        if not train_has_all(tr_idx):\n",
    "            continue\n",
    "        folds.append((tr_idx, va_idx))\n",
    "        if len(folds) >= n_splits:\n",
    "            break\n",
    "\n",
    "    # final fallback: last 20%\n",
    "    if not folds:\n",
    "        cut = int(len(d) * 0.8)\n",
    "        folds = [(d.index[:cut].to_numpy(), d.index[cut:].to_numpy())]\n",
    "    return folds\n",
    "\n",
    "folds = make_time_folds_filtered(df, y, n_splits=N_SPLITS, gap_days=GAP_DAYS)\n",
    "\n",
    "print(\"\\nFolds:\")\n",
    "for i, (tr, va) in enumerate(folds):\n",
    "    dtr = (df.loc[tr, \"tanggal\"].min(), df.loc[tr, \"tanggal\"].max())\n",
    "    dva = (df.loc[va, \"tanggal\"].min(), df.loc[va, \"tanggal\"].max())\n",
    "    print(f\"fold{i}: train={len(tr)} [{dtr[0]}..{dtr[1]}] | valid={len(va)} [{dva[0]}..{dva[1]}]\")\n",
    "\n",
    "# ----------------------------\n",
    "# Train CV (CPU only)\n",
    "# ----------------------------\n",
    "K = len(classes)\n",
    "oof_proba = np.zeros((len(df), K), dtype=np.float32)\n",
    "models = []\n",
    "fold_scores = []\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n=== SEED {seed} | task_type=CPU ===\")\n",
    "    for fi, (tr_idx, va_idx) in enumerate(folds):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n",
    "        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]\n",
    "\n",
    "        train_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n",
    "        valid_pool = Pool(X_va, y_va, cat_features=cat_cols)\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            loss_function=\"MultiClass\",\n",
    "            eval_metric=\"TotalF1\",\n",
    "            classes_count=K,\n",
    "            class_weights=class_weights,\n",
    "            iterations=ITERATIONS,\n",
    "            learning_rate=LR,\n",
    "            depth=DEPTH,\n",
    "            l2_leaf_reg=L2,\n",
    "            random_strength=1.0,\n",
    "            bagging_temperature=0.5,\n",
    "            border_count=128,\n",
    "            random_seed=seed,\n",
    "            od_type=\"Iter\",\n",
    "            od_wait=400,\n",
    "            task_type=\"CPU\",\n",
    "            thread_count=-1,\n",
    "            verbose=250\n",
    "        )\n",
    "\n",
    "        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n",
    "\n",
    "        proba = model.predict_proba(X_va)\n",
    "        pred_int = np.argmax(proba, axis=1)\n",
    "        score = f1_score(y_va, pred_int, average=\"macro\")\n",
    "\n",
    "        oof_proba[va_idx] += (proba / len(SEEDS))\n",
    "        fold_scores.append(score)\n",
    "        models.append(model)\n",
    "\n",
    "        print(f\"[seed {seed} fold {fi}] macroF1={score:.5f} | best_iter={model.get_best_iteration()}\")\n",
    "\n",
    "# ----------------------------\n",
    "# OOF summary\n",
    "# ----------------------------\n",
    "oof_pred_int = np.argmax(oof_proba, axis=1)\n",
    "oof_pred = np.array([id_to_class[i] for i in oof_pred_int])\n",
    "oof_macro_f1 = f1_score(y_str, oof_pred, average=\"macro\")\n",
    "\n",
    "print(\"\\n=== OOF RESULTS ===\")\n",
    "print(\"Fold macroF1:\", [round(s, 5) for s in fold_scores])\n",
    "print(\"OOF macroF1 :\", round(oof_macro_f1, 6))\n",
    "print(\"\\nOOF classification report:\")\n",
    "print(classification_report(y_str, oof_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4a6cb",
   "metadata": {
    "papermill": {
     "duration": 0.007289,
     "end_time": "2026-01-30T17:31:01.649999",
     "exception": false,
     "start_time": "2026-01-30T17:31:01.642710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference, Ensembling, Submission & QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c4cfcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T17:31:01.666720Z",
     "iopub.status.busy": "2026-01-30T17:31:01.666322Z",
     "iopub.status.idle": "2026-01-30T17:31:01.692316Z",
     "shell.execute_reply": "2026-01-30T17:31:01.691413Z"
    },
    "papermill": {
     "duration": 0.036929,
     "end_time": "2026-01-30T17:31:01.694081",
     "exception": false,
     "start_time": "2026-01-30T17:31:01.657152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] STEP 5 starting...\n",
      "[WARN] Missing df_test_like / models → creating DUMMY submission (TEST ONLY)\n",
      "\n",
      "=== QA REPORT ===\n",
      "rows_submission: 455\n",
      "rows_sample: 455\n",
      "id_unique: True\n",
      "missing_pred: 0\n",
      "label_distribution: {'SEDANG': 455}\n",
      "\n",
      "[OK] submission saved to /kaggle/working/submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-01_DKI1</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-01_DKI2</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-01_DKI3</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-01_DKI4</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-01_DKI5</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-09-02_DKI1</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-09-02_DKI2</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-09-02_DKI3</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-09-02_DKI4</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-09-02_DKI5</td>\n",
       "      <td>SEDANG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id category\n",
       "0  2025-09-01_DKI1   SEDANG\n",
       "1  2025-09-01_DKI2   SEDANG\n",
       "2  2025-09-01_DKI3   SEDANG\n",
       "3  2025-09-01_DKI4   SEDANG\n",
       "4  2025-09-01_DKI5   SEDANG\n",
       "5  2025-09-02_DKI1   SEDANG\n",
       "6  2025-09-02_DKI2   SEDANG\n",
       "7  2025-09-02_DKI3   SEDANG\n",
       "8  2025-09-02_DKI4   SEDANG\n",
       "9  2025-09-02_DKI5   SEDANG"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 5 — Inference, Ensembling, Submission & QA (ONE CELL) — FAIL-SAFE\n",
    "# Behavior:\n",
    "# - If df_test_like + models exist  -> REAL inference\n",
    "# - Else                           -> DUMMY submission (SEDANG)\n",
    "# Output:\n",
    "# - /kaggle/working/submission.csv\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import Pool\n",
    "\n",
    "# ----------------------------\n",
    "# Guards (minimal)\n",
    "# ----------------------------\n",
    "if \"sub\" not in globals():\n",
    "    raise RuntimeError(\"Missing `sub` (sample_submission). Jalankan STEP 1.\")\n",
    "\n",
    "ID_COL = \"id\" if \"id\" in sub.columns else sub.columns[0]\n",
    "SUB_TARGET_COL = \"category\" if \"category\" in sub.columns else sub.columns[-1]\n",
    "MISSING_CAT = \"__MISSING__\"\n",
    "\n",
    "print(\"[INFO] STEP 5 starting...\")\n",
    "\n",
    "# ============================================================\n",
    "# CASE A — FULL PIPELINE AVAILABLE → REAL INFERENCE\n",
    "# ============================================================\n",
    "can_infer = all(k in globals() for k in [\"models\", \"df_test_like\", \"feature_cols\", \"cat_cols\", \"id_to_class\"])\n",
    "\n",
    "if can_infer:\n",
    "    print(\"[INFO] Found models + df_test_like → running REAL inference\")\n",
    "\n",
    "    df_test_fe = df_test_like.copy()\n",
    "\n",
    "    # ensure ID exists\n",
    "    if ID_COL not in df_test_fe.columns:\n",
    "        raise RuntimeError(f\"{ID_COL} not found in df_test_like\")\n",
    "\n",
    "    # ensure all features exist\n",
    "    for c in feature_cols:\n",
    "        if c not in df_test_fe.columns:\n",
    "            df_test_fe[c] = np.nan\n",
    "\n",
    "    df_test_fe = df_test_fe[[ID_COL] + feature_cols].reset_index(drop=True)\n",
    "    X_test = df_test_fe[feature_cols].copy()\n",
    "\n",
    "    # sanitize categoricals\n",
    "    for c in cat_cols:\n",
    "        X_test[c] = X_test[c].where(X_test[c].notna(), MISSING_CAT).astype(str)\n",
    "        X_test[c] = X_test[c].replace({\"nan\": MISSING_CAT, \"None\": MISSING_CAT, \"\": MISSING_CAT})\n",
    "\n",
    "    # sanitize numerics\n",
    "    num_cols = [c for c in X_test.columns if c not in cat_cols]\n",
    "    for c in num_cols:\n",
    "        X_test[c] = pd.to_numeric(X_test[c], errors=\"coerce\")\n",
    "\n",
    "    # ensemble\n",
    "    K = len(id_to_class)\n",
    "    proba_ens = np.zeros((len(X_test), K), dtype=np.float32)\n",
    "    pool = Pool(X_test, cat_features=cat_cols)\n",
    "\n",
    "    for model in models:\n",
    "        proba_ens += model.predict_proba(pool) / len(models)\n",
    "\n",
    "    pred_int = np.argmax(proba_ens, axis=1)\n",
    "    pred_label = [id_to_class[i] for i in pred_int]\n",
    "\n",
    "    submission = sub.copy()\n",
    "    submission = submission.merge(\n",
    "        df_test_fe[[ID_COL]].assign(_row=np.arange(len(df_test_fe))),\n",
    "        on=ID_COL,\n",
    "        how=\"left\"\n",
    "    ).sort_values(\"_row\")\n",
    "\n",
    "    submission[SUB_TARGET_COL] = pred_label\n",
    "    submission = submission.drop(columns=\"_row\")\n",
    "\n",
    "# ============================================================\n",
    "# CASE B — MISSING OBJECTS → DUMMY SUBMISSION (TEST ONLY)\n",
    "# ============================================================\n",
    "else:\n",
    "    print(\"[WARN] Missing df_test_like / models → creating DUMMY submission (TEST ONLY)\")\n",
    "\n",
    "    submission = sub.copy()\n",
    "    submission[SUB_TARGET_COL] = \"SEDANG\"  # majority class, safe\n",
    "\n",
    "# ----------------------------\n",
    "# QA (ALWAYS)\n",
    "# ----------------------------\n",
    "qa = {\n",
    "    \"rows_submission\": len(submission),\n",
    "    \"rows_sample\": len(sub),\n",
    "    \"id_unique\": submission[ID_COL].is_unique,\n",
    "    \"missing_pred\": int(submission[SUB_TARGET_COL].isna().sum()),\n",
    "    \"label_distribution\": submission[SUB_TARGET_COL].value_counts().to_dict(),\n",
    "}\n",
    "\n",
    "print(\"\\n=== QA REPORT ===\")\n",
    "for k, v in qa.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "assert qa[\"rows_submission\"] == qa[\"rows_sample\"], \"Row count mismatch\"\n",
    "assert qa[\"id_unique\"], \"Duplicate IDs in submission\"\n",
    "assert qa[\"missing_pred\"] == 0, \"Missing predictions\"\n",
    "\n",
    "# ----------------------------\n",
    "# Save\n",
    "# ----------------------------\n",
    "OUT_PATH = \"/kaggle/working/submission.csv\"\n",
    "submission.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(f\"\\n[OK] submission saved to {OUT_PATH}\")\n",
    "display(submission.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15499519,
     "sourceId": 129145,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6497.370346,
   "end_time": "2026-01-30T17:31:02.422129",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-30T15:42:45.051783",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
