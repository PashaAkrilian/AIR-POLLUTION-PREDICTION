{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":129145,"databundleVersionId":15499519,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:14:10.123318Z","iopub.execute_input":"2026-02-06T04:14:10.123754Z","iopub.status.idle":"2026-02-06T04:14:11.555357Z","shell.execute_reply.started":"2026-02-06T04:14:10.123711Z","shell.execute_reply":"2026-02-06T04:14:11.554290Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/penyisihan-datavidia-10/sample_submission.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2025.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/NDVI (vegetation index)/indeks-ndvi-jakarta.csv\n/kaggle/input/penyisihan-datavidia-10/kualitas-air-sungai/data-kualitas-air-sungai-komponen-data.csv\n/kaggle/input/penyisihan-datavidia-10/libur-nasional/dataset-libur-nasional-dan-weekend.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki3-jagakarsa.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki2-kelapagading.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki1-bundaranhi.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki5-kebonjeruk.csv\n/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki4-lubangbuaya.csv\n/kaggle/input/penyisihan-datavidia-10/jumlah-penduduk/data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data Loading & Sanity Checks","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 1 — DATA LOADING & SANITY CHECKS (FINAL / VALID)\n# Datavidia 10.0 | Forecasting-aware | Audit-safe | ONE CELL\n# ============================================================\n\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\n\n# ------------------\n# CONFIG\n# ------------------\nROOT = \"/kaggle/input/penyisihan-datavidia-10\"\nISPU_DIR = f\"{ROOT}/ISPU\"\nCUTOFF_DATE = pd.Timestamp(\"2025-09-01\")\nSEED = 42\nnp.random.seed(SEED)\n\n# ------------------\n# 1. LOAD ALL ISPU FILES\n# ------------------\nispu_files = sorted(glob.glob(f\"{ISPU_DIR}/*.csv\"))\nassert len(ispu_files) > 0, \"ISPU files not found\"\n\ndfs = []\nfor fp in ispu_files:\n    d = pd.read_csv(fp)\n    d[\"__source_file\"] = os.path.basename(fp)\n    dfs.append(d)\n\ndf_raw = pd.concat(dfs, ignore_index=True)\nprint(f\"[OK] ISPU raw loaded: {df_raw.shape}\")\n\n# ------------------\n# 2. STANDARDIZE COLUMN NAMES\n# ------------------\ndf = df_raw.copy()\ndf.columns = (\n    df.columns\n      .str.strip()\n      .str.lower()\n      .str.replace(\" \", \"_\")\n      .str.replace(\"__\", \"_\")\n)\n\n# ------------------\n# 3. PARSE DATE (HARlAN ONLY)\n# NOTE:\n# Dataset ISPU harian yang valid hanya tersedia mulai 2022.\n# File sebelum itu bersifat agregat / non-harian dan tidak\n# dapat digunakan untuk forecasting harian.\n# ------------------\nif \"tanggal\" not in df.columns:\n    raise RuntimeError(\"Column 'tanggal' not found (required for daily ISPU)\")\n\ndf[\"tanggal\"] = pd.to_datetime(df[\"tanggal\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"tanggal\"])\n\n# enforce FAQ cutoff\ndf = df[df[\"tanggal\"] < CUTOFF_DATE].copy()\n\n# enforce DAILY VALID PERIOD (important & explicit)\ndf = df[df[\"tanggal\"] >= \"2022-01-01\"].copy()\n\n# ------------------\n# 4. STANDARDIZE STATION NAME\n# ------------------\ndf[\"stasiun\"] = (\n    df[\"stasiun\"]\n      .astype(str)\n      .str.upper()\n      .str.strip()\n)\n\nSTATION_MAP = {\n    \"DKI5 KEBON JERUK JAKARTA BARAT\": \"DKI5 KEBON JERUK\",\n}\ndf[\"stasiun\"] = df[\"stasiun\"].replace(STATION_MAP)\n\n# ------------------\n# 5. NORMALIZE LABEL (3 CLASSES)\n# ------------------\ndf[\"kategori\"] = (\n    df[\"kategori\"]\n      .astype(str)\n      .str.upper()\n      .str.strip()\n)\n\nLABEL_MAP = {\n    \"BAIK\": \"BAIK\",\n    \"SEDANG\": \"SEDANG\",\n    \"TIDAK SEHAT\": \"TIDAK SEHAT\",\n    \"SANGAT TIDAK SEHAT\": \"TIDAK SEHAT\",\n    \"BERBAHAYA\": \"TIDAK SEHAT\",\n}\n\ndf[\"kategori\"] = df[\"kategori\"].map(LABEL_MAP)\ndf = df.dropna(subset=[\"kategori\"])\n\n# ------------------\n# 6. REMOVE DUPLICATES\n# ------------------\ndf = df.drop_duplicates(subset=[\"tanggal\", \"stasiun\"])\n\n# ------------------\n# 7. FINAL SANITY CHECKS\n# ------------------\nprint(\"\\n[INFO] Date range (daily ISPU):\")\nprint(df[\"tanggal\"].min(), \"→\", df[\"tanggal\"].max())\n\nprint(\"\\n[INFO] Label distribution:\")\nprint(df[\"kategori\"].value_counts())\n\nprint(\"\\n[INFO] Stations:\")\nprint(df[\"stasiun\"].value_counts())\n\nassert df[\"kategori\"].nunique() == 3\nassert df[\"tanggal\"].min() >= pd.Timestamp(\"2022-01-01\")\n\n# ------------------\n# 8. LOAD AUXILIARY DATA (NO MERGE YET)\n# ------------------\ndf_ndvi = pd.read_csv(f\"{ROOT}/NDVI (vegetation index)/indeks-ndvi-jakarta.csv\")\ndf_libur = pd.read_csv(f\"{ROOT}/libur-nasional/dataset-libur-nasional-dan-weekend.csv\")\ndf_air = pd.read_csv(f\"{ROOT}/kualitas-air-sungai/data-kualitas-air-sungai-komponen-data.csv\")\ndf_pop = pd.read_csv(\n    f\"{ROOT}/jumlah-penduduk/\"\n    \"data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\"\n)\n\nCUACA_DIR = f\"{ROOT}/cuaca-harian\"\ncuaca_files = sorted(glob.glob(f\"{CUACA_DIR}/*.csv\"))\ndf_cuaca = []\nfor fp in cuaca_files:\n    d = pd.read_csv(fp)\n    d[\"stasiun_cuaca\"] = os.path.basename(fp).replace(\".csv\", \"\")\n    df_cuaca.append(d)\ndf_cuaca = pd.concat(df_cuaca, ignore_index=True)\n\nprint(\"\\n[OK] Auxiliary datasets loaded\")\nprint(\"NDVI:\", df_ndvi.shape)\nprint(\"Libur:\", df_libur.shape)\nprint(\"Kualitas Air:\", df_air.shape)\nprint(\"Penduduk:\", df_pop.shape)\nprint(\"Cuaca:\", df_cuaca.shape)\n\n# ------------------\n# 9. EXPORT CLEAN ISPU\n# ------------------\ndf_ispu_clean = df.copy()\n\nprint(\"\\nSTAGE 1 VALID COMPLETE\")\nprint(\"df_ispu_clean is SAFE and READY for STAGE 2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:21:08.916506Z","iopub.execute_input":"2026-02-06T04:21:08.916867Z","iopub.status.idle":"2026-02-06T04:21:09.327980Z","shell.execute_reply.started":"2026-02-06T04:21:08.916839Z","shell.execute_reply":"2026-02-06T04:21:09.327045Z"}},"outputs":[{"name":"stdout","text":"[OK] ISPU raw loaded: (16902, 24)\n\n[INFO] Date range (daily ISPU):\n2022-12-01 00:00:00 → 2023-11-30 00:00:00\n\n[INFO] Label distribution:\nkategori\nSEDANG         1358\nBAIK            236\nTIDAK SEHAT     210\nName: count, dtype: int64\n\n[INFO] Stations:\nstasiun\nDKI2 KELAPA GADING    363\nDKI3 JAGAKARSA        361\nDKI1 BUNDERAN HI      361\nDKI4 LUBANG BUAYA     361\nDKI5 KEBON JERUK      358\nName: count, dtype: int64\n\n[OK] Auxiliary datasets loaded\nNDVI: (1810, 3)\nLibur: (5844, 5)\nKualitas Air: (14400, 12)\nPenduduk: (34176, 9)\nCuaca: (28610, 25)\n\nSTAGE 1 VALID COMPLETE\ndf_ispu_clean is SAFE and READY for STAGE 2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Master Table Building (Correct Joins)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 2 — MASTER TABLE BUILDING (FINAL MERGED, STAGE-3 READY)\n# Datavidia 10.0 | Leak-safe | Group-wise merge_asof | ONE CELL\n# ============================================================\n\nimport pandas as pd\n\n# ============================================================\n# 1. BASE TABLE (ISPU as anchor)\n# ============================================================\ndf_base = df_ispu_clean.copy()\ndf_base[\"tanggal\"] = pd.to_datetime(df_base[\"tanggal\"])\n\n# ============================================================\n# 2. JOIN LIBUR NASIONAL (EXACT DATE)\n# ============================================================\ndf_libur2 = df_libur.copy()\ndf_libur2.columns = (\n    df_libur2.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n)\ndf_libur2[\"tanggal\"] = pd.to_datetime(df_libur2[\"tanggal\"])\n\ndf_base = df_base.merge(\n    df_libur2[[\"tanggal\", \"is_holiday_nasional\", \"is_weekend\"]],\n    on=\"tanggal\",\n    how=\"left\"\n)\n\ndf_base[\"is_holiday_nasional\"] = df_base[\"is_holiday_nasional\"].fillna(0).astype(int)\ndf_base[\"is_weekend\"] = df_base[\"is_weekend\"].fillna(0).astype(int)\n\n# ============================================================\n# 3. PREPARE WEATHER DATA\n# ============================================================\ndf_cuaca2 = df_cuaca.copy()\ndf_cuaca2.columns = (\n    df_cuaca2.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n)\n\nif \"time\" in df_cuaca2.columns:\n    df_cuaca2[\"tanggal\"] = pd.to_datetime(df_cuaca2[\"time\"])\nelif \"tanggal\" in df_cuaca2.columns:\n    df_cuaca2[\"tanggal\"] = pd.to_datetime(df_cuaca2[\"tanggal\"])\nelse:\n    raise RuntimeError(\"No date column in weather data\")\n\ndf_cuaca2 = df_cuaca2.dropna(subset=[\"tanggal\", \"stasiun_cuaca\"])\n\nCUACA_MAP = {\n    \"DKI1 BUNDERAN HI\": \"cuaca-harian-dki1-bundaranhi\",\n    \"DKI2 KELAPA GADING\": \"cuaca-harian-dki2-kelapagading\",\n    \"DKI3 JAGAKARSA\": \"cuaca-harian-dki3-jagakarsa\",\n    \"DKI4 LUBANG BUAYA\": \"cuaca-harian-dki4-lubangbuaya\",\n    \"DKI5 KEBON JERUK\": \"cuaca-harian-dki5-kebonjeruk\",\n}\n\ndf_base[\"stasiun_cuaca\"] = df_base[\"stasiun\"].map(CUACA_MAP)\ndf_base = df_base.dropna(subset=[\"stasiun_cuaca\"])\n\n# ============================================================\n# 4. MERGE WEATHER (GROUP-WISE ASOF, NO ERROR)\n# ============================================================\nout = []\nfor st in df_base[\"stasiun_cuaca\"].unique():\n    left = (\n        df_base[df_base[\"stasiun_cuaca\"] == st]\n        .sort_values(\"tanggal\")\n        .reset_index(drop=True)\n    )\n    right = (\n        df_cuaca2[df_cuaca2[\"stasiun_cuaca\"] == st]\n        .sort_values(\"tanggal\")\n        .reset_index(drop=True)\n    )\n    merged = pd.merge_asof(\n        left, right,\n        on=\"tanggal\",\n        direction=\"backward\",\n        allow_exact_matches=True\n    )\n    out.append(merged)\n\ndf_master = pd.concat(out, ignore_index=True)\n\n# ============================================================\n# 5. MERGE NDVI (GROUP-WISE ASOF)\n# ============================================================\ndf_ndvi2 = df_ndvi.copy()\ndf_ndvi2.columns = (\n    df_ndvi2.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n)\ndf_ndvi2[\"tanggal\"] = pd.to_datetime(df_ndvi2[\"tanggal\"])\ndf_ndvi2 = df_ndvi2.dropna(subset=[\"tanggal\", \"stasiun_id\"])\n\nout = []\nfor st in df_master[\"stasiun\"].unique():\n    left = (\n        df_master[df_master[\"stasiun\"] == st]\n        .sort_values(\"tanggal\")\n        .reset_index(drop=True)\n    )\n    right = (\n        df_ndvi2[df_ndvi2[\"stasiun_id\"] == st]\n        .sort_values(\"tanggal\")\n        .reset_index(drop=True)\n    )\n    merged = pd.merge_asof(\n        left, right,\n        on=\"tanggal\",\n        direction=\"backward\"\n    )\n    out.append(merged)\n\ndf_master = pd.concat(out, ignore_index=True)\n\n# ============================================================\n# 6. MERGE POPULATION (YEAR-LEVEL, STATIC)\n# ============================================================\ndf_pop2 = df_pop.copy()\ndf_pop2.columns = (\n    df_pop2.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n)\n\ndf_pop_year = (\n    df_pop2\n    .groupby(\"tahun\", as_index=False)\n    .agg(jumlah_penduduk=(\"jumlah_penduduk\", \"sum\"))\n)\n\ndf_master[\"tahun\"] = df_master[\"tanggal\"].dt.year\ndf_master = df_master.merge(df_pop_year, on=\"tahun\", how=\"left\")\n\n# ============================================================\n# 7. SELECT FINAL STAGE-2 COLUMNS (CLEAN & READY)\n# ============================================================\nFINAL_COLS = [\n    \"tanggal\",\n    \"stasiun\",\n    \"kategori\",\n    \"is_holiday_nasional\",\n    \"is_weekend\",\n    \"temperature_2m_mean\",\n    \"relative_humidity_2m_mean\",\n    \"wind_speed_10m_mean\",\n    \"precipitation_sum\",\n    \"ndvi\",\n    \"jumlah_penduduk\",\n]\n\nFINAL_COLS = [c for c in FINAL_COLS if c in df_master.columns]\ndf_stage2 = df_master[FINAL_COLS].copy()\n\n# ============================================================\n# 8. FINAL SORT & SANITY CHECK\n# ============================================================\ndf_stage2 = (\n    df_stage2\n    .sort_values([\"stasiun\", \"tanggal\"])\n    .reset_index(drop=True)\n)\n\nprint(\"[INFO] STAGE 2 FINAL TABLE\")\nprint(\"Shape:\", df_stage2.shape)\nprint(\"Date range:\", df_stage2[\"tanggal\"].min(), \"→\", df_stage2[\"tanggal\"].max())\nprint(\"\\nMissing values:\")\nprint(df_stage2.isna().sum())\n\nprint(\"\\nSTAGE 2 FINAL COMPLETE\")\nprint(\"df_stage2 READY for STAGE 3 (Feature Engineering)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:33:28.455266Z","iopub.execute_input":"2026-02-06T04:33:28.455649Z","iopub.status.idle":"2026-02-06T04:33:28.653484Z","shell.execute_reply.started":"2026-02-06T04:33:28.455614Z","shell.execute_reply":"2026-02-06T04:33:28.652462Z"}},"outputs":[{"name":"stdout","text":"[INFO] STAGE 2 FINAL TABLE\nShape: (1804, 7)\nDate range: 2022-12-01 00:00:00 → 2023-11-30 00:00:00\n\nMissing values:\ntanggal                   0\nstasiun                   0\nkategori                  0\nis_holiday_nasional       0\nis_weekend                0\nndvi                   1804\njumlah_penduduk        1804\ndtype: int64\n\nSTAGE 2 FINAL COMPLETE\ndf_stage2 READY for STAGE 3 (Feature Engineering)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Feature Engineering (Time-Series + Calendar + Robustness)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 3 — FEATURE ENGINEERING (TIME-SERIES + CALENDAR)\n# Forecasting-safe | Robust | ONE CELL\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\n\n# ------------------\n# 0. COPY & SORT (CRITICAL)\n# ------------------\ndf = df_stage2.copy()\ndf[\"tanggal\"] = pd.to_datetime(df[\"tanggal\"])\ndf = df.sort_values([\"stasiun\", \"tanggal\"]).reset_index(drop=True)\n\n# ------------------\n# 1. TARGET ENCODING (LABEL -> ID)\n# ------------------\nLABELS = [\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\"]\nlabel_to_id = {k:i for i,k in enumerate(LABELS)}\nid_to_label = {i:k for k,i in label_to_id.items()}\n\ndf[\"y\"] = df[\"kategori\"].map(label_to_id).astype(int)\n\n# ------------------\n# 2. CALENDAR FEATURES (SAFE)\n# ------------------\ndf[\"dow\"] = df[\"tanggal\"].dt.weekday        # 0=Mon\ndf[\"week\"] = df[\"tanggal\"].dt.isocalendar().week.astype(int)\ndf[\"month\"] = df[\"tanggal\"].dt.month\ndf[\"is_month_start\"] = df[\"tanggal\"].dt.is_month_start.astype(int)\ndf[\"is_month_end\"] = df[\"tanggal\"].dt.is_month_end.astype(int)\n\n# Cyclical encoding\ndf[\"dow_sin\"] = np.sin(2 * np.pi * df[\"dow\"] / 7)\ndf[\"dow_cos\"] = np.cos(2 * np.pi * df[\"dow\"] / 7)\ndf[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\ndf[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n\n# ------------------\n# 3. TIME-SERIES FEATURES (PAST-ONLY)\n#    - gunakan label historis sebagai proxy dinamika polusi\n# ------------------\nLAGS = [1, 2, 3, 7, 14]\nROLLS = [3, 7, 14]\n\n# Lag features\nfor l in LAGS:\n    df[f\"y_lag_{l}\"] = df.groupby(\"stasiun\")[\"y\"].shift(l)\n\n# Rolling statistics on lagged target\nfor w in ROLLS:\n    grp = df.groupby(\"stasiun\")[\"y\"]\n    df[f\"y_roll_mean_{w}\"] = grp.shift(1).rolling(w).mean()\n    df[f\"y_roll_std_{w}\"]  = grp.shift(1).rolling(w).std()\n    df[f\"y_roll_min_{w}\"]  = grp.shift(1).rolling(w).min()\n    df[f\"y_roll_max_{w}\"]  = grp.shift(1).rolling(w).max()\n\n# Trend-like feature (difference)\ndf[\"y_diff_1\"] = df.groupby(\"stasiun\")[\"y\"].shift(1) - df.groupby(\"stasiun\")[\"y\"].shift(2)\n\n# ------------------\n# 4. ROBUSTNESS FEATURES\n# ------------------\n# Weekend/Holiday interaction\ndf[\"holiday_or_weekend\"] = ((df[\"is_holiday_nasional\"] == 1) | (df[\"is_weekend\"] == 1)).astype(int)\n\n# Station identity as categorical (for CatBoost / target encoding later)\ndf[\"stasiun_cat\"] = df[\"stasiun\"].astype(\"category\")\n\n# ------------------\n# 5. DROP ROWS WITH INSUFFICIENT HISTORY\n#    (IMPORTANT: avoid leakage & NaN explosion)\n# ------------------\nMIN_HISTORY = max(max(LAGS), max(ROLLS))\ndf = df.groupby(\"stasiun\").apply(lambda x: x.iloc[MIN_HISTORY:]).reset_index(drop=True)\n\n# ------------------\n# 6. FINAL FEATURE SET\n# ------------------\nDROP_COLS = [\n    \"kategori\", \"stasiun\", \"tanggal\"\n]\n\nFEATURE_COLS = [c for c in df.columns if c not in DROP_COLS + [\"y\"]]\n\n# ------------------\n# 7. SANITY CHECK\n# ------------------\nprint(\"[INFO] STAGE 3 FEATURE TABLE\")\nprint(\"Shape:\", df.shape)\nprint(\"Target distribution (y):\")\nprint(df[\"y\"].value_counts())\nprint(\"\\nMissing values (top 10):\")\nprint(df[FEATURE_COLS].isna().sum().sort_values(ascending=False).head(10))\n\n# ------------------\n# 8. EXPORT\n# ------------------\ndf_stage3 = df.copy()\n\nprint(\"\\nSTAGE 3 COMPLETE\")\nprint(\"df_stage3 + FEATURE_COLS READY for STAGE 4 (Model Training)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T04:59:55.983896Z","iopub.execute_input":"2026-02-06T04:59:55.984230Z","iopub.status.idle":"2026-02-06T04:59:56.056044Z","shell.execute_reply.started":"2026-02-06T04:59:55.984203Z","shell.execute_reply":"2026-02-06T04:59:56.055119Z"}},"outputs":[{"name":"stdout","text":"[INFO] STAGE 3 FEATURE TABLE\nShape: (1734, 37)\nTarget distribution (y):\ny\n1    1304\n0     222\n2     208\nName: count, dtype: int64\n\nMissing values (top 10):\nndvi                   1734\njumlah_penduduk        1734\nis_holiday_nasional       0\nis_weekend                0\ndow                       0\nweek                      0\nmonth                     0\nis_month_start            0\nis_month_end              0\ndow_sin                   0\ndtype: int64\n\nSTAGE 3 COMPLETE\ndf_stage3 + FEATURE_COLS READY for STAGE 4 (Model Training)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/197539507.py:76: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df = df.groupby(\"stasiun\").apply(lambda x: x.iloc[MIN_HISTORY:]).reset_index(drop=True)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Model Training ","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 4 — MODEL TRAINING (CATBOOST, FIXED TIME-CV)\n# Forecasting-safe | No empty fold | Macro-F1 oriented\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import f1_score\n\n# ------------------\n# CONFIG\n# ------------------\nSEED = 42\nN_FOLDS = 5\nMIN_TRAIN_DAYS = 120   # minimal history sebelum validasi\nTARGET_COL = \"y\"\n\nnp.random.seed(SEED)\n\n# ------------------\n# 1. PREPARE DATA\n# ------------------\ndf = df_stage3.copy()\ndf = df.sort_values([\"tanggal\", \"stasiun\"]).reset_index(drop=True)\n\nX = df[FEATURE_COLS]\ny = df[TARGET_COL].values\ndates = df[\"tanggal\"]\n\n# categorical features (CatBoost)\nCAT_COLS = []\nif \"stasiun_cat\" in FEATURE_COLS:\n    CAT_COLS.append(FEATURE_COLS.index(\"stasiun_cat\"))\n\n# ------------------\n# 2. BUILD SAFE TIME-BASED FOLDS\n# ------------------\nunique_dates = np.sort(dates.unique())\n\n# validasi hanya setelah MIN_TRAIN_DAYS\nvalid_start = unique_dates[MIN_TRAIN_DAYS:]\n\n# bagi tanggal validasi jadi N_FOLDS\nval_date_splits = np.array_split(valid_start, N_FOLDS)\n\nfolds = []\nfor val_dates in val_date_splits:\n    tr_idx = dates < val_dates.min()\n    va_idx = dates.isin(val_dates)\n\n    # safety check\n    if tr_idx.sum() == 0 or va_idx.sum() == 0:\n        continue\n\n    folds.append((tr_idx.values, va_idx.values))\n\nprint(f\"[INFO] Total usable folds: {len(folds)}\")\n\n# ------------------\n# 3. CLASS WEIGHTS (IMBALANCE)\n# ------------------\nclass_counts = np.bincount(y)\nclass_weights = class_counts.sum() / (len(class_counts) * class_counts)\nclass_weights = class_weights.tolist()\n\nprint(\"[INFO] Class weights:\", class_weights)\n\n# ------------------\n# 4. TRAIN PER FOLD\n# ------------------\noof_pred = np.full(len(df), -1, dtype=int)\nmodels_by_fold = []\nfold_scores = []\n\nfor fold, (tr_idx, va_idx) in enumerate(folds, 1):\n    print(f\"\\n[INFO] Fold {fold}/{len(folds)}\")\n\n    X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n    X_va, y_va = X.iloc[va_idx], y[va_idx]\n\n    train_pool = Pool(X_tr, y_tr, cat_features=CAT_COLS)\n    val_pool   = Pool(X_va, y_va, cat_features=CAT_COLS)\n\n    model = CatBoostClassifier(\n        loss_function=\"MultiClass\",\n        eval_metric=\"TotalF1\",\n        iterations=1500,\n        learning_rate=0.05,\n        depth=8,\n        l2_leaf_reg=5,\n        class_weights=class_weights,\n        random_seed=SEED,\n        early_stopping_rounds=150,\n        verbose=200\n    )\n\n    model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n\n    preds = model.predict(val_pool).astype(int).ravel()\n    oof_pred[va_idx] = preds\n\n    f1 = f1_score(y_va, preds, average=\"macro\")\n    fold_scores.append(f1)\n\n    print(f\"[INFO] Fold {fold} Macro F1:\", round(f1, 5))\n\n    models_by_fold.append(model)\n\n# ------------------\n# 5. CV RESULT (ONLY VALID OOF)\n# ------------------\nvalid_oof = oof_pred != -1\ncv_macro_f1 = f1_score(y[valid_oof], oof_pred[valid_oof], average=\"macro\")\n\nprint(\"\\n==============================\")\nprint(\"[RESULT] CV Macro F1:\", round(cv_macro_f1, 5))\nprint(\"[RESULT] Fold scores:\", [round(s, 5) for s in fold_scores])\nprint(\"==============================\")\n\n# ------------------\n# 6. EXPORT OOF\n# ------------------\ndf_oof = df.loc[valid_oof, [\"tanggal\", \"stasiun\"]].copy()\ndf_oof[\"y_true\"] = y[valid_oof]\ndf_oof[\"y_pred\"] = oof_pred[valid_oof]\n\nprint(\"\\nSTAGE 4 COMPLETE (FIXED)\")\nprint(\"models_by_fold + df_oof READY for STAGE 5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T05:26:13.909894Z","iopub.execute_input":"2026-02-06T05:26:13.910237Z","iopub.status.idle":"2026-02-06T05:26:22.908103Z","shell.execute_reply.started":"2026-02-06T05:26:13.910208Z","shell.execute_reply":"2026-02-06T05:26:22.906935Z"}},"outputs":[{"name":"stdout","text":"[INFO] Total usable folds: 5\n[INFO] Class weights: [2.6036036036036037, 0.4432515337423313, 2.7788461538461537]\n\n[INFO] Fold 1/5\n0:\tlearn: 0.7941981\ttest: 0.2942814\tbest: 0.2942814 (0)\ttotal: 77.3ms\tremaining: 1m 55s\nStopped by overfitting detector  (150 iterations wait)\n\nbestTest = 0.4287574785\nbestIteration = 5\n\nShrink model to first 6 iterations.\n[INFO] Fold 1 Macro F1: 0.38703\n\n[INFO] Fold 2/5\n0:\tlearn: 0.7568778\ttest: 0.5123793\tbest: 0.5123793 (0)\ttotal: 11.2ms\tremaining: 16.8s\nStopped by overfitting detector  (150 iterations wait)\n\nbestTest = 0.8490305943\nbestIteration = 7\n\nShrink model to first 8 iterations.\n[INFO] Fold 2 Macro F1: 0.64206\n\n[INFO] Fold 3/5\n0:\tlearn: 0.7451601\ttest: 0.7831151\tbest: 0.7831151 (0)\ttotal: 7.69ms\tremaining: 11.5s\n200:\tlearn: 0.8989361\ttest: 0.8381340\tbest: 0.8498091 (74)\ttotal: 1.45s\tremaining: 9.34s\nStopped by overfitting detector  (150 iterations wait)\n\nbestTest = 0.8498091475\nbestIteration = 74\n\nShrink model to first 75 iterations.\n[INFO] Fold 3 Macro F1: 0.71935\n\n[INFO] Fold 4/5\n0:\tlearn: 0.7948139\ttest: 0.7740225\tbest: 0.7740225 (0)\ttotal: 8.18ms\tremaining: 12.3s\n200:\tlearn: 0.8965629\ttest: 0.8054637\tbest: 0.8105532 (139)\ttotal: 1.58s\tremaining: 10.2s\nStopped by overfitting detector  (150 iterations wait)\n\nbestTest = 0.8105532417\nbestIteration = 139\n\nShrink model to first 140 iterations.\n[INFO] Fold 4 Macro F1: 0.56836\n\n[INFO] Fold 5/5\n0:\tlearn: 0.7748996\ttest: 0.7281864\tbest: 0.7281864 (0)\ttotal: 10ms\tremaining: 15s\nStopped by overfitting detector  (150 iterations wait)\n\nbestTest = 0.74121518\nbestIteration = 14\n\nShrink model to first 15 iterations.\n[INFO] Fold 5 Macro F1: 0.77282\n\n==============================\n[RESULT] CV Macro F1: 0.64962\n[RESULT] Fold scores: [0.38703, 0.64206, 0.71935, 0.56836, 0.77282]\n==============================\n\nSTAGE 4 COMPLETE (FIXED)\nmodels_by_fold + df_oof READY for STAGE 5\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Inference, Ensembling, Submission & QA","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STAGE 5 — INFERENCE, RANK-BASED DECISION, SUBMISSION & QA\n# Datavidia 10.0 | FINAL ANTI-COLLAPSE VERSION\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom catboost import Pool\n\n# ------------------\n# CONFIG\n# ------------------\nSAMPLE_SUB_PATH = \"/kaggle/input/penyisihan-datavidia-10/sample_submission.csv\"\nOUT_SUB_PATH = \"/kaggle/working/submission.csv\"\n\n# target quota (SAFE DEFAULT)\nTARGET_DIST = {\n    \"SEDANG\": 0.60,\n    \"TIDAK SEHAT\": 0.25,\n    \"BAIK\": 0.15,\n}\n\n# ------------------\n# 1. LOAD SAMPLE SUBMISSION\n# ------------------\nsub = pd.read_csv(SAMPLE_SUB_PATH)\nsub[\"tanggal\"] = pd.to_datetime(sub[\"id\"].str.split(\"_\").str[0])\nsub[\"stasiun\"] = sub[\"id\"].str.split(\"_\").str[1]\n\n# ------------------\n# 2. LAST-KNOWN FEATURES PER STASIUN\n# ------------------\ndf_last = (\n    df_stage3\n    .sort_values([\"stasiun\", \"tanggal\"])\n    .groupby(\"stasiun\", as_index=False)\n    .tail(1)\n    .reset_index(drop=True)\n)\n\ndf_test = sub.merge(\n    df_last.drop(columns=[\"tanggal\", \"y\"], errors=\"ignore\"),\n    on=\"stasiun\",\n    how=\"left\"\n)\n\n# ensure categorical feature\ndf_test[\"stasiun_cat\"] = df_test.get(\"stasiun_cat\", df_test[\"stasiun\"]).astype(str)\n\n# ------------------\n# 3. CALENDAR FEATURES\n# ------------------\ndf_test[\"dow\"] = df_test[\"tanggal\"].dt.weekday\ndf_test[\"month\"] = df_test[\"tanggal\"].dt.month\ndf_test[\"is_weekend\"] = df_test[\"tanggal\"].dt.weekday.isin([5, 6]).astype(int)\n\n# ------------------\n# 4. MODEL INFERENCE (PROBABILITY)\n# ------------------\nX_test = df_test[FEATURE_COLS]\nCAT_COLS = [FEATURE_COLS.index(\"stasiun_cat\")] if \"stasiun_cat\" in FEATURE_COLS else []\n\ntest_pool = Pool(X_test, cat_features=CAT_COLS)\n\nproba = None\nfor m in models_by_fold:\n    p = m.predict_proba(test_pool)\n    proba = p if proba is None else proba + p\n\nproba = proba / len(models_by_fold)\n\n# ------------------\n# 5. RANK-BASED CLASS ASSIGNMENT (KEY FIX)\n# ------------------\nN = len(df_test)\n\n# hitung kuota\nquota = {\n    k: int(v * N)\n    for k, v in TARGET_DIST.items()\n}\n\n# pastikan total pas\nquota[\"SEDANG\"] = N - quota[\"BAIK\"] - quota[\"TIDAK SEHAT\"]\n\n# mapping\nlabel_to_idx = label_to_id\nidx_to_label = id_to_label\n\n# ranking berdasarkan confidence masing-masing kelas\nrank_baik = np.argsort(-proba[:, label_to_idx[\"BAIK\"]])\nrank_tidak = np.argsort(-proba[:, label_to_idx[\"TIDAK SEHAT\"]])\n\nassigned = np.full(N, -1)\n\n# assign TIDAK SEHAT\nassigned[rank_tidak[:quota[\"TIDAK SEHAT\"]]] = label_to_idx[\"TIDAK SEHAT\"]\n\n# assign BAIK (yang belum terisi)\ncnt = 0\nfor i in rank_baik:\n    if assigned[i] == -1:\n        assigned[i] = label_to_idx[\"BAIK\"]\n        cnt += 1\n        if cnt >= quota[\"BAIK\"]:\n            break\n\n# sisanya SEDANG\nassigned[assigned == -1] = label_to_idx[\"SEDANG\"]\n\ny_pred = [idx_to_label[i] for i in assigned]\n\n# ------------------\n# 6. BUILD SUBMISSION\n# ------------------\nsub_out = sub[[\"id\"]].copy()\nsub_out[\"category\"] = y_pred\nsub_out.to_csv(OUT_SUB_PATH, index=False)\n\n# ------------------\n# 7. QA\n# ------------------\nprint(\"[QA] Distribution:\")\nprint(pd.Series(y_pred).value_counts())\n\nprint(\"\\nSTAGE 5 FINAL COMPLETE — ANTI COLLAPSE\")\nprint(\"submission.csv READY TO UPLOAD\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T06:42:25.696211Z","iopub.execute_input":"2026-02-06T06:42:25.697112Z","iopub.status.idle":"2026-02-06T06:42:25.749792Z","shell.execute_reply.started":"2026-02-06T06:42:25.697075Z","shell.execute_reply":"2026-02-06T06:42:25.748642Z"}},"outputs":[{"name":"stdout","text":"[QA] Distribution:\nSEDANG         274\nTIDAK SEHAT    113\nBAIK            68\nName: count, dtype: int64\n\nSTAGE 5 FINAL COMPLETE — ANTI COLLAPSE\nsubmission.csv READY TO UPLOAD\n","output_type":"stream"}],"execution_count":23}]}