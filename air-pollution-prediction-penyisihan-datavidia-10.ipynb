{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fba448e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-05T02:18:47.188129Z",
     "iopub.status.busy": "2026-02-05T02:18:47.187625Z",
     "iopub.status.idle": "2026-02-05T02:18:48.531912Z",
     "shell.execute_reply": "2026-02-05T02:18:48.530863Z"
    },
    "papermill": {
     "duration": 1.352829,
     "end_time": "2026-02-05T02:18:48.533996",
     "exception": false,
     "start_time": "2026-02-05T02:18:47.181167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/penyisihan-datavidia-10/sample_submission.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2012-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-2023-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2010-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2018-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2025.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2014-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/data-indeks-standar-pencemar-udara-(ispu)-di-provinsi-dki-jakarta-komponen-data-2024.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2011-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2019-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2017-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2015-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2016-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2013-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2020-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2021-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/ISPU/indeks-standar-pencemaran-udara-(ispu)-tahun-2022-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/NDVI (vegetation index)/indeks-ndvi-jakarta.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/kualitas-air-sungai/data-kualitas-air-sungai-komponen-data.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/libur-nasional/dataset-libur-nasional-dan-weekend.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki3-jagakarsa.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki2-kelapagading.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki1-bundaranhi.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki5-kebonjeruk.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/cuaca-harian/cuaca-harian-dki4-lubangbuaya.csv\n",
      "/kaggle/input/penyisihan-datavidia-10/jumlah-penduduk/data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f67c365",
   "metadata": {
    "papermill": {
     "duration": 0.003831,
     "end_time": "2026-02-05T02:18:48.542193",
     "exception": false,
     "start_time": "2026-02-05T02:18:48.538362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading & Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f45103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:18:48.552382Z",
     "iopub.status.busy": "2026-02-05T02:18:48.551879Z",
     "iopub.status.idle": "2026-02-05T02:18:49.637402Z",
     "shell.execute_reply": "2026-02-05T02:18:49.636083Z"
    },
    "papermill": {
     "duration": 1.094352,
     "end_time": "2026-02-05T02:18:49.640237",
     "exception": false,
     "start_time": "2026-02-05T02:18:48.545885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/2288816290.py:151: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  d_dayfirst = pd.to_datetime(d_raw, errors=\"coerce\", dayfirst=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1 FILE QA (raw parse) ===\n",
      "                                                    rows  pct_date_nat  \\\n",
      "__source_file                                                            \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr...  1825      0.000000   \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr...  1830      0.000000   \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr...  1215      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...  1825      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   365      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   366      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   365      0.082192   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...  1825      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   365      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...  1830      0.041530   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...  1825      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   365      1.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   345      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   366      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...  1825      0.000000   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...   365      0.002740   \n",
      "\n",
      "                                                     date_min   date_max  \\\n",
      "__source_file                                                              \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr... 2022-12-01 2023-11-30   \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr... 2024-01-01 2024-12-31   \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr... 2025-01-01 2025-08-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2010-01-01 2010-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2011-01-01 2011-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2012-01-01 2012-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2013-01-01 2013-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2014-01-01 2014-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2015-01-01 2015-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2016-01-01 2016-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2017-01-01 2017-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...        NaT        NaT   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2019-01-01 2019-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2020-01-01 2020-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2021-01-01 2021-12-31   \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20... 2020-02-01 2022-12-30   \n",
      "\n",
      "                                                    pct_station_valid  \n",
      "__source_file                                                          \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr...           1.000000  \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr...           1.000000  \n",
      "data-indeks-standar-pencemar-udara-(ispu)-di-pr...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           0.912568  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           1.000000  \n",
      "indeks-standar-pencemaran-udara-(ispu)-tahun-20...           0.997260  \n",
      "\n",
      "=== STAGE 1 SUMMARY (REV8) ===\n",
      "ISPU files loaded           : 16\n",
      "Rows after date<=HIST_END   : 16430\n",
      "Rows removed (corrupt/bad)  : 33\n",
      "Rows after dedup            : 16183\n",
      "Train rows (labeled 3c)     : 14724\n",
      "\n",
      "Date range per station (hist):\n",
      "                    min        max  count\n",
      "stasiun_code                             \n",
      "DKI1         2010-01-01 2025-08-31   2888\n",
      "DKI2         2010-01-01 2025-08-31   3352\n",
      "DKI3         2010-01-01 2025-08-31   3223\n",
      "DKI4         2010-01-01 2025-08-31   3603\n",
      "DKI5         2010-01-01 2025-08-31   3117\n",
      "\n",
      "Label distribution (train only):\n",
      "label_3\n",
      "SEDANG         10138\n",
      "TIDAK SEHAT     2366\n",
      "BAIK            2220\n",
      "Name: count, dtype: Int64\n",
      "\n",
      "Last 30 days coverage (unique stations per day):\n",
      "tanggal\n",
      "2025-08-02    5\n",
      "2025-08-03    5\n",
      "2025-08-04    5\n",
      "2025-08-05    5\n",
      "2025-08-06    5\n",
      "2025-08-07    5\n",
      "2025-08-08    5\n",
      "2025-08-09    5\n",
      "2025-08-10    5\n",
      "2025-08-11    5\n",
      "2025-08-12    5\n",
      "2025-08-13    5\n",
      "2025-08-14    5\n",
      "2025-08-15    5\n",
      "2025-08-16    5\n",
      "2025-08-17    5\n",
      "2025-08-18    5\n",
      "2025-08-19    5\n",
      "2025-08-20    5\n",
      "2025-08-21    5\n",
      "2025-08-22    5\n",
      "2025-08-23    5\n",
      "2025-08-24    5\n",
      "2025-08-25    5\n",
      "2025-08-26    5\n",
      "2025-08-27    5\n",
      "2025-08-28    5\n",
      "2025-08-29    5\n",
      "2025-08-30    5\n",
      "2025-08-31    5\n",
      "Name: stasiun_code, dtype: int64\n",
      "\n",
      "Expected HIST_END           : 2025-08-31\n",
      "Observed max date (hist)    : 2025-08-31\n",
      "\n",
      "[OK] Histori mencapai cutoff. Lanjut Stage 2 aman.\n",
      "\n",
      "[OK] Stage 1 completed: df_ispu_hist, df_ispu_train, df_sample ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 1 — Data Loading & Sanity Checks (REV8: OUT-OF-BOUNDS SAFE + MASK MERGE)\n",
    "# Fix utama:\n",
    "# - Date parse: multi-strategy + VALID RANGE clamp (anti tahun ngaco 124904)\n",
    "# - periode_data YYYYMM divalidasi (min/max) sebelum dipakai\n",
    "# - Merge kandidat tanggal pakai mask assignment (bukan fillna chain) -> anti pandas AssertionError\n",
    "# - Robust CSV reader: auto delimiter + skip bad lines + encoding fallback\n",
    "#\n",
    "# Output:\n",
    "#   df_sample, df_ispu_hist, df_ispu_train\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n",
    "SAMPLE_PATH = ROOT / \"sample_submission.csv\"\n",
    "ISPU_DIR = ROOT / \"ISPU\"\n",
    "\n",
    "CUTOFF_DATE = pd.Timestamp(\"2025-09-01\")\n",
    "HIST_END = CUTOFF_DATE - pd.Timedelta(days=1)  # 2025-08-31\n",
    "VALID_STATIONS = [f\"DKI{i}\" for i in range(1, 6)]\n",
    "\n",
    "# Range valid untuk tanggal ISPU (clamp semua parse di luar ini -> NaT)\n",
    "MIN_VALID_DATE = pd.Timestamp(\"2009-01-01\")\n",
    "MAX_VALID_DATE = HIST_END\n",
    "\n",
    "# Range valid untuk periode_data (YYYYMM)\n",
    "MIN_VALID_YYYYMM = 200901\n",
    "MAX_VALID_YYYYMM = 202512\n",
    "\n",
    "# Excel serial range aman (kira-kira 1995 s.d. 2036)\n",
    "MIN_EXCEL_SERIAL = 35000\n",
    "MAX_EXCEL_SERIAL = 50000\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def norm_colname(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"\\ufeff\", \"\")  # BOM\n",
    "    s = re.sub(r\"[^\\w]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    tries = [\n",
    "        dict(),\n",
    "        dict(encoding=\"utf-8-sig\"),\n",
    "        dict(encoding=\"latin1\"),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\"),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\", encoding=\"utf-8-sig\"),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\", encoding=\"latin1\"),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for kw in tries:\n",
    "        try:\n",
    "            return pd.read_csv(path, **kw)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed reading {path.name}: {last_err}\")\n",
    "\n",
    "def series_or_na(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    return df[col] if col in df.columns else pd.Series(pd.NA, index=df.index)\n",
    "\n",
    "def coalesce_cols(df: pd.DataFrame, cols: list[str]) -> pd.Series:\n",
    "    mats = [series_or_na(df, c) for c in cols]\n",
    "    return pd.concat(mats, axis=1).bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "def clean_missing_tokens(s: pd.Series) -> pd.Series:\n",
    "    return s.replace({\"---\": pd.NA, \"—\": pd.NA, \"-\": pd.NA, \"\": pd.NA, \"NA\": pd.NA, \"N/A\": pd.NA})\n",
    "\n",
    "def to_num(s: pd.Series) -> pd.Series:\n",
    "    s = clean_missing_tokens(s)\n",
    "    if s.dtype.name == \"string\":\n",
    "        ss = s.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "        return pd.to_numeric(ss, errors=\"coerce\")\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def coalesce_num(df: pd.DataFrame, cols: list[str]) -> pd.Series:\n",
    "    mats = [to_num(series_or_na(df, c)) for c in cols]\n",
    "    return pd.concat(mats, axis=1).bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "def clamp_datetime(s: pd.Series) -> pd.Series:\n",
    "    # Pastikan datetime64[ns] dan buang yang di luar rentang valid\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    ok = s.between(MIN_VALID_DATE, MAX_VALID_DATE, inclusive=\"both\")\n",
    "    return s.where(ok, pd.NaT)\n",
    "\n",
    "def extract_yyyymm(per: pd.Series) -> pd.Series:\n",
    "    s = clean_missing_tokens(per.astype(\"string\"))\n",
    "\n",
    "    # 1) ambil 6 digit pertama yang tampil (mis. 202501)\n",
    "    a = s.str.extract(r\"(\\d{6})\", expand=False)\n",
    "\n",
    "    # 2) pola YYYY-MM / YYYY/MM\n",
    "    b = s.str.extract(r\"(\\d{4})\\D(\\d{2})\", expand=True)\n",
    "    b_yyyymm = pd.Series(pd.NA, index=s.index, dtype=\"string\")\n",
    "    if b.shape[1] >= 2:\n",
    "        b_yyyymm = b[0].astype(\"string\").str.zfill(4) + b[1].astype(\"string\").str.zfill(2)\n",
    "\n",
    "    out = a.fillna(b_yyyymm)\n",
    "    yyyymm = to_num(out)\n",
    "\n",
    "    # VALIDASI range: kalau bukan 200901..202512 -> anggap sampah\n",
    "    ok = yyyymm.between(MIN_VALID_YYYYMM, MAX_VALID_YYYYMM, inclusive=\"both\")\n",
    "    return yyyymm.where(ok, np.nan)\n",
    "\n",
    "def ymd_from_yyyymm_day(yyyymm: pd.Series, day: pd.Series) -> pd.Series:\n",
    "    yyyymm = to_num(yyyymm)\n",
    "    day = to_num(day)\n",
    "\n",
    "    y = np.floor(yyyymm / 100.0)\n",
    "    m = np.floor(yyyymm % 100.0)\n",
    "    d = np.floor(day)\n",
    "\n",
    "    # validasi komponen tanggal\n",
    "    mask = (\n",
    "        np.isfinite(y) & np.isfinite(m) & np.isfinite(d) &\n",
    "        (y >= 2009) & (y <= 2025) &\n",
    "        (m >= 1) & (m <= 12) &\n",
    "        (d >= 1) & (d <= 31)\n",
    "    )\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=day.index, dtype=\"datetime64[ns]\")\n",
    "    if mask.any():\n",
    "        ys = pd.Series(y[mask].astype(np.int64), index=out.index[mask]).astype(str).str.zfill(4)\n",
    "        ms = pd.Series(m[mask].astype(np.int64), index=out.index[mask]).astype(str).str.zfill(2)\n",
    "        ds = pd.Series(d[mask].astype(np.int64), index=out.index[mask]).astype(str).str.zfill(2)\n",
    "        out.loc[mask] = pd.to_datetime(ys + \"-\" + ms + \"-\" + ds, errors=\"coerce\")\n",
    "    return clamp_datetime(out)\n",
    "\n",
    "def merge_dates(primary: pd.Series, fallback: pd.Series) -> pd.Series:\n",
    "    # Gabung tanpa fillna chain untuk menghindari pandas AssertionError\n",
    "    out = primary.copy()\n",
    "    m = out.isna() & fallback.notna()\n",
    "    if m.any():\n",
    "        out.loc[m] = fallback.loc[m].values\n",
    "    return out\n",
    "\n",
    "def parse_date_flexible(df: pd.DataFrame) -> pd.Series:\n",
    "    d_raw = clean_missing_tokens(coalesce_cols(df, [\"tanggal\", \"time\", \"date\"])).astype(\"string\")\n",
    "\n",
    "    # A) format YYYY-MM-DD (silence warning dayfirst)\n",
    "    d_iso = pd.to_datetime(d_raw, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "    d_iso = clamp_datetime(d_iso)\n",
    "\n",
    "    # B) dayfirst parse (dd/mm/yyyy dll)\n",
    "    d_dayfirst = pd.to_datetime(d_raw, errors=\"coerce\", dayfirst=True)\n",
    "    d_dayfirst = clamp_datetime(d_dayfirst)\n",
    "\n",
    "    # C) excel serial (hanya jika serial masuk range aman)\n",
    "    d_num = to_num(d_raw)\n",
    "    serial_ok = d_num.between(MIN_EXCEL_SERIAL, MAX_EXCEL_SERIAL, inclusive=\"both\")\n",
    "    d_excel = pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "    if serial_ok.any():\n",
    "        d_excel.loc[serial_ok] = pd.to_datetime(d_num.loc[serial_ok], unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    d_excel = clamp_datetime(d_excel)\n",
    "\n",
    "    # D) periode_data YYYYMM + day-of-month (kalau tanggal hanya 1..31)\n",
    "    yyyymm = extract_yyyymm(series_or_na(df, \"periode_data\"))\n",
    "    d_from_per = ymd_from_yyyymm_day(yyyymm, d_raw)\n",
    "\n",
    "    # E) fallback YYYYMMDD (8 digit)\n",
    "    d_yyyymmdd = d_raw.str.extract(r\"(\\d{8})\", expand=False)\n",
    "    d_full8 = pd.to_datetime(d_yyyymmdd, format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    d_full8 = clamp_datetime(d_full8)\n",
    "\n",
    "    # MERGE berurutan (tanpa fillna chain)\n",
    "    out = pd.Series(pd.NaT, index=df.index, dtype=\"datetime64[ns]\")\n",
    "    out = merge_dates(out, d_iso)\n",
    "    out = merge_dates(out, d_dayfirst)\n",
    "    out = merge_dates(out, d_excel)\n",
    "    out = merge_dates(out, d_from_per)\n",
    "    out = merge_dates(out, d_full8)\n",
    "    return out\n",
    "\n",
    "_LOC_TO_DKI = [\n",
    "    (r\"BUNDARAN\\s*HI|\\bHI\\b\", \"DKI1\"),\n",
    "    (r\"KELAPA\\s*GADING\", \"DKI2\"),\n",
    "    (r\"JAGAKARSA\", \"DKI3\"),\n",
    "    (r\"LUBANG\\s*BUAYA\", \"DKI4\"),\n",
    "    (r\"KEBON\\s*JERUK\", \"DKI5\"),\n",
    "]\n",
    "\n",
    "def parse_station_code(raw: pd.Series) -> pd.Series:\n",
    "    s = raw.astype(\"string\").str.upper().str.strip()\n",
    "    s2 = s.str.replace(\"SPKU\", \"\", regex=False).str.replace(\"STASIUN\", \"\", regex=False)\n",
    "    s2 = s2.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "\n",
    "    code = s2.str.extract(r\"(DKI\\s*[-]?\\s*[1-5])\", expand=False)\n",
    "    code = code.str.replace(r\"\\s*-\\s*\", \"\", regex=True).str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "    loc_code = pd.Series(pd.NA, index=s.index, dtype=\"string\")\n",
    "    for pat, rep in _LOC_TO_DKI:\n",
    "        hit = s2.str.contains(pat, regex=True, na=False)\n",
    "        loc_code = loc_code.fillna(pd.Series(np.where(hit, rep, pd.NA), index=s.index, dtype=\"string\"))\n",
    "\n",
    "    digit = s2.str.extract(r\"\\b([1-5])\\b\", expand=False)\n",
    "    digit_code = (\"DKI\" + digit).astype(\"string\")\n",
    "\n",
    "    return code.fillna(loc_code).fillna(digit_code)\n",
    "\n",
    "def map_label_3class(raw: pd.Series) -> pd.Series:\n",
    "    s = raw.astype(\"string\").str.upper().str.strip()\n",
    "    s = s.replace({\n",
    "        \"SANGAT TIDAK SEHAT\": \"TIDAK SEHAT\",\n",
    "        \"BERBAHAYA\": \"TIDAK SEHAT\",\n",
    "        \"TIDAK ADA DATA\": pd.NA,\n",
    "        \"NA\": pd.NA,\n",
    "    })\n",
    "    return s.where(s.isin([\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\"]), pd.NA)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) sample_submission\n",
    "# -----------------------------\n",
    "df_sample = pd.read_csv(SAMPLE_PATH)\n",
    "id_col = df_sample.columns[0]\n",
    "target_col = df_sample.columns[1] if len(df_sample.columns) > 1 else \"category\"\n",
    "\n",
    "parts = df_sample[id_col].astype(str).str.split(\"_\", n=1, expand=True)\n",
    "df_sample[\"tanggal_target\"] = pd.to_datetime(parts[0], errors=\"coerce\")\n",
    "df_sample[\"stasiun_code\"] = parts[1].astype(\"string\").str.upper().str.strip()\n",
    "\n",
    "assert df_sample[\"tanggal_target\"].min() == pd.Timestamp(\"2025-09-01\")\n",
    "assert df_sample[\"tanggal_target\"].max() == pd.Timestamp(\"2025-11-30\")\n",
    "assert df_sample.shape[0] == 455\n",
    "assert set(df_sample[\"stasiun_code\"].unique()) == set(VALID_STATIONS)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load semua ISPU\n",
    "# -----------------------------\n",
    "ispu_files = sorted(ISPU_DIR.glob(\"*.csv\"))\n",
    "if len(ispu_files) == 0:\n",
    "    raise RuntimeError(f\"Tidak menemukan file ISPU di {ISPU_DIR}\")\n",
    "\n",
    "frames = []\n",
    "for fp in ispu_files:\n",
    "    dfi = read_csv_robust(fp)\n",
    "    dfi.columns = [norm_colname(c) for c in dfi.columns]\n",
    "    dfi[\"__source_file\"] = fp.name\n",
    "    frames.append(dfi)\n",
    "\n",
    "df_ispu0 = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Normalize core fields\n",
    "# -----------------------------\n",
    "raw_station = coalesce_cols(df_ispu0, [\"stasiun\", \"lokasi_spku\", \"stasiun_id\", \"lokasi\"])\n",
    "raw_label   = coalesce_cols(df_ispu0, [\"kategori\", \"categori\", \"category\"])\n",
    "raw_crit    = coalesce_cols(df_ispu0, [\"parameter_pencemar_kritis\", \"critical\", \"parameter_kritis\"])\n",
    "\n",
    "tanggal = parse_date_flexible(df_ispu0)\n",
    "st_code = parse_station_code(raw_station)\n",
    "\n",
    "max_idx = coalesce_num(df_ispu0, [\"max\", \"maks\", \"indeks\", \"nilai_indeks\"])\n",
    "pm10 = coalesce_num(df_ispu0, [\"pm10\", \"pm_10\", \"pm_sepuluh\"])\n",
    "pm25 = coalesce_num(df_ispu0, [\"pm25\", \"pm_25\", \"pm_duakomalima\", \"pm_dua_koma_lima\", \"pm2_5\", \"pm2_5_\"])\n",
    "so2  = coalesce_num(df_ispu0, [\"so2\", \"sulfur_dioksida\"])\n",
    "co   = coalesce_num(df_ispu0, [\"co\", \"karbon_monoksida\"])\n",
    "o3   = coalesce_num(df_ispu0, [\"o3\", \"ozon\"])\n",
    "no2  = coalesce_num(df_ispu0, [\"no2\", \"nitrogen_dioksida\"])\n",
    "\n",
    "df_ispu = pd.DataFrame({\n",
    "    \"tanggal\": tanggal,\n",
    "    \"stasiun_raw\": raw_station.astype(\"string\"),\n",
    "    \"stasiun_code\": st_code.astype(\"string\"),\n",
    "    \"label_raw\": raw_label.astype(\"string\"),\n",
    "    \"critical_raw\": raw_crit.astype(\"string\"),\n",
    "    \"max\": max_idx,\n",
    "    \"pm10\": pm10,\n",
    "    \"pm25\": pm25,\n",
    "    \"so2\": so2,\n",
    "    \"co\": co,\n",
    "    \"o3\": o3,\n",
    "    \"no2\": no2,\n",
    "    \"__source_file\": df_ispu0[\"__source_file\"].astype(\"string\"),\n",
    "})\n",
    "\n",
    "df_ispu[\"label_3\"] = map_label_3class(df_ispu[\"label_raw\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 4) FILE-LEVEL QA (raw parse)\n",
    "# -----------------------------\n",
    "tmp = df_ispu.copy()\n",
    "tmp[\"is_valid_station\"] = tmp[\"stasiun_code\"].isin(VALID_STATIONS)\n",
    "tmp[\"is_valid_date\"] = tmp[\"tanggal\"].notna()\n",
    "\n",
    "file_qa = (tmp.groupby(\"__source_file\")\n",
    "             .agg(\n",
    "                 rows=(\"tanggal\",\"size\"),\n",
    "                 pct_date_nat=(\"is_valid_date\", lambda x: 1 - float(np.mean(x))),\n",
    "                 date_min=(\"tanggal\",\"min\"),\n",
    "                 date_max=(\"tanggal\",\"max\"),\n",
    "                 pct_station_valid=(\"is_valid_station\",\"mean\"),\n",
    "             )\n",
    "             .sort_index())\n",
    "\n",
    "print(\"=== STAGE 1 FILE QA (raw parse) ===\")\n",
    "print(file_qa)\n",
    "print()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Filter & clean (cutoff + corrupt + bad station)\n",
    "# -----------------------------\n",
    "df_ispu = df_ispu.loc[df_ispu[\"tanggal\"].notna()].copy()\n",
    "df_ispu = df_ispu.loc[df_ispu[\"tanggal\"] <= HIST_END].copy()\n",
    "\n",
    "station_upper = df_ispu[\"stasiun_raw\"].fillna(\"\").str.upper()\n",
    "label_upper   = df_ispu[\"label_raw\"].fillna(\"\").str.upper()\n",
    "\n",
    "class_tokens = {\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\", \"SANGAT TIDAK SEHAT\", \"BERBAHAYA\", \"TIDAK ADA DATA\"}\n",
    "poll_tokens  = {\"O3\", \"NO2\", \"SO2\", \"CO\", \"PM10\", \"PM2.5\", \"PM25\", \"PM_10\", \"PM_2_5\"}\n",
    "\n",
    "is_corrupt = (station_upper.isin(class_tokens) | label_upper.isin(poll_tokens))\n",
    "is_bad_station = ~df_ispu[\"stasiun_code\"].isin(VALID_STATIONS)\n",
    "\n",
    "n_before = len(df_ispu)\n",
    "df_ispu = df_ispu.loc[~(is_corrupt | is_bad_station)].copy()\n",
    "n_after = len(df_ispu)\n",
    "n_removed = n_before - n_after\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Dedup best record per (tanggal, stasiun)\n",
    "# -----------------------------\n",
    "feat_cols = [\"max\", \"pm10\", \"pm25\", \"so2\", \"co\", \"o3\", \"no2\"]\n",
    "completeness = df_ispu[feat_cols].notna().sum(axis=1).astype(int)\n",
    "label_bonus = df_ispu[\"label_3\"].notna().astype(int)\n",
    "df_ispu[\"_score\"] = completeness * 10 + label_bonus\n",
    "\n",
    "df_ispu = (\n",
    "    df_ispu.sort_values([\"tanggal\", \"stasiun_code\", \"_score\"], ascending=[True, True, False])\n",
    "           .drop_duplicates([\"tanggal\", \"stasiun_code\"], keep=\"first\")\n",
    "           .drop(columns=[\"_score\"])\n",
    "           .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_ispu_hist = df_ispu.copy()\n",
    "df_ispu_train = df_ispu_hist.loc[df_ispu_hist[\"label_3\"].notna()].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Sanity summary\n",
    "# -----------------------------\n",
    "print(\"=== STAGE 1 SUMMARY (REV8) ===\")\n",
    "print(f\"ISPU files loaded           : {len(ispu_files)}\")\n",
    "print(f\"Rows after date<=HIST_END   : {n_before}\")\n",
    "print(f\"Rows removed (corrupt/bad)  : {n_removed}\")\n",
    "print(f\"Rows after dedup            : {len(df_ispu_hist)}\")\n",
    "print(f\"Train rows (labeled 3c)     : {len(df_ispu_train)}\")\n",
    "print()\n",
    "\n",
    "rng = df_ispu_hist.groupby(\"stasiun_code\")[\"tanggal\"].agg([\"min\",\"max\",\"count\"]).reindex(VALID_STATIONS)\n",
    "print(\"Date range per station (hist):\")\n",
    "print(rng)\n",
    "print()\n",
    "\n",
    "print(\"Label distribution (train only):\")\n",
    "print(df_ispu_train[\"label_3\"].value_counts(dropna=False))\n",
    "print()\n",
    "\n",
    "cov = df_ispu_hist.groupby(\"tanggal\")[\"stasiun_code\"].nunique().sort_index()\n",
    "print(\"Last 30 days coverage (unique stations per day):\")\n",
    "print(cov.tail(30))\n",
    "print()\n",
    "\n",
    "obs_max = df_ispu_hist[\"tanggal\"].max()\n",
    "print(f\"Expected HIST_END           : {HIST_END.date()}\")\n",
    "print(f\"Observed max date (hist)    : {obs_max.date() if pd.notna(obs_max) else obs_max}\")\n",
    "\n",
    "if obs_max != HIST_END:\n",
    "    print(\"\\n[WARN] Histori belum mencapai 2025-08-31.\")\n",
    "    print(\"Cek tabel FILE QA: cari file 2024/2025 yang date_max kecil atau pct_date_nat tinggi.\")\n",
    "else:\n",
    "    print(\"\\n[OK] Histori mencapai cutoff. Lanjut Stage 2 aman.\")\n",
    "\n",
    "print(\"\\n[OK] Stage 1 completed: df_ispu_hist, df_ispu_train, df_sample ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa296b75",
   "metadata": {
    "papermill": {
     "duration": 0.004109,
     "end_time": "2026-02-05T02:18:49.648688",
     "exception": false,
     "start_time": "2026-02-05T02:18:49.644579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Master Table Building (Correct Joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb12f0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:18:49.659686Z",
     "iopub.status.busy": "2026-02-05T02:18:49.659339Z",
     "iopub.status.idle": "2026-02-05T02:18:52.386802Z",
     "shell.execute_reply": "2026-02-05T02:18:52.385277Z"
    },
    "papermill": {
     "duration": 2.736339,
     "end_time": "2026-02-05T02:18:52.389093",
     "exception": false,
     "start_time": "2026-02-05T02:18:49.652754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/3419119886.py:89: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  d_df = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
      "/tmp/ipykernel_17/3419119886.py:332: RuntimeWarning: Engine has switched to 'python' because numexpr does not support extension array dtypes. Please set your engine to python manually.\n",
      "  .query(\"tahun >= 2009 and tahun <= 2025 and bulan >= 1 and bulan <= 12\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 2 SUMMARY (REV2) ===\n",
      "Submission rows: 455\n",
      "Target date range: 2025-09-01 -> 2025-11-30\n",
      "Stations in submission: ['DKI1', 'DKI2', 'DKI3', 'DKI4', 'DKI5']\n",
      "\n",
      "ISPU hist max date: 2025-08-31 | HIST_END: 2025-08-31\n",
      "Last observed date per station:\n",
      "stasiun_code\n",
      "DKI1   2025-08-31\n",
      "DKI2   2025-08-31\n",
      "DKI3   2025-08-31\n",
      "DKI4   2025-08-31\n",
      "DKI5   2025-08-31\n",
      "Name: tanggal, dtype: datetime64[ns]\n",
      "\n",
      "Horizon_days range (MUST 1..91): 1 -> 91\n",
      "\n",
      "Master table (df_master): (28610, 10) | missing label_3: 13886\n",
      "Aux shapes: NDVI daily (1810, 5) | NDVI monthly (945, 6) | Weather (28610, 25) | Water monthly (4, 6) | Pop yearly (4, 3)\n",
      "\n",
      "df_targets preview:\n",
      "                id tanggal_target stasiun_code last_obs_date  horizon_days  \\\n",
      "0  2025-09-01_DKI1     2025-09-01         DKI1    2025-08-31             1   \n",
      "1  2025-09-01_DKI2     2025-09-01         DKI2    2025-08-31             1   \n",
      "2  2025-09-01_DKI3     2025-09-01         DKI3    2025-08-31             1   \n",
      "3  2025-09-01_DKI4     2025-09-01         DKI4    2025-08-31             1   \n",
      "4  2025-09-01_DKI5     2025-09-01         DKI5    2025-08-31             1   \n",
      "5  2025-09-02_DKI1     2025-09-02         DKI1    2025-08-31             2   \n",
      "6  2025-09-02_DKI2     2025-09-02         DKI2    2025-08-31             2   \n",
      "7  2025-09-02_DKI3     2025-09-02         DKI3    2025-08-31             2   \n",
      "8  2025-09-02_DKI4     2025-09-02         DKI4    2025-08-31             2   \n",
      "9  2025-09-02_DKI5     2025-09-02         DKI5    2025-08-31             2   \n",
      "\n",
      "   is_weekend  is_holiday_nasional  holiday_or_weekend  \n",
      "0           0                    0                   0  \n",
      "1           0                    0                   0  \n",
      "2           0                    0                   0  \n",
      "3           0                    0                   0  \n",
      "4           0                    0                   0  \n",
      "5           0                    0                   0  \n",
      "6           0                    0                   0  \n",
      "7           0                    0                   0  \n",
      "8           0                    0                   0  \n",
      "9           0                    0                   0  \n",
      "\n",
      "[OK] Stage 2 completed: df_sub, df_calendar, df_targets, df_master, df_ndvi/df_ndvi_m, df_weather, df_water_m, df_pop_y ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 2 — Master Table Building (Correct Joins) — REV2 (TOPSCORE FOUNDATION)\n",
    "#\n",
    "# Upgrade utama:\n",
    "# - Build df_master: grid harian (tanggal x stasiun) sampai HIST_END (1 row per key)\n",
    "# - Calendar + holiday safe untuk future (Sep–Nov 2025) + cyclical time features lengkap\n",
    "# - df_targets: index submission + fitur kalender + last_obs_date + horizon_days (HARUS 1..91)\n",
    "# - Load aux tables (NDVI, Weather, Water, Pop) forecast-safe (<= HIST_END) + cleaned + dedup\n",
    "# - Robust date parsing (ISO first -> dayfirst -> excel) + clamp range\n",
    "#\n",
    "# Output:\n",
    "#   df_sub, df_calendar, df_targets, df_master\n",
    "#   last_obs_by_station\n",
    "#   df_ndvi, df_weather, df_water_m, df_pop_y\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- GUARDS ----------\n",
    "need = [\"df_ispu_hist\", \"HIST_END\", \"CUTOFF_DATE\", \"VALID_STATIONS\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from STAGE 1: {miss}. Jalankan STAGE 1 (REV8) dulu.\")\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n",
    "\n",
    "SUB_PATH    = DATA_ROOT / \"sample_submission.csv\"\n",
    "HOL_PATH    = DATA_ROOT / \"libur-nasional\" / \"dataset-libur-nasional-dan-weekend.csv\"\n",
    "NDVI_PATH   = DATA_ROOT / \"NDVI (vegetation index)\" / \"indeks-ndvi-jakarta.csv\"\n",
    "WATER_PATH  = DATA_ROOT / \"kualitas-air-sungai\" / \"data-kualitas-air-sungai-komponen-data.csv\"\n",
    "POP_PATH    = DATA_ROOT / \"jumlah-penduduk\" / \"data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\"\n",
    "WEATHER_DIR = DATA_ROOT / \"cuaca-harian\"\n",
    "\n",
    "MIN_VALID_DATE = pd.Timestamp(\"2009-01-01\")\n",
    "MAX_TARGET_DATE = pd.Timestamp(\"2025-11-30\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def norm_colname(c: str) -> str:\n",
    "    c = str(c).strip().lower().replace(\"\\ufeff\", \"\")\n",
    "    c = re.sub(r\"[()\\[\\]{}]\", \"\", c)\n",
    "    c = re.sub(r\"[%/]\", \"_\", c)\n",
    "    c = re.sub(r\"[^a-z0-9]+\", \"_\", c)\n",
    "    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "    return c\n",
    "\n",
    "def read_csv_robust(path: Path) -> pd.DataFrame:\n",
    "    tries = [\n",
    "        dict(),\n",
    "        dict(encoding=\"utf-8-sig\"),\n",
    "        dict(encoding=\"latin1\"),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\"),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\", encoding=\"utf-8-sig\"),\n",
    "        dict(engine=\"python\", sep=None, on_bad_lines=\"skip\", encoding=\"latin1\"),\n",
    "    ]\n",
    "    last_err = None\n",
    "    for kw in tries:\n",
    "        try:\n",
    "            return pd.read_csv(path, **kw)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Failed reading {path.name}: {last_err}\")\n",
    "\n",
    "def clean_missing_tokens(s: pd.Series) -> pd.Series:\n",
    "    return s.replace({\"---\": pd.NA, \"—\": pd.NA, \"-\": pd.NA, \"\": pd.NA, \"NA\": pd.NA, \"N/A\": pd.NA})\n",
    "\n",
    "def to_num(s: pd.Series) -> pd.Series:\n",
    "    s = clean_missing_tokens(s)\n",
    "    if s.dtype.name == \"string\":\n",
    "        ss = s.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "        return pd.to_numeric(ss, errors=\"coerce\")\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def clamp_date(dt: pd.Series) -> pd.Series:\n",
    "    dt = pd.to_datetime(dt, errors=\"coerce\")\n",
    "    ok = dt.between(MIN_VALID_DATE, MAX_TARGET_DATE, inclusive=\"both\")\n",
    "    return dt.where(ok, pd.NaT)\n",
    "\n",
    "def parse_any_date(s: pd.Series) -> pd.Series:\n",
    "    s = clean_missing_tokens(s).astype(\"string\")\n",
    "\n",
    "    # ISO first (yyyy-mm-dd) -> no warning\n",
    "    d_iso = pd.to_datetime(s, errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "    d_iso = clamp_date(d_iso)\n",
    "\n",
    "    # dayfirst fallback (dd/mm/yyyy etc)\n",
    "    d_df = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    d_df = clamp_date(d_df)\n",
    "\n",
    "    # excel serial fallback (batasi range aman)\n",
    "    n = to_num(s)\n",
    "    serial_ok = n.between(35000, 50000, inclusive=\"both\")\n",
    "    d_xl = pd.Series(pd.NaT, index=s.index, dtype=\"datetime64[ns]\")\n",
    "    if serial_ok.any():\n",
    "        d_xl.loc[serial_ok] = pd.to_datetime(n.loc[serial_ok], unit=\"D\", origin=\"1899-12-30\", errors=\"coerce\")\n",
    "    d_xl = clamp_date(d_xl)\n",
    "\n",
    "    out = pd.Series(pd.NaT, index=s.index, dtype=\"datetime64[ns]\")\n",
    "    m = out.isna() & d_iso.notna()\n",
    "    out.loc[m] = d_iso.loc[m].values\n",
    "    m = out.isna() & d_df.notna()\n",
    "    out.loc[m] = d_df.loc[m].values\n",
    "    m = out.isna() & d_xl.notna()\n",
    "    out.loc[m] = d_xl.loc[m].values\n",
    "    return out\n",
    "\n",
    "def parse_id_to_date_station(id_s: pd.Series) -> tuple[pd.Series, pd.Series]:\n",
    "    parts = id_s.astype(str).str.split(\"_\", n=1, expand=True)\n",
    "    dt = pd.to_datetime(parts[0], errors=\"coerce\")\n",
    "    st = parts[1].astype(\"string\").str.upper().str.strip()\n",
    "    return dt, st\n",
    "\n",
    "def normalize_station_code(s: pd.Series) -> pd.Series:\n",
    "    x = s.astype(\"string\").str.upper().str.strip()\n",
    "    code = x.str.extract(r\"(DKI\\s*[-]?\\s*[1-5])\", expand=False)\n",
    "    code = code.str.replace(r\"\\s*-\\s*\", \"\", regex=True).str.replace(\" \", \"\", regex=False)\n",
    "    digit = x.str.extract(r\"\\b([1-5])\\b\", expand=False)\n",
    "    digit_code = (\"DKI\" + digit).astype(\"string\")\n",
    "    return code.fillna(digit_code)\n",
    "\n",
    "def pick_first_col(df: pd.DataFrame, cols: list[str]) -> str | None:\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ============================================================\n",
    "# 2.1 Parse sample_submission -> df_sub\n",
    "# ============================================================\n",
    "df_sub0 = pd.read_csv(SUB_PATH)\n",
    "df_sub0.columns = [norm_colname(c) for c in df_sub0.columns]\n",
    "id_col = \"id\" if \"id\" in df_sub0.columns else df_sub0.columns[0]\n",
    "\n",
    "df_sub = pd.DataFrame({\"id\": df_sub0[id_col].astype(str)})\n",
    "df_sub[\"tanggal_target\"], df_sub[\"stasiun_code\"] = parse_id_to_date_station(df_sub[\"id\"])\n",
    "df_sub[\"stasiun_code\"] = normalize_station_code(df_sub[\"stasiun_code\"])\n",
    "\n",
    "df_sub = df_sub.loc[df_sub[\"tanggal_target\"].notna()].copy()\n",
    "df_sub = df_sub.loc[df_sub[\"stasiun_code\"].isin(VALID_STATIONS)].reset_index(drop=True)\n",
    "\n",
    "# hard QA (critical)\n",
    "if len(df_sub) != 455:\n",
    "    raise RuntimeError(f\"Submission index harus 455 baris, sekarang {len(df_sub)}\")\n",
    "if df_sub[\"tanggal_target\"].min() != pd.Timestamp(\"2025-09-01\") or df_sub[\"tanggal_target\"].max() != pd.Timestamp(\"2025-11-30\"):\n",
    "    raise RuntimeError(\"Range tanggal submission tidak sesuai 2025-09-01 s.d. 2025-11-30\")\n",
    "if set(df_sub[\"stasiun_code\"].unique()) != set(VALID_STATIONS):\n",
    "    raise RuntimeError(\"Stasiun di submission tidak lengkap DKI1..DKI5\")\n",
    "\n",
    "# ============================================================\n",
    "# 2.2 ISPU history subset (<= HIST_END) + last_obs_date\n",
    "# ============================================================\n",
    "df_hist = df_ispu_hist.loc[\n",
    "    df_ispu_hist[\"stasiun_code\"].isin(VALID_STATIONS) &\n",
    "    (df_ispu_hist[\"tanggal\"] <= HIST_END)\n",
    "].copy()\n",
    "\n",
    "# last obs per stasiun (harusnya 2025-08-31 untuk semua)\n",
    "last_obs_by_station = df_hist.groupby(\"stasiun_code\")[\"tanggal\"].max().reindex(VALID_STATIONS)\n",
    "\n",
    "# fallback jika ada station yang kosong (harusnya tidak terjadi)\n",
    "last_obs_by_station = last_obs_by_station.fillna(HIST_END)\n",
    "\n",
    "# ============================================================\n",
    "# 2.3 Build df_master (grid harian stasiun x tanggal) — TOPSCORE FOUNDATION\n",
    "# ============================================================\n",
    "hist_min = df_hist[\"tanggal\"].min()\n",
    "hist_max = df_hist[\"tanggal\"].max()\n",
    "\n",
    "all_dates = pd.date_range(hist_min, HIST_END, freq=\"D\")\n",
    "grid = pd.MultiIndex.from_product([all_dates, VALID_STATIONS], names=[\"tanggal\", \"stasiun_code\"])\n",
    "df_master = pd.DataFrame(index=grid).reset_index()\n",
    "\n",
    "# pastikan df_hist unique key (stage 1 sudah dedup, tapi cek lagi)\n",
    "dup = df_hist.duplicated([\"tanggal\", \"stasiun_code\"]).sum()\n",
    "if dup > 0:\n",
    "    raise RuntimeError(f\"df_hist masih memiliki duplikasi key: {dup} baris. Stage 1 dedup belum aman.\")\n",
    "\n",
    "# merge ISPU fields (label + numeric) ke master\n",
    "keep_cols = [c for c in [\"tanggal\",\"stasiun_code\",\"label_3\",\"max\",\"pm10\",\"pm25\",\"so2\",\"co\",\"o3\",\"no2\"] if c in df_hist.columns]\n",
    "df_master = df_master.merge(df_hist[keep_cols], on=[\"tanggal\",\"stasiun_code\"], how=\"left\")\n",
    "\n",
    "# ============================================================\n",
    "# 2.4 Calendar table + holiday (allowed future features)\n",
    "# ============================================================\n",
    "cal_min = min(hist_min, df_sub[\"tanggal_target\"].min())\n",
    "cal_max = max(HIST_END, df_sub[\"tanggal_target\"].max())\n",
    "\n",
    "df_calendar = pd.DataFrame({\"tanggal\": pd.date_range(cal_min, cal_max, freq=\"D\")})\n",
    "df_calendar[\"year\"] = df_calendar[\"tanggal\"].dt.year\n",
    "df_calendar[\"month\"] = df_calendar[\"tanggal\"].dt.month\n",
    "df_calendar[\"day\"] = df_calendar[\"tanggal\"].dt.day\n",
    "df_calendar[\"dow\"] = df_calendar[\"tanggal\"].dt.dayofweek\n",
    "df_calendar[\"dayofyear\"] = df_calendar[\"tanggal\"].dt.dayofyear\n",
    "df_calendar[\"weekofyear\"] = df_calendar[\"tanggal\"].dt.isocalendar().week.astype(int)\n",
    "df_calendar[\"is_month_start\"] = df_calendar[\"tanggal\"].dt.is_month_start.astype(int)\n",
    "df_calendar[\"is_month_end\"] = df_calendar[\"tanggal\"].dt.is_month_end.astype(int)\n",
    "\n",
    "# cyclical\n",
    "df_calendar[\"sin_doy\"] = np.sin(2*np.pi*df_calendar[\"dayofyear\"]/365.25)\n",
    "df_calendar[\"cos_doy\"] = np.cos(2*np.pi*df_calendar[\"dayofyear\"]/365.25)\n",
    "df_calendar[\"sin_month\"] = np.sin(2*np.pi*df_calendar[\"month\"]/12.0)\n",
    "df_calendar[\"cos_month\"] = np.cos(2*np.pi*df_calendar[\"month\"]/12.0)\n",
    "df_calendar[\"sin_dow\"] = np.sin(2*np.pi*df_calendar[\"dow\"]/7.0)\n",
    "df_calendar[\"cos_dow\"] = np.cos(2*np.pi*df_calendar[\"dow\"]/7.0)\n",
    "\n",
    "# holiday table\n",
    "df_hol = read_csv_robust(HOL_PATH)\n",
    "df_hol.columns = [norm_colname(c) for c in df_hol.columns]\n",
    "tcol_h = pick_first_col(df_hol, [\"tanggal\", \"date\", \"time\"])\n",
    "if tcol_h is None:\n",
    "    raise RuntimeError(\"Holiday table tidak punya kolom tanggal/time/date yang bisa diparse.\")\n",
    "\n",
    "df_hol[\"tanggal\"] = parse_any_date(df_hol[tcol_h])\n",
    "keep_h = [c for c in [\"tanggal\",\"is_holiday_nasional\",\"nama_libur\",\"is_weekend\",\"day_name\"] if c in df_hol.columns]\n",
    "df_hol = df_hol[keep_h].dropna(subset=[\"tanggal\"]).drop_duplicates(\"tanggal\")\n",
    "\n",
    "df_calendar = df_calendar.merge(df_hol, on=\"tanggal\", how=\"left\")\n",
    "\n",
    "# fallback weekend/holiday\n",
    "df_calendar[\"is_weekend\"] = df_calendar[\"is_weekend\"].fillna((df_calendar[\"dow\"] >= 5).astype(int)).astype(int)\n",
    "df_calendar[\"is_holiday_nasional\"] = df_calendar[\"is_holiday_nasional\"].fillna(0).astype(int)\n",
    "df_calendar[\"holiday_or_weekend\"] = ((df_calendar[\"is_weekend\"] == 1) | (df_calendar[\"is_holiday_nasional\"] == 1)).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# 2.5 df_targets = submission index + calendar features + horizon\n",
    "# ============================================================\n",
    "df_targets = df_sub.merge(\n",
    "    df_calendar,\n",
    "    left_on=\"tanggal_target\",\n",
    "    right_on=\"tanggal\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"tanggal\"])\n",
    "\n",
    "df_targets[\"last_obs_date\"] = df_targets[\"stasiun_code\"].map(last_obs_by_station)\n",
    "df_targets[\"horizon_days\"] = (df_targets[\"tanggal_target\"] - df_targets[\"last_obs_date\"]).dt.days\n",
    "\n",
    "# CRITICAL QA: horizon harus 1..91 (Sep–Nov from 2025-08-31)\n",
    "hmin, hmax = int(df_targets[\"horizon_days\"].min()), int(df_targets[\"horizon_days\"].max())\n",
    "if hmin < 1 or hmax > 91:\n",
    "    raise RuntimeError(f\"horizon_days tidak sesuai. Dapat {hmin}..{hmax}, seharusnya 1..91. Cek last_obs_date / cutoff.\")\n",
    "\n",
    "# ============================================================\n",
    "# 2.6 Aux tables (forecasting-safe: <= HIST_END)\n",
    "# ============================================================\n",
    "\n",
    "# ---- NDVI (keep minimal, dedup) ----\n",
    "df_ndvi = read_csv_robust(NDVI_PATH)\n",
    "df_ndvi.columns = [norm_colname(c) for c in df_ndvi.columns]\n",
    "\n",
    "tcol_n = pick_first_col(df_ndvi, [\"tanggal\", \"time\", \"date\"])\n",
    "if tcol_n:\n",
    "    df_ndvi[\"tanggal\"] = parse_any_date(df_ndvi[tcol_n])\n",
    "else:\n",
    "    df_ndvi[\"tanggal\"] = pd.NaT\n",
    "\n",
    "sid_col = pick_first_col(df_ndvi, [\"stasiun_code\", \"stasiun_id\", \"stasiun\", \"lokasi\"])\n",
    "sid = df_ndvi[sid_col] if sid_col else pd.Series(pd.NA, index=df_ndvi.index)\n",
    "df_ndvi[\"stasiun_code\"] = normalize_station_code(sid)\n",
    "\n",
    "if \"ndvi\" in df_ndvi.columns:\n",
    "    df_ndvi[\"ndvi\"] = to_num(df_ndvi[\"ndvi\"])\n",
    "else:\n",
    "    df_ndvi[\"ndvi\"] = np.nan\n",
    "\n",
    "df_ndvi = df_ndvi.loc[df_ndvi[\"tanggal\"].notna()].copy()\n",
    "df_ndvi = df_ndvi.loc[df_ndvi[\"tanggal\"] <= HIST_END].copy()\n",
    "df_ndvi = df_ndvi.loc[df_ndvi[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "df_ndvi = df_ndvi[[\"tanggal\",\"stasiun_code\",\"ndvi\"]].drop_duplicates([\"tanggal\",\"stasiun_code\"]).reset_index(drop=True)\n",
    "\n",
    "# tambahan: versi bulanan (lebih stabil untuk asof join di stage 3)\n",
    "df_ndvi[\"year\"] = df_ndvi[\"tanggal\"].dt.year\n",
    "df_ndvi[\"month\"] = df_ndvi[\"tanggal\"].dt.month\n",
    "df_ndvi_m = (df_ndvi.groupby([\"stasiun_code\",\"year\",\"month\"], as_index=False)\n",
    "                    .agg(ndvi_mean=(\"ndvi\",\"mean\"), ndvi_n=(\"ndvi\",\"count\")))\n",
    "df_ndvi_m[\"tanggal_bulan\"] = pd.to_datetime(df_ndvi_m[\"year\"].astype(int).astype(str) + \"-\" +\n",
    "                                            df_ndvi_m[\"month\"].astype(int).astype(str).str.zfill(2) + \"-01\")\n",
    "\n",
    "# ---- Weather (per stasiun, forecasting-safe <= HIST_END) ----\n",
    "w_files = sorted(WEATHER_DIR.glob(\"cuaca-harian-*.csv\"))\n",
    "w_list = []\n",
    "for p in w_files:\n",
    "    m = re.search(r\"dki(\\d+)\", p.name.lower())\n",
    "    st_code = f\"DKI{m.group(1)}\" if m else None\n",
    "    if st_code not in VALID_STATIONS:\n",
    "        continue\n",
    "\n",
    "    d = read_csv_robust(p)\n",
    "    d.columns = [norm_colname(c) for c in d.columns]\n",
    "    tcol = pick_first_col(d, [\"time\",\"tanggal\",\"date\",\"waktu\"])\n",
    "    if not tcol:\n",
    "        continue\n",
    "\n",
    "    d[\"tanggal\"] = parse_any_date(d[tcol])\n",
    "    d[\"stasiun_code\"] = st_code\n",
    "    d = d.drop(columns=[tcol], errors=\"ignore\")\n",
    "    d = d.dropna(subset=[\"tanggal\"]).copy()\n",
    "    d = d.loc[d[\"tanggal\"] <= HIST_END].copy()\n",
    "\n",
    "    # convert numeric columns except station/date\n",
    "    for c in d.columns:\n",
    "        if c in [\"tanggal\",\"stasiun_code\"]:\n",
    "            continue\n",
    "        d[c] = to_num(d[c].astype(\"string\"))\n",
    "\n",
    "    d = d.drop_duplicates([\"tanggal\",\"stasiun_code\"]).reset_index(drop=True)\n",
    "    w_list.append(d)\n",
    "\n",
    "df_weather = pd.concat(w_list, ignore_index=True) if len(w_list) else pd.DataFrame(columns=[\"tanggal\",\"stasiun_code\"])\n",
    "\n",
    "# ---- Water quality (aggregate by year-month; forecasting-safe <= HIST_END) ----\n",
    "df_water = read_csv_robust(WATER_PATH)\n",
    "df_water.columns = [norm_colname(c) for c in df_water.columns]\n",
    "\n",
    "for c in [\"hasil_pengukuran\",\"baku_mutu\"]:\n",
    "    if c in df_water.columns:\n",
    "        df_water[c] = to_num(df_water[c].astype(\"string\"))\n",
    "\n",
    "ycol = pick_first_col(df_water, [\"periode_data\",\"tahun\"])\n",
    "mcol = pick_first_col(df_water, [\"bulan_sampling\",\"bulan\"])\n",
    "df_water[\"tahun\"] = to_num(df_water[ycol].astype(\"string\")) if ycol else np.nan\n",
    "df_water[\"bulan\"] = to_num(df_water[mcol].astype(\"string\")) if mcol else np.nan\n",
    "\n",
    "bm = df_water[\"baku_mutu\"] if \"baku_mutu\" in df_water.columns else pd.Series(np.nan, index=df_water.index)\n",
    "hp = df_water[\"hasil_pengukuran\"] if \"hasil_pengukuran\" in df_water.columns else pd.Series(np.nan, index=df_water.index)\n",
    "exceed = (hp / bm).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df_water_m = (\n",
    "    pd.DataFrame({\"tahun\": df_water[\"tahun\"], \"bulan\": df_water[\"bulan\"], \"exceed_ratio\": exceed})\n",
    "      .dropna(subset=[\"tahun\",\"bulan\"])\n",
    "      .query(\"tahun >= 2009 and tahun <= 2025 and bulan >= 1 and bulan <= 12\")\n",
    "      .groupby([\"tahun\",\"bulan\"], as_index=False)\n",
    "      .agg(\n",
    "          water_exceed_mean=(\"exceed_ratio\",\"mean\"),\n",
    "          water_exceed_rate=(\"exceed_ratio\", lambda s: float((s > 1).mean())),\n",
    "          water_n=(\"exceed_ratio\",\"count\")\n",
    "      )\n",
    ")\n",
    "df_water_m[\"tanggal_bulan\"] = pd.to_datetime(df_water_m[\"tahun\"].astype(int).astype(str) + \"-\" +\n",
    "                                             df_water_m[\"bulan\"].astype(int).astype(str).str.zfill(2) + \"-01\")\n",
    "df_water_m = df_water_m.loc[df_water_m[\"tanggal_bulan\"] <= HIST_END].reset_index(drop=True)\n",
    "\n",
    "# ---- Population (aggregate yearly total; forecasting-safe by definition) ----\n",
    "df_pop = read_csv_robust(POP_PATH)\n",
    "df_pop.columns = [norm_colname(c) for c in df_pop.columns]\n",
    "ycol_p = pick_first_col(df_pop, [\"tahun\",\"periode_data\"])\n",
    "df_pop[\"tahun\"] = to_num(df_pop[ycol_p].astype(\"string\")) if ycol_p else np.nan\n",
    "df_pop[\"jumlah_penduduk\"] = to_num(df_pop[\"jumlah_penduduk\"].astype(\"string\")) if \"jumlah_penduduk\" in df_pop.columns else np.nan\n",
    "\n",
    "df_pop_y = (\n",
    "    df_pop.dropna(subset=[\"tahun\"])\n",
    "          .groupby(\"tahun\", as_index=False)\n",
    "          .agg(pop_total=(\"jumlah_penduduk\",\"sum\"))\n",
    "          .sort_values(\"tahun\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "df_pop_y[\"pop_yoy\"] = df_pop_y[\"pop_total\"].pct_change()\n",
    "\n",
    "# ============================================================\n",
    "# 2.7 QA prints (ringkas, penting)\n",
    "# ============================================================\n",
    "print(\"=== STAGE 2 SUMMARY (REV2) ===\")\n",
    "print(\"Submission rows:\", len(df_sub))\n",
    "print(\"Target date range:\", df_sub[\"tanggal_target\"].min().date(), \"->\", df_sub[\"tanggal_target\"].max().date())\n",
    "print(\"Stations in submission:\", sorted(df_sub[\"stasiun_code\"].unique().tolist()))\n",
    "\n",
    "print(\"\\nISPU hist max date:\", df_hist[\"tanggal\"].max().date(), \"| HIST_END:\", HIST_END.date())\n",
    "print(\"Last observed date per station:\")\n",
    "print(last_obs_by_station)\n",
    "\n",
    "print(\"\\nHorizon_days range (MUST 1..91):\", int(df_targets[\"horizon_days\"].min()), \"->\", int(df_targets[\"horizon_days\"].max()))\n",
    "\n",
    "print(\"\\nMaster table (df_master):\", df_master.shape, \"| missing label_3:\", int(df_master[\"label_3\"].isna().sum()))\n",
    "print(\"Aux shapes:\",\n",
    "      \"NDVI daily\", df_ndvi.shape,\n",
    "      \"| NDVI monthly\", df_ndvi_m.shape,\n",
    "      \"| Weather\", df_weather.shape,\n",
    "      \"| Water monthly\", df_water_m.shape,\n",
    "      \"| Pop yearly\", df_pop_y.shape)\n",
    "\n",
    "print(\"\\ndf_targets preview:\")\n",
    "print(df_targets[[\"id\",\"tanggal_target\",\"stasiun_code\",\"last_obs_date\",\"horizon_days\",\"is_weekend\",\"is_holiday_nasional\",\"holiday_or_weekend\"]].head(10))\n",
    "\n",
    "print(\"\\n[OK] Stage 2 completed: df_sub, df_calendar, df_targets, df_master, df_ndvi/df_ndvi_m, df_weather, df_water_m, df_pop_y ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39291bd4",
   "metadata": {
    "papermill": {
     "duration": 0.00481,
     "end_time": "2026-02-05T02:18:52.398719",
     "exception": false,
     "start_time": "2026-02-05T02:18:52.393909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering (Time-Series + Calendar + Robustness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fac0e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:18:52.412477Z",
     "iopub.status.busy": "2026-02-05T02:18:52.411955Z",
     "iopub.status.idle": "2026-02-05T02:19:24.248712Z",
     "shell.execute_reply": "2026-02-05T02:19:24.247558Z"
    },
    "papermill": {
     "duration": 31.851899,
     "end_time": "2026-02-05T02:19:24.255909",
     "exception": false,
     "start_time": "2026-02-05T02:18:52.404010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 3 SUMMARY (REV TOP v3.3) ===\n",
      "HIST_END: 2025-08-31\n",
      "df_ts_feat: (16183, 225) | POLL_COLS: ['max', 'pm10', 'pm25', 'so2', 'co', 'o3', 'no2'] | WX_COLS: 8 | use_ndvi: True\n",
      "df_targets_feat: (455, 293) | features: 279 | cat: 1 | num: 278\n",
      "\n",
      "df_targets_feat preview:\n",
      "                id tanggal_target stasiun_code  horizon_days  month  \\\n",
      "0  2025-09-01_DKI1     2025-09-01         DKI1             1      9   \n",
      "1  2025-09-01_DKI2     2025-09-01         DKI2             1      9   \n",
      "2  2025-09-01_DKI3     2025-09-01         DKI3             1      9   \n",
      "3  2025-09-01_DKI4     2025-09-01         DKI4             1      9   \n",
      "4  2025-09-01_DKI5     2025-09-01         DKI5             1      9   \n",
      "5  2025-09-02_DKI1     2025-09-02         DKI1             2      9   \n",
      "6  2025-09-02_DKI2     2025-09-02         DKI2             2      9   \n",
      "7  2025-09-02_DKI3     2025-09-02         DKI3             2      9   \n",
      "8  2025-09-02_DKI4     2025-09-02         DKI4             2      9   \n",
      "9  2025-09-02_DKI5     2025-09-02         DKI5             2      9   \n",
      "\n",
      "   weekofyear  p_tidak_mon  p_tidak_wk  \n",
      "0          36     0.037736    0.039216  \n",
      "1          36     0.213115    0.170732  \n",
      "2          36     0.203209    0.166667  \n",
      "3          36     0.380734    0.403846  \n",
      "4          36     0.169399    0.162791  \n",
      "5          36     0.037736    0.039216  \n",
      "6          36     0.213115    0.170732  \n",
      "7          36     0.203209    0.166667  \n",
      "8          36     0.380734    0.403846  \n",
      "9          36     0.169399    0.162791  \n",
      "\n",
      "df_train_sup: (586647, 290)\n",
      "Train target date range: 2020-01-01 -> 2025-08-31\n",
      "Horizon range: 1 -> 91\n",
      "Label distribution:\n",
      "label_3\n",
      "SEDANG         443099\n",
      "TIDAK SEHAT     82250\n",
      "BAIK            61298\n",
      "Name: count, dtype: Int64\n",
      "Has target_date alias: True\n",
      "\n",
      "[OK] Stage 3 completed: df_ts_feat, df_targets_feat, df_train_sup, FEATURE_COLS_MODEL ready.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 3 — Feature Engineering + Supervised Forecasting Table\n",
    "# REV TOP v3.3 (FIX: df_targets_feat month missing)\n",
    "#\n",
    "# Fix utama:\n",
    "# - Selalu bikin year/month/day/dow/dayofyear/weekofyear dari tanggal_target\n",
    "#   (jangan tergantung df_calendar punya month atau tidak)\n",
    "# - merge_asof NDVI: per-station (no global sort pitfalls)\n",
    "#\n",
    "# Output:\n",
    "#   df_ts_feat, df_targets_feat, df_train_sup\n",
    "#   FEATURE_COLS_MODEL, CAT_FEATURES, NUM_FEATURES\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- guards ----------\n",
    "need = [\"df_calendar\",\"df_targets\",\"df_ispu_hist\",\"df_weather\",\"df_ndvi\",\"VALID_STATIONS\",\"HIST_END\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from previous stages: {miss}. Missing={miss}\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "MAX_H = 91\n",
    "TRAIN_START = pd.Timestamp(\"2020-01-01\")\n",
    "BUILD_TRAIN_TABLE = True\n",
    "\n",
    "WINS_POLL = [7, 14, 30]\n",
    "WINS_WX   = [7, 14, 30]\n",
    "MINP_POLL = {7: 3, 14: 5, 30: 10}\n",
    "MINP_WX   = {7: 3, 14: 5, 30: 10}\n",
    "\n",
    "USE_SAMPLE_WEIGHT = True\n",
    "WEIGHT_LONGH_MAX = 2.0\n",
    "WEIGHT_SEP_NOV = 1.3\n",
    "\n",
    "LABELS = [\"BAIK\",\"SEDANG\",\"TIDAK SEHAT\"]\n",
    "MAP_CODE = {\"BAIK\": 0, \"SEDANG\": 1, \"TIDAK SEHAT\": 2}\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _dedup_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "def _ensure_station(df: pd.DataFrame, col=\"stasiun_code\") -> pd.DataFrame:\n",
    "    df = _dedup_cols(df)\n",
    "    if col not in df.columns:\n",
    "        raise RuntimeError(f\"Missing '{col}' in df cols={df.columns.tolist()[:50]}\")\n",
    "    df[col] = df[col].astype(\"string\").str.upper().str.strip()\n",
    "    return df\n",
    "\n",
    "def _safe_num(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _grp_roll(series_shifted: pd.Series, by: pd.Series, w: int, func: str, minp: int) -> pd.Series:\n",
    "    r = series_shifted.groupby(by).rolling(w, min_periods=minp)\n",
    "    out = getattr(r, func)()\n",
    "    return out.reset_index(level=0, drop=True)\n",
    "\n",
    "def _merge_safe(left: pd.DataFrame, right: pd.DataFrame, on, how=\"left\", validate=None) -> pd.DataFrame:\n",
    "    left = _dedup_cols(left)\n",
    "    right = _dedup_cols(right)\n",
    "    out = left.merge(right, on=on, how=how, validate=validate)\n",
    "    return _dedup_cols(out)\n",
    "\n",
    "def _add_dateparts(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Force year/month/day/dow/dayofyear/weekofyear + cyclic features from a date column.\"\"\"\n",
    "    d = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    df[\"year\"] = d.dt.year.astype(\"Int64\")\n",
    "    df[\"month\"] = d.dt.month.astype(\"Int64\")\n",
    "    df[\"day\"] = d.dt.day.astype(\"Int64\")\n",
    "    df[\"dow\"] = d.dt.dayofweek.astype(\"Int64\")\n",
    "    df[\"dayofyear\"] = d.dt.dayofyear.astype(\"Int64\")\n",
    "    df[\"weekofyear\"] = d.dt.isocalendar().week.astype(int)\n",
    "\n",
    "    # cyclic (float)\n",
    "    df[\"sin_doy\"] = np.sin(2*np.pi*df[\"dayofyear\"].astype(float)/365.25)\n",
    "    df[\"cos_doy\"] = np.cos(2*np.pi*df[\"dayofyear\"].astype(float)/365.25)\n",
    "    df[\"sin_month\"] = np.sin(2*np.pi*df[\"month\"].astype(float)/12.0)\n",
    "    df[\"cos_month\"] = np.cos(2*np.pi*df[\"month\"].astype(float)/12.0)\n",
    "    df[\"sin_dow\"] = np.sin(2*np.pi*df[\"dow\"].astype(float)/7.0)\n",
    "    df[\"cos_dow\"] = np.cos(2*np.pi*df[\"dow\"].astype(float)/7.0)\n",
    "\n",
    "    df[\"is_month_start\"] = d.dt.is_month_start.astype(int)\n",
    "    df[\"is_month_end\"]   = d.dt.is_month_end.astype(int)\n",
    "    return df\n",
    "\n",
    "# ============================================================\n",
    "# 3.0 Build base TS (ISPU hist) + weather\n",
    "# ============================================================\n",
    "dfh = _ensure_station(df_ispu_hist.copy(), \"stasiun_code\")\n",
    "dfh[\"tanggal\"] = pd.to_datetime(dfh[\"tanggal\"], errors=\"coerce\")\n",
    "dfh = dfh.dropna(subset=[\"tanggal\"]).copy()\n",
    "dfh = dfh.loc[dfh[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "dfh = dfh.loc[dfh[\"tanggal\"] <= pd.Timestamp(HIST_END)].copy()\n",
    "\n",
    "LBL_COL = \"label_3\" if \"label_3\" in dfh.columns else None\n",
    "if LBL_COL is None:\n",
    "    raise RuntimeError(\"df_ispu_hist harus punya label_3.\")\n",
    "\n",
    "POLL_COLS = [c for c in [\"max\",\"pm10\",\"pm25\",\"so2\",\"co\",\"o3\",\"no2\"] if c in dfh.columns]\n",
    "if len(POLL_COLS) == 0:\n",
    "    raise RuntimeError(\"POLL_COLS tidak ditemukan di df_ispu_hist.\")\n",
    "\n",
    "for c in POLL_COLS:\n",
    "    dfh[c] = _safe_num(dfh[c])\n",
    "\n",
    "dfw = _ensure_station(df_weather.copy(), \"stasiun_code\")\n",
    "dfw[\"tanggal\"] = pd.to_datetime(dfw[\"tanggal\"], errors=\"coerce\")\n",
    "dfw = dfw.dropna(subset=[\"tanggal\"]).copy()\n",
    "dfw = dfw.loc[dfw[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "dfw = dfw.loc[dfw[\"tanggal\"] <= pd.Timestamp(HIST_END)].copy()\n",
    "\n",
    "wx_candidates = [\n",
    "    \"temperature_2m_mean\",\"temperature_2m_max\",\"temperature_2m_min\",\n",
    "    \"precipitation_sum\",\"precipitation_hours\",\n",
    "    \"wind_speed_10m_mean\",\"wind_speed_10m_max\",\"wind_speed_10m_min\",\n",
    "    \"wind_gusts_10m_mean\",\"wind_gusts_10m_max\",\"wind_gusts_10m_min\",\n",
    "    \"relative_humidity_2m_mean\",\"relative_humidity_2m_max\",\"relative_humidity_2m_min\",\n",
    "    \"cloud_cover_mean\",\"cloud_cover_max\",\"cloud_cover_min\",\n",
    "    \"surface_pressure_mean\",\"surface_pressure_max\",\"surface_pressure_min\",\n",
    "    \"shortwave_radiation_sum\",\n",
    "    \"wind_direction_10m_dominant\",\"winddirection_10m_dominant\"\n",
    "]\n",
    "WX_COLS = [c for c in wx_candidates if c in dfw.columns]\n",
    "for c in WX_COLS:\n",
    "    dfw[c] = _safe_num(dfw[c])\n",
    "\n",
    "df_ts = _merge_safe(\n",
    "    dfh[[\"tanggal\",\"stasiun_code\",LBL_COL] + POLL_COLS],\n",
    "    dfw[[\"tanggal\",\"stasiun_code\"] + WX_COLS],\n",
    "    on=[\"tanggal\",\"stasiun_code\"],\n",
    "    how=\"left\"\n",
    ").sort_values([\"stasiun_code\",\"tanggal\"]).reset_index(drop=True)\n",
    "\n",
    "# ============================================================\n",
    "# 3.1 NDVI merge_asof PER STATION\n",
    "# ============================================================\n",
    "df_ts[\"ndvi\"] = np.nan\n",
    "dfn = df_ndvi.copy()\n",
    "use_ndvi = (len(dfn) > 0) and {\"tanggal\",\"stasiun_code\",\"ndvi\"}.issubset(dfn.columns)\n",
    "\n",
    "if use_ndvi:\n",
    "    dfn = _ensure_station(dfn, \"stasiun_code\")\n",
    "    dfn[\"tanggal\"] = pd.to_datetime(dfn[\"tanggal\"], errors=\"coerce\")\n",
    "    dfn[\"ndvi\"] = _safe_num(dfn[\"ndvi\"])\n",
    "    dfn = dfn.dropna(subset=[\"tanggal\"]).copy()\n",
    "    dfn = dfn.loc[dfn[\"stasiun_code\"].isin(VALID_STATIONS)].copy()\n",
    "    dfn = dfn.loc[dfn[\"tanggal\"] <= pd.Timestamp(HIST_END)].copy()\n",
    "    dfn = (dfn.sort_values([\"stasiun_code\",\"tanggal\"])\n",
    "              .drop_duplicates([\"stasiun_code\",\"tanggal\"], keep=\"last\")\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    ndvi_out = []\n",
    "    for st in VALID_STATIONS:\n",
    "        left = df_ts.loc[df_ts[\"stasiun_code\"] == st, [\"tanggal\"]].copy()\n",
    "        right = dfn.loc[dfn[\"stasiun_code\"] == st, [\"tanggal\",\"ndvi\"]].copy()\n",
    "        left = left.sort_values(\"tanggal\").reset_index()\n",
    "        right = right.sort_values(\"tanggal\").reset_index(drop=True)\n",
    "\n",
    "        if len(right) == 0:\n",
    "            left[\"ndvi\"] = np.nan\n",
    "        else:\n",
    "            tmp = pd.merge_asof(left, right, on=\"tanggal\", direction=\"backward\", allow_exact_matches=True)\n",
    "            left[\"ndvi\"] = tmp[\"ndvi\"].values\n",
    "\n",
    "        ndvi_out.append(left.set_index(\"index\")[\"ndvi\"])\n",
    "    df_ts[\"ndvi\"] = pd.concat(ndvi_out).sort_index().values\n",
    "\n",
    "# ============================================================\n",
    "# 3.2 Rolling/Lag features (PAST-ONLY)\n",
    "# ============================================================\n",
    "df_ts = df_ts.sort_values([\"stasiun_code\",\"tanggal\"]).reset_index(drop=True)\n",
    "by = df_ts[\"stasiun_code\"]\n",
    "g  = df_ts.groupby(\"stasiun_code\", sort=False)\n",
    "\n",
    "df_ts[\"_ycode\"] = df_ts[LBL_COL].astype(\"string\").map(MAP_CODE).astype(\"float\")\n",
    "\n",
    "feat_blocks = {}\n",
    "\n",
    "# pollutants\n",
    "for c in POLL_COLS:\n",
    "    s1 = g[c].shift(1)\n",
    "    feat_blocks[f\"{c}_lag1\"] = s1\n",
    "    feat_blocks[f\"{c}_lag7\"] = g[c].shift(7)\n",
    "    for w in WINS_POLL:\n",
    "        minp = MINP_POLL.get(w, 1)\n",
    "        feat_blocks[f\"{c}_rmean{w}\"] = _grp_roll(s1, by, w, \"mean\", minp)\n",
    "        feat_blocks[f\"{c}_rstd{w}\"]  = _grp_roll(s1, by, w, \"std\",  minp)\n",
    "        feat_blocks[f\"{c}_rmin{w}\"]  = _grp_roll(s1, by, w, \"min\",  minp)\n",
    "        feat_blocks[f\"{c}_rmax{w}\"]  = _grp_roll(s1, by, w, \"max\",  minp)\n",
    "\n",
    "    feat_blocks[f\"{c}_mom_7_30\"]  = feat_blocks[f\"{c}_rmean7\"]  - feat_blocks[f\"{c}_rmean30\"]\n",
    "    feat_blocks[f\"{c}_mom_14_30\"] = feat_blocks[f\"{c}_rmean14\"] - feat_blocks[f\"{c}_rmean30\"]\n",
    "\n",
    "    na_flag = s1.isna().astype(float)\n",
    "    feat_blocks[f\"{c}_na_rate30\"] = _grp_roll(na_flag, by, 30, \"mean\", 10)\n",
    "\n",
    "# weather rolling\n",
    "for c in WX_COLS:\n",
    "    s1 = g[c].shift(1)\n",
    "    feat_blocks[f\"{c}_lag1\"] = s1\n",
    "    feat_blocks[f\"{c}_lag7\"] = g[c].shift(7)\n",
    "    for w in WINS_WX:\n",
    "        minp = MINP_WX.get(w, 1)\n",
    "        feat_blocks[f\"{c}_rmean{w}\"] = _grp_roll(s1, by, w, \"mean\", minp)\n",
    "        feat_blocks[f\"{c}_rstd{w}\"]  = _grp_roll(s1, by, w, \"std\",  minp)\n",
    "\n",
    "# ndvi rolling\n",
    "df_ts[\"ndvi\"] = _safe_num(df_ts[\"ndvi\"])\n",
    "nd1 = g[\"ndvi\"].shift(1)\n",
    "feat_blocks[\"ndvi_lag1\"] = nd1\n",
    "feat_blocks[\"ndvi_rmean30\"] = _grp_roll(nd1, by, 30, \"mean\", 10)\n",
    "\n",
    "df_ts_feat = pd.concat([df_ts, pd.DataFrame(feat_blocks)], axis=1)\n",
    "df_ts_feat = _dedup_cols(df_ts_feat)\n",
    "\n",
    "# label-derived (past-only)\n",
    "y1 = g[\"_ycode\"].shift(1)\n",
    "y2 = g[\"_ycode\"].shift(2)\n",
    "df_ts_feat[\"y_lag1\"] = y1\n",
    "df_ts_feat[\"y_lag7\"] = g[\"_ycode\"].shift(7)\n",
    "\n",
    "y_change = (y1 != y2).astype(float)\n",
    "y_change = y_change.where(y1.notna() & y2.notna(), np.nan)\n",
    "df_ts_feat[\"y_change_lag1\"] = y_change\n",
    "\n",
    "for w in [7, 14, 30]:\n",
    "    minp = MINP_POLL.get(w, 1)\n",
    "    df_ts_feat[f\"p_baik_{w}\"]   = _grp_roll((y1 == 0).astype(float), by, w, \"mean\", minp)\n",
    "    df_ts_feat[f\"p_sedang_{w}\"] = _grp_roll((y1 == 1).astype(float), by, w, \"mean\", minp)\n",
    "    df_ts_feat[f\"p_tidak_{w}\"]  = _grp_roll((y1 == 2).astype(float), by, w, \"mean\", minp)\n",
    "    df_ts_feat[f\"n_trans_{w}\"]  = _grp_roll(y_change.fillna(0.0), by, w, \"sum\",  minp)\n",
    "\n",
    "# FIX NA->int (aman)\n",
    "lbl_s = df_ts_feat[LBL_COL].astype(\"string\")\n",
    "is_tidak = lbl_s.eq(\"TIDAK SEHAT\").fillna(False).astype(np.int8)\n",
    "\n",
    "last_tidak_date = df_ts_feat[\"tanggal\"].where(is_tidak == 1, pd.NaT)\n",
    "last_tidak_date = last_tidak_date.groupby(by).ffill().shift(1)\n",
    "df_ts_feat[\"days_since_tidak\"] = (df_ts_feat[\"tanggal\"] - last_tidak_date).dt.days.astype(\"float\")\n",
    "\n",
    "lag_lbl  = g[LBL_COL].shift(1)\n",
    "lag_lbl2 = g[LBL_COL].shift(2)\n",
    "streak_break = (lag_lbl != lag_lbl2) | lag_lbl.isna()\n",
    "streak_id = streak_break.groupby(by).cumsum()\n",
    "df_ts_feat[\"streak_len_lag1\"] = df_ts_feat.groupby([\"stasiun_code\", streak_id]).cumcount() + 1\n",
    "df_ts_feat[\"streak_len_lag1\"] = df_ts_feat[\"streak_len_lag1\"].where(lag_lbl.notna(), np.nan)\n",
    "\n",
    "# add time keys (for climatology)\n",
    "df_ts_feat[\"month\"] = df_ts_feat[\"tanggal\"].dt.month\n",
    "df_ts_feat[\"weekofyear\"] = df_ts_feat[\"tanggal\"].dt.isocalendar().week.astype(int)\n",
    "df_ts_feat[\"dow\"] = df_ts_feat[\"tanggal\"].dt.dayofweek\n",
    "\n",
    "# ============================================================\n",
    "# 3.3 Climatology + label priors\n",
    "# ============================================================\n",
    "df_pol_mon = (df_ts_feat.groupby([\"stasiun_code\",\"month\"], as_index=False)[POLL_COLS]\n",
    "                        .mean()\n",
    "                        .rename(columns={c: f\"{c}_mon_mean\" for c in POLL_COLS}))\n",
    "df_pol_wk  = (df_ts_feat.groupby([\"stasiun_code\",\"weekofyear\"], as_index=False)[POLL_COLS]\n",
    "                        .mean()\n",
    "                        .rename(columns={c: f\"{c}_wk_mean\" for c in POLL_COLS}))\n",
    "\n",
    "df_wx_mon = (df_ts_feat.groupby([\"stasiun_code\",\"month\"], as_index=False)[WX_COLS]\n",
    "                      .mean()\n",
    "                      .rename(columns={c: f\"{c}_mon_mean\" for c in WX_COLS})) if len(WX_COLS) else pd.DataFrame(columns=[\"stasiun_code\",\"month\"])\n",
    "df_wx_wk  = (df_ts_feat.groupby([\"stasiun_code\",\"weekofyear\"], as_index=False)[WX_COLS]\n",
    "                      .mean()\n",
    "                      .rename(columns={c: f\"{c}_wk_mean\" for c in WX_COLS})) if len(WX_COLS) else pd.DataFrame(columns=[\"stasiun_code\",\"weekofyear\"])\n",
    "\n",
    "df_ndvi_mon = (df_ts_feat.groupby([\"stasiun_code\",\"month\"], as_index=False)[\"ndvi\"]\n",
    "                        .mean()\n",
    "                        .rename(columns={\"ndvi\":\"ndvi_mon_mean\"}))\n",
    "\n",
    "# label priors from history\n",
    "dfh_lbl = dfh.loc[dfh[LBL_COL].notna(), [\"tanggal\",\"stasiun_code\",LBL_COL]].copy()\n",
    "dfh_lbl[\"month\"] = dfh_lbl[\"tanggal\"].dt.month\n",
    "dfh_lbl[\"weekofyear\"] = dfh_lbl[\"tanggal\"].dt.isocalendar().week.astype(int)\n",
    "\n",
    "tmp_m = pd.crosstab([dfh_lbl[\"stasiun_code\"], dfh_lbl[\"month\"]], dfh_lbl[LBL_COL], normalize=\"index\").reset_index()\n",
    "tmp_w = pd.crosstab([dfh_lbl[\"stasiun_code\"], dfh_lbl[\"weekofyear\"]], dfh_lbl[LBL_COL], normalize=\"index\").reset_index()\n",
    "for k in LABELS:\n",
    "    if k not in tmp_m.columns: tmp_m[k] = 0.0\n",
    "    if k not in tmp_w.columns: tmp_w[k] = 0.0\n",
    "\n",
    "df_lbl_mon = tmp_m.rename(columns={\"BAIK\":\"p_baik_mon\",\"SEDANG\":\"p_sedang_mon\",\"TIDAK SEHAT\":\"p_tidak_mon\"})\n",
    "df_lbl_wk  = tmp_w.rename(columns={\"BAIK\":\"p_baik_wk\",\"SEDANG\":\"p_sedang_wk\",\"TIDAK SEHAT\":\"p_tidak_wk\"})\n",
    "df_lbl_mon = _ensure_station(df_lbl_mon, \"stasiun_code\")\n",
    "df_lbl_wk  = _ensure_station(df_lbl_wk, \"stasiun_code\")\n",
    "\n",
    "# ============================================================\n",
    "# 3.4 df_targets_feat (submission) — FORCE month exists (FIX KEYERROR)\n",
    "# ============================================================\n",
    "df_targets_feat = _ensure_station(df_targets.copy(), \"stasiun_code\")\n",
    "df_targets_feat[\"tanggal_target\"] = pd.to_datetime(df_targets_feat[\"tanggal_target\"], errors=\"coerce\")\n",
    "df_targets_feat[\"anchor_date\"] = pd.to_datetime(df_targets_feat[\"last_obs_date\"], errors=\"coerce\")\n",
    "\n",
    "# FORCE dateparts from tanggal_target (ini FIX month hilang)\n",
    "df_targets_feat = _add_dateparts(df_targets_feat, \"tanggal_target\")\n",
    "\n",
    "# attach holiday info from df_calendar if available (optional)\n",
    "cal = _dedup_cols(df_calendar.copy())\n",
    "cal[\"tanggal\"] = pd.to_datetime(cal[\"tanggal\"], errors=\"coerce\")\n",
    "cal = cal.dropna(subset=[\"tanggal\"]).copy()\n",
    "\n",
    "if \"is_weekend\" not in cal.columns:\n",
    "    cal[\"is_weekend\"] = (cal[\"tanggal\"].dt.dayofweek >= 5).astype(int)\n",
    "if \"is_holiday_nasional\" not in cal.columns:\n",
    "    cal[\"is_holiday_nasional\"] = 0\n",
    "if \"holiday_or_weekend\" not in cal.columns:\n",
    "    cal[\"holiday_or_weekend\"] = ((cal[\"is_weekend\"].astype(int) == 1) | (cal[\"is_holiday_nasional\"].astype(int) == 1)).astype(int)\n",
    "\n",
    "cal_keep = [c for c in [\"tanggal\",\"is_weekend\",\"is_holiday_nasional\",\"holiday_or_weekend\",\"day_name\",\"nama_libur\"] if c in cal.columns]\n",
    "df_targets_feat = df_targets_feat.merge(cal[cal_keep], left_on=\"tanggal_target\", right_on=\"tanggal\", how=\"left\")\n",
    "df_targets_feat = df_targets_feat.drop(columns=[\"tanggal\"], errors=\"ignore\")\n",
    "\n",
    "# horizon transforms\n",
    "df_targets_feat[\"horizon_weeks\"]  = df_targets_feat[\"horizon_days\"] / 7.0\n",
    "df_targets_feat[\"horizon_months\"] = df_targets_feat[\"horizon_days\"] / 30.0\n",
    "df_targets_feat[\"log1p_horizon\"]  = np.log1p(df_targets_feat[\"horizon_days\"].clip(lower=0))\n",
    "\n",
    "# anchor snapshot at HIST_END (prefix a_)\n",
    "drop_raw = {\"label_3\",\"_ycode\"}\n",
    "anchor_cols_raw = [c for c in df_ts_feat.columns if c not in drop_raw and c not in [\"tanggal\",\"stasiun_code\",\"month\",\"weekofyear\",\"dow\"]]\n",
    "df_anchor = df_ts_feat.loc[df_ts_feat[\"tanggal\"] == pd.Timestamp(HIST_END), [\"stasiun_code\"] + anchor_cols_raw].copy()\n",
    "df_anchor = _ensure_station(df_anchor, \"stasiun_code\")\n",
    "df_anchor = df_anchor.rename(columns={c: f\"a_{c}\" for c in anchor_cols_raw})\n",
    "df_targets_feat = _merge_safe(df_targets_feat, df_anchor, on=[\"stasiun_code\"], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "# climatology + priors (sekarang month sudah pasti ada -> tidak KeyError)\n",
    "df_targets_feat = _merge_safe(df_targets_feat, df_pol_mon, on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "df_targets_feat = _merge_safe(df_targets_feat, df_pol_wk,  on=[\"stasiun_code\",\"weekofyear\"], how=\"left\", validate=\"m:1\")\n",
    "if len(WX_COLS):\n",
    "    df_targets_feat = _merge_safe(df_targets_feat, df_wx_mon, on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "    df_targets_feat = _merge_safe(df_targets_feat, df_wx_wk,  on=[\"stasiun_code\",\"weekofyear\"], how=\"left\", validate=\"m:1\")\n",
    "df_targets_feat = _merge_safe(df_targets_feat, df_ndvi_mon, on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "df_targets_feat = _merge_safe(df_targets_feat, df_lbl_mon,  on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "df_targets_feat = _merge_safe(df_targets_feat, df_lbl_wk,   on=[\"stasiun_code\",\"weekofyear\"], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "for c in [\"p_baik_mon\",\"p_sedang_mon\",\"p_tidak_mon\",\"p_baik_wk\",\"p_sedang_wk\",\"p_tidak_wk\"]:\n",
    "    if c in df_targets_feat.columns:\n",
    "        df_targets_feat[c] = df_targets_feat[c].fillna(0.0)\n",
    "\n",
    "# anomalies\n",
    "for c in [\"max\",\"pm25\",\"pm10\",\"o3\",\"no2\"]:\n",
    "    a_col = f\"a_{c}_rmean7\"\n",
    "    m_col = f\"{c}_mon_mean\"\n",
    "    if a_col in df_targets_feat.columns and m_col in df_targets_feat.columns:\n",
    "        df_targets_feat[f\"{c}_anom_anchor7_vs_mon\"] = df_targets_feat[a_col] - df_targets_feat[m_col]\n",
    "\n",
    "# ============================================================\n",
    "# 3.5 df_train_sup (vectorized)\n",
    "# ============================================================\n",
    "df_train_sup = None\n",
    "if BUILD_TRAIN_TABLE:\n",
    "    y = dfh.loc[dfh[LBL_COL].isin(LABELS), [\"tanggal\",\"stasiun_code\",LBL_COL]].copy()\n",
    "    y = y.rename(columns={\"tanggal\":\"tanggal_target\"}).copy()\n",
    "    y[\"tanggal_target\"] = pd.to_datetime(y[\"tanggal_target\"])\n",
    "    y = y.loc[y[\"tanggal_target\"] >= TRAIN_START].copy()\n",
    "\n",
    "    H = np.arange(1, MAX_H + 1, dtype=np.int16)\n",
    "    n0 = len(y)\n",
    "    rep = np.repeat(np.arange(n0), len(H))\n",
    "    hvec = np.tile(H, n0)\n",
    "\n",
    "    df_sup = y.iloc[rep].reset_index(drop=True)\n",
    "    df_sup[\"horizon_days\"] = hvec.astype(np.int16)\n",
    "    df_sup[\"anchor_date\"] = df_sup[\"tanggal_target\"] - pd.to_timedelta(df_sup[\"horizon_days\"].astype(int), unit=\"D\")\n",
    "\n",
    "    min_anchor = df_ts_feat[\"tanggal\"].min()\n",
    "    max_anchor = df_ts_feat[\"tanggal\"].max()\n",
    "    df_sup = df_sup.loc[(df_sup[\"anchor_date\"] >= min_anchor) & (df_sup[\"anchor_date\"] <= max_anchor)].copy()\n",
    "    df_sup = df_sup.reset_index(drop=True)\n",
    "\n",
    "    df_anchor_all = df_ts_feat[[\"tanggal\",\"stasiun_code\"] + anchor_cols_raw].copy()\n",
    "    df_anchor_all = df_anchor_all.rename(columns={\"tanggal\":\"anchor_date\"})\n",
    "    df_anchor_all = df_anchor_all.rename(columns={c: f\"a_{c}\" for c in anchor_cols_raw})\n",
    "    df_anchor_all = _ensure_station(df_anchor_all, \"stasiun_code\")\n",
    "    df_anchor_all[\"anchor_date\"] = pd.to_datetime(df_anchor_all[\"anchor_date\"])\n",
    "\n",
    "    df_sup = _merge_safe(df_sup, df_anchor_all, on=[\"anchor_date\",\"stasiun_code\"], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "    core = []\n",
    "    if f\"a_{POLL_COLS[0]}_lag1\" in df_sup.columns: core.append(f\"a_{POLL_COLS[0]}_lag1\")\n",
    "    if f\"a_{POLL_COLS[0]}_rmean30\" in df_sup.columns: core.append(f\"a_{POLL_COLS[0]}_rmean30\")\n",
    "    if len(core):\n",
    "        df_sup = df_sup.dropna(subset=core).copy()\n",
    "\n",
    "    # FORCE dateparts from tanggal_target (buat month pasti ada)\n",
    "    df_sup = _add_dateparts(df_sup, \"tanggal_target\")\n",
    "\n",
    "    # merge optional calendar flags\n",
    "    df_sup = df_sup.merge(cal[cal_keep].rename(columns={\"tanggal\":\"tanggal_target\"}), on=\"tanggal_target\", how=\"left\", validate=\"m:1\")\n",
    "\n",
    "    df_sup[\"horizon_weeks\"]  = df_sup[\"horizon_days\"] / 7.0\n",
    "    df_sup[\"horizon_months\"] = df_sup[\"horizon_days\"] / 30.0\n",
    "    df_sup[\"log1p_horizon\"]  = np.log1p(df_sup[\"horizon_days\"].astype(float))\n",
    "\n",
    "    df_sup = _merge_safe(df_sup, df_pol_mon, on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "    df_sup = _merge_safe(df_sup, df_pol_wk,  on=[\"stasiun_code\",\"weekofyear\"], how=\"left\", validate=\"m:1\")\n",
    "    if len(WX_COLS):\n",
    "        df_sup = _merge_safe(df_sup, df_wx_mon, on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "        df_sup = _merge_safe(df_sup, df_wx_wk,  on=[\"stasiun_code\",\"weekofyear\"], how=\"left\", validate=\"m:1\")\n",
    "    df_sup = _merge_safe(df_sup, df_ndvi_mon, on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "    df_sup = _merge_safe(df_sup, df_lbl_mon,  on=[\"stasiun_code\",\"month\"], how=\"left\", validate=\"m:1\")\n",
    "    df_sup = _merge_safe(df_sup, df_lbl_wk,   on=[\"stasiun_code\",\"weekofyear\"], how=\"left\", validate=\"m:1\")\n",
    "\n",
    "    for c in [\"p_baik_mon\",\"p_sedang_mon\",\"p_tidak_mon\",\"p_baik_wk\",\"p_sedang_wk\",\"p_tidak_wk\"]:\n",
    "        if c in df_sup.columns:\n",
    "            df_sup[c] = df_sup[c].fillna(0.0)\n",
    "\n",
    "    for c in [\"max\",\"pm25\",\"pm10\",\"o3\",\"no2\"]:\n",
    "        a_col = f\"a_{c}_rmean7\"\n",
    "        m_col = f\"{c}_mon_mean\"\n",
    "        if a_col in df_sup.columns and m_col in df_sup.columns:\n",
    "            df_sup[f\"{c}_anom_anchor7_vs_mon\"] = df_sup[a_col] - df_sup[m_col]\n",
    "\n",
    "    df_sup[\"y\"] = df_sup[LBL_COL].map(MAP_CODE).astype(int)\n",
    "    df_sup[\"target_date\"] = df_sup[\"tanggal_target\"]  # alias utk Stage 4\n",
    "\n",
    "    if USE_SAMPLE_WEIGHT:\n",
    "        h = df_sup[\"horizon_days\"].astype(float)\n",
    "        w_h = 1.0 + (WEIGHT_LONGH_MAX - 1.0) * (h - 1.0) / max(1.0, (MAX_H - 1.0))\n",
    "        m = df_sup[\"month\"].astype(int)\n",
    "        w_m = np.where(m.isin([9,10,11]), WEIGHT_SEP_NOV, 1.0)\n",
    "        df_sup[\"sample_weight\"] = (w_h * w_m).astype(\"float32\")\n",
    "    else:\n",
    "        df_sup[\"sample_weight\"] = 1.0\n",
    "\n",
    "    df_train_sup = _dedup_cols(df_sup).copy()\n",
    "\n",
    "# ============================================================\n",
    "# 3.6 Feature list\n",
    "# ============================================================\n",
    "CAT_FEATURES = [c for c in [\"stasiun_code\",\"day_name\",\"nama_libur\"] if c in df_targets_feat.columns]\n",
    "\n",
    "BASE_NUM = [c for c in [\n",
    "    \"year\",\"month\",\"day\",\"dow\",\"dayofyear\",\"weekofyear\",\n",
    "    \"sin_doy\",\"cos_doy\",\"sin_month\",\"cos_month\",\"sin_dow\",\"cos_dow\",\n",
    "    \"is_weekend\",\"is_holiday_nasional\",\"holiday_or_weekend\",\n",
    "    \"is_month_start\",\"is_month_end\",\n",
    "    \"horizon_days\",\"horizon_weeks\",\"horizon_months\",\"log1p_horizon\"\n",
    "] if c in df_targets_feat.columns]\n",
    "\n",
    "ANCHOR_NUM = [c for c in df_targets_feat.columns if c.startswith(\"a_\")]\n",
    "PRIOR_NUM  = [c for c in [\"p_baik_mon\",\"p_sedang_mon\",\"p_tidak_mon\",\"p_baik_wk\",\"p_sedang_wk\",\"p_tidak_wk\",\"ndvi_mon_mean\"] if c in df_targets_feat.columns]\n",
    "CLIM_NUM   = [c for c in df_targets_feat.columns if c.endswith(\"_mon_mean\") or c.endswith(\"_wk_mean\")]\n",
    "ANOM_NUM   = [c for c in df_targets_feat.columns if c.endswith(\"_anom_anchor7_vs_mon\")]\n",
    "\n",
    "def _dedup_list(xs):\n",
    "    out, seen = [], set()\n",
    "    for x in xs:\n",
    "        if x not in seen:\n",
    "            out.append(x); seen.add(x)\n",
    "    return out\n",
    "\n",
    "NUM_FEATURES = _dedup_list(BASE_NUM + PRIOR_NUM + CLIM_NUM + ANOM_NUM + ANCHOR_NUM)\n",
    "CAT_FEATURES = _dedup_list(CAT_FEATURES)\n",
    "FEATURE_COLS_MODEL = CAT_FEATURES + NUM_FEATURES\n",
    "\n",
    "# ============================================================\n",
    "# 3.7 QA\n",
    "# ============================================================\n",
    "print(\"=== STAGE 3 SUMMARY (REV TOP v3.3) ===\")\n",
    "print(\"HIST_END:\", pd.Timestamp(HIST_END).date())\n",
    "print(\"df_ts_feat:\", df_ts_feat.shape, \"| POLL_COLS:\", POLL_COLS, \"| WX_COLS:\", len(WX_COLS), \"| use_ndvi:\", bool(use_ndvi))\n",
    "print(\"df_targets_feat:\", df_targets_feat.shape, \"| features:\", len(FEATURE_COLS_MODEL), \"| cat:\", len(CAT_FEATURES), \"| num:\", len(NUM_FEATURES))\n",
    "\n",
    "# hard check month existence (fix for your KeyError)\n",
    "if \"month\" not in df_targets_feat.columns:\n",
    "    raise RuntimeError(\"BUG: month still missing in df_targets_feat. Check tanggal_target parsing.\")\n",
    "if \"month\" not in df_pol_mon.columns:\n",
    "    raise RuntimeError(\"BUG: month missing in df_pol_mon. Check df_ts_feat month creation.\")\n",
    "\n",
    "show_cols = [\"id\",\"tanggal_target\",\"stasiun_code\",\"horizon_days\",\"month\",\"weekofyear\"]\n",
    "show_cols += [c for c in [\"is_weekend\",\"is_holiday_nasional\",\"p_tidak_mon\",\"p_tidak_wk\"] if c in df_targets_feat.columns]\n",
    "show_cols = [c for c in show_cols if c in df_targets_feat.columns]\n",
    "print(\"\\ndf_targets_feat preview:\")\n",
    "print(df_targets_feat[show_cols].head(10))\n",
    "\n",
    "if BUILD_TRAIN_TABLE:\n",
    "    print(\"\\ndf_train_sup:\", df_train_sup.shape)\n",
    "    print(\"Train target date range:\", df_train_sup[\"tanggal_target\"].min().date(), \"->\", df_train_sup[\"tanggal_target\"].max().date())\n",
    "    print(\"Horizon range:\", int(df_train_sup[\"horizon_days\"].min()), \"->\", int(df_train_sup[\"horizon_days\"].max()))\n",
    "    print(\"Label distribution:\")\n",
    "    print(df_train_sup[LBL_COL].value_counts())\n",
    "    print(\"Has target_date alias:\", \"target_date\" in df_train_sup.columns)\n",
    "\n",
    "print(\"\\n[OK] Stage 3 completed: df_ts_feat, df_targets_feat, df_train_sup, FEATURE_COLS_MODEL ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78038297",
   "metadata": {
    "papermill": {
     "duration": 0.00452,
     "end_time": "2026-02-05T02:19:24.265177",
     "exception": false,
     "start_time": "2026-02-05T02:19:24.260657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "233d34bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T02:19:24.276640Z",
     "iopub.status.busy": "2026-02-05T02:19:24.276119Z",
     "iopub.status.idle": "2026-02-05T14:01:00.482353Z",
     "shell.execute_reply": "2026-02-05T14:01:00.479337Z"
    },
    "papermill": {
     "duration": 42096.216459,
     "end_time": "2026-02-05T14:01:00.486119",
     "exception": false,
     "start_time": "2026-02-05T02:19:24.269660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/938672737.py:149: RuntimeWarning: invalid value encountered in power\n",
      "  w_h = np.where(h < 14, 0.85, np.where(h < 30, 1.00, 1.00 + 0.55*((h-30)/(91-30))**1.25)).astype(\"float64\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 4 SETUP (REV TOP v6.2) ===\n",
      "Detected date_col: target_date | label_col: label_3 | horizon_col: horizon_days\n",
      "Rows: 586647 | Years: [2020, 2021, 2022, 2023, 2024, 2025]\n",
      "Val folds: ['val_year_2023', 'val_year_2024', 'val_year_2025']\n",
      "Features: 279 | cat: 1 | num: 278\n",
      "Label counts: {1: 443099, 2: 82250, 0: 61298}\n",
      "\n",
      "--- Fold val_year_2023: train=180852 valid=142537 | task_type=CPU ---\n",
      "0:\tlearn: 1.0748509\ttest: 1.0884126\tbest: 1.0884126 (0)\ttotal: 4.97s\tremaining: 11h 3m 1s\n",
      "300:\tlearn: 0.2372476\ttest: 1.5215647\tbest: 0.9383125 (38)\ttotal: 21m 45s\tremaining: 9h 16m 35s\n",
      "600:\tlearn: 0.1315743\ttest: 1.9899362\tbest: 0.9383125 (38)\ttotal: 43m 17s\tremaining: 8h 52m 56s\n",
      "900:\tlearn: 0.0844847\ttest: 2.2760111\tbest: 0.9383125 (38)\ttotal: 1h 4m 46s\tremaining: 8h 30m 22s\n",
      "Stopped by overfitting detector  (900 iterations wait)\n",
      "\n",
      "bestTest = 0.9383124597\n",
      "bestIteration = 38\n",
      "\n",
      "Shrink model to first 39 iterations.\n",
      "  seed=42 macroF1=0.55625 | best_iter=38\n",
      "0:\tlearn: 1.0756165\ttest: 1.0841223\tbest: 1.0841223 (0)\ttotal: 4.42s\tremaining: 9h 49m 2s\n",
      "300:\tlearn: 0.2347964\ttest: 1.5819824\tbest: 0.9436903 (20)\ttotal: 21m 49s\tremaining: 9h 18m 4s\n",
      "600:\tlearn: 0.1293633\ttest: 2.0073128\tbest: 0.9436903 (20)\ttotal: 43m 25s\tremaining: 8h 54m 32s\n",
      "900:\tlearn: 0.0821359\ttest: 2.3819842\tbest: 0.9436903 (20)\ttotal: 1h 4m 53s\tremaining: 8h 31m 13s\n",
      "Stopped by overfitting detector  (900 iterations wait)\n",
      "\n",
      "bestTest = 0.9436903407\n",
      "bestIteration = 20\n",
      "\n",
      "Shrink model to first 21 iterations.\n",
      "  seed=777 macroF1=0.55595 | best_iter=20\n",
      "  ENSEMBLE macroF1=0.55659 | seeds=[42, 777]\n",
      "\n",
      "--- Fold val_year_2024: train=323389 valid=153824 | task_type=CPU ---\n",
      "0:\tlearn: 1.0729991\ttest: 1.0863546\tbest: 1.0863546 (0)\ttotal: 7.38s\tremaining: 16h 23m 58s\n",
      "300:\tlearn: 0.2629072\ttest: 1.3518559\tbest: 0.9817994 (24)\ttotal: 34m 45s\tremaining: 14h 48m 58s\n",
      "600:\tlearn: 0.1650163\ttest: 1.6491929\tbest: 0.9817994 (24)\ttotal: 1h 9m 27s\tremaining: 14h 15m 11s\n",
      "900:\tlearn: 0.1135651\ttest: 1.9079217\tbest: 0.9817994 (24)\ttotal: 1h 44m 14s\tremaining: 13h 41m 22s\n",
      "Stopped by overfitting detector  (900 iterations wait)\n",
      "\n",
      "bestTest = 0.981799377\n",
      "bestIteration = 24\n",
      "\n",
      "Shrink model to first 25 iterations.\n",
      "  seed=42 macroF1=0.39868 | best_iter=24\n",
      "0:\tlearn: 1.0719883\ttest: 1.0879661\tbest: 1.0879661 (0)\ttotal: 7.03s\tremaining: 15h 37m 30s\n",
      "300:\tlearn: 0.2671523\ttest: 1.3557321\tbest: 0.9796477 (31)\ttotal: 34m 31s\tremaining: 14h 43m 13s\n",
      "600:\tlearn: 0.1665093\ttest: 1.6679192\tbest: 0.9796477 (31)\ttotal: 1h 8m 43s\tremaining: 14h 6m 3s\n",
      "900:\tlearn: 0.1147261\ttest: 1.9310084\tbest: 0.9796477 (31)\ttotal: 1h 42m 42s\tremaining: 13h 29m 14s\n",
      "Stopped by overfitting detector  (900 iterations wait)\n",
      "\n",
      "bestTest = 0.9796476665\n",
      "bestIteration = 31\n",
      "\n",
      "Shrink model to first 32 iterations.\n",
      "  seed=777 macroF1=0.39773 | best_iter=31\n",
      "  ENSEMBLE macroF1=0.39471 | seeds=[42, 777]\n",
      "\n",
      "--- Fold val_year_2025: train=477213 valid=109434 | task_type=CPU ---\n",
      "0:\tlearn: 1.0752104\ttest: 1.0804035\tbest: 1.0804035 (0)\ttotal: 10.9s\tremaining: 1d 11m 29s\n",
      "300:\tlearn: 0.3376447\ttest: 1.0817708\tbest: 0.8582214 (88)\ttotal: 54m 25s\tremaining: 23h 12m 13s\n",
      "600:\tlearn: 0.2202869\ttest: 1.3821056\tbest: 0.8582214 (88)\ttotal: 1h 48m 27s\tremaining: 22h 15m 20s\n",
      "900:\tlearn: 0.1576808\ttest: 1.6398140\tbest: 0.8582214 (88)\ttotal: 2h 42m 30s\tremaining: 21h 20m 27s\n",
      "Stopped by overfitting detector  (900 iterations wait)\n",
      "\n",
      "bestTest = 0.8582213918\n",
      "bestIteration = 88\n",
      "\n",
      "Shrink model to first 89 iterations.\n",
      "  seed=42 macroF1=0.52199 | best_iter=88\n",
      "0:\tlearn: 1.0753930\ttest: 1.0830289\tbest: 1.0830289 (0)\ttotal: 11.3s\tremaining: 1d 1h 11m 47s\n",
      "300:\tlearn: 0.3396037\ttest: 1.0582298\tbest: 0.8384497 (66)\ttotal: 54m 30s\tremaining: 23h 14m 6s\n",
      "600:\tlearn: 0.2228877\ttest: 1.3503417\tbest: 0.8384497 (66)\ttotal: 1h 48m 43s\tremaining: 22h 18m 34s\n",
      "900:\tlearn: 0.1590863\ttest: 1.5921606\tbest: 0.8384497 (66)\ttotal: 2h 43m 15s\tremaining: 21h 26m 15s\n",
      "Stopped by overfitting detector  (900 iterations wait)\n",
      "\n",
      "bestTest = 0.8384497335\n",
      "bestIteration = 66\n",
      "\n",
      "Shrink model to first 67 iterations.\n",
      "  seed=777 macroF1=0.51550 | best_iter=66\n",
      "  ENSEMBLE macroF1=0.51860 | seeds=[42, 777]\n",
      "\n",
      "=== STAGE 4 SUMMARY (REV TOP v6.2) ===\n",
      "            fold  n_train  n_valid  macro_f1_ens  macro_f1_seed_min  \\\n",
      "0  val_year_2023   180852   142537      0.556594           0.555951   \n",
      "1  val_year_2024   323389   153824      0.394708           0.397731   \n",
      "2  val_year_2025   477213   109434      0.518595           0.515497   \n",
      "\n",
      "   macro_f1_seed_max  macro_f1_seed_mean  \n",
      "0           0.556245            0.556098  \n",
      "1           0.398681            0.398206  \n",
      "2           0.521987            0.518742  \n",
      "OOF macro F1: 0.5019164527451933\n",
      "\n",
      "[OK] Stage 4 completed. Globals ready for Stage 5:\n",
      "- models_by_fold (dict fold -> list[CatBoostClassifier])\n",
      "- oof_pred_proba, oof_macro_f1, cv_report\n",
      "- FEATURE_COLS_MODEL_USED, CAT_FEATS_MODEL_USED, LABELS, id_to_label\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 4 — Model Training (Forecasting-safe + Time-based CV)\n",
    "# REV TOP v6.2 — FIX string[python] dtype (NO np.issubdtype)\n",
    "#\n",
    "# Requires from STAGE 3:\n",
    "#   df_train_sup, FEATURE_COLS_MODEL\n",
    "#\n",
    "# Outputs:\n",
    "#   models_by_fold (dict fold -> list[CatBoostClassifier])\n",
    "#   oof_pred_proba, oof_macro_f1, cv_report\n",
    "#   FEATURE_COLS_MODEL_USED, CAT_FEATS_MODEL_USED, LABELS, id_to_label\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from pandas.api.types import (\n",
    "    is_datetime64_any_dtype,\n",
    "    is_timedelta64_dtype,\n",
    ")\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# ---------- guards ----------\n",
    "need = [\"df_train_sup\", \"FEATURE_COLS_MODEL\"]\n",
    "miss = [k for k in need if k not in globals()]\n",
    "if miss:\n",
    "    raise RuntimeError(f\"Missing globals from STAGE 3: {miss}. Jalankan STAGE 3 dulu.\")\n",
    "\n",
    "SEED_BASE = 42\n",
    "np.random.seed(SEED_BASE)\n",
    "\n",
    "LABELS = [\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\"]\n",
    "label_to_id = {k: i for i, k in enumerate(LABELS)}\n",
    "id_to_label = {i: k for k, i in label_to_id.items()}\n",
    "\n",
    "# =========================\n",
    "# helpers\n",
    "# =========================\n",
    "def _dedup_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _macro_f1(y_true, proba_3):\n",
    "    pred = np.asarray(proba_3).argmax(axis=1)\n",
    "    return float(f1_score(y_true, pred, average=\"macro\"))\n",
    "\n",
    "def _sanitize_for_cb(df: pd.DataFrame, cat_cols, num_cols) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\").fillna(\"NA\").astype(str)\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "def _is_datetime_like(s: pd.Series) -> bool:\n",
    "    # aman untuk dtype \"string[python]\" dll\n",
    "    return is_datetime64_any_dtype(s) or is_timedelta64_dtype(s)\n",
    "\n",
    "def _drop_forbidden_features(cols):\n",
    "    # pastikan tidak ada leakage/kolom target masuk fitur\n",
    "    forbid_exact = {\n",
    "        \"label_3\", \"kategori_3\", \"kategori\", \"y\",\n",
    "        \"target_date\", \"tanggal_target\", \"tanggal\", \"date\", \"time\",\n",
    "        \"anchor_date\", \"cutoff_date\",\n",
    "    }\n",
    "    forbid_substr = [\"label\", \"kategori\", \"target_date\", \"tanggal_target\", \"anchor_date\", \"cutoff_date\"]\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if c in forbid_exact:\n",
    "            continue\n",
    "        low = str(c).lower()\n",
    "        if any(s in low for s in forbid_substr):\n",
    "            continue\n",
    "        out.append(c)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# 4.1 Prepare supervised table\n",
    "# =========================\n",
    "df = _dedup_cols(df_train_sup.copy())\n",
    "\n",
    "date_col  = _pick_col(df, [\"target_date\", \"tanggal_target\", \"tanggal\", \"date\", \"time\"])\n",
    "label_col = _pick_col(df, [\"label_3\", \"kategori_3\", \"kategori\"])\n",
    "hcol      = _pick_col(df, [\"horizon_days\", \"horizon\", \"h\"])\n",
    "\n",
    "if date_col is None:\n",
    "    raise RuntimeError(f\"Cannot find target date column in df_train_sup. cols(head)={df.columns.tolist()[:60]}\")\n",
    "if label_col is None:\n",
    "    raise RuntimeError(\"Cannot find label column (label_3/kategori_3/kategori) in df_train_sup.\")\n",
    "if hcol is None:\n",
    "    raise RuntimeError(\"Cannot find horizon column (horizon_days/horizon/h) in df_train_sup.\")\n",
    "if \"stasiun_code\" not in df.columns:\n",
    "    raise RuntimeError(\"df_train_sup missing stasiun_code. Pastikan Stage 3 benar.\")\n",
    "\n",
    "df[\"target_date\"] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"target_date\"]).copy()\n",
    "\n",
    "df[\"label_3\"] = df[label_col].astype(\"string\").str.upper().str.strip()\n",
    "df[\"label_3\"] = df[\"label_3\"].replace({\"SANGAT TIDAK SEHAT\":\"TIDAK SEHAT\",\"BERBAHAYA\":\"TIDAK SEHAT\"})\n",
    "df = df.loc[df[\"label_3\"].isin(LABELS)].copy()\n",
    "df[\"y\"] = df[\"label_3\"].map(label_to_id).astype(int)\n",
    "\n",
    "df[\"horizon_days\"] = pd.to_numeric(df[hcol], errors=\"coerce\").astype(\"float32\")\n",
    "df = df.dropna(subset=[\"horizon_days\"]).copy()\n",
    "df[\"horizon_days\"] = df[\"horizon_days\"].clip(1, 91)\n",
    "\n",
    "df[\"stasiun_code\"] = df[\"stasiun_code\"].astype(\"string\").str.upper().str.strip()\n",
    "\n",
    "# =========================\n",
    "# 4.2 Feature list (strict + safe)  (NO np.issubdtype)\n",
    "# =========================\n",
    "raw_feats = [c for c in FEATURE_COLS_MODEL if c in df.columns]\n",
    "raw_feats = _drop_forbidden_features(raw_feats)\n",
    "\n",
    "FEATURE_COLS_MODEL_USED = []\n",
    "for c in raw_feats:\n",
    "    s = df[c]\n",
    "    if _is_datetime_like(s):\n",
    "        continue\n",
    "    FEATURE_COLS_MODEL_USED.append(c)\n",
    "\n",
    "FEATURE_COLS_MODEL_USED = list(dict.fromkeys(FEATURE_COLS_MODEL_USED))\n",
    "if len(FEATURE_COLS_MODEL_USED) < 50:\n",
    "    raise RuntimeError(f\"Too few usable features: {len(FEATURE_COLS_MODEL_USED)}. Cek Stage 3 FEATURE_COLS_MODEL.\")\n",
    "\n",
    "# cat features\n",
    "if \"CAT_FEATURES\" in globals() and isinstance(globals()[\"CAT_FEATURES\"], (list, tuple)):\n",
    "    CAT_FEATS_MODEL_USED = [c for c in globals()[\"CAT_FEATURES\"] if c in FEATURE_COLS_MODEL_USED]\n",
    "else:\n",
    "    CAT_FEATS_MODEL_USED = [c for c in [\"stasiun_code\", \"day_name\", \"nama_libur\"] if c in FEATURE_COLS_MODEL_USED]\n",
    "\n",
    "NUM_FEATS_MODEL_USED = [c for c in FEATURE_COLS_MODEL_USED if c not in CAT_FEATS_MODEL_USED]\n",
    "\n",
    "# sanitize for CatBoost\n",
    "df = _sanitize_for_cb(df, CAT_FEATS_MODEL_USED, NUM_FEATS_MODEL_USED)\n",
    "\n",
    "# =========================\n",
    "# 4.3 Sample weights (LB-oriented: long-horizon + recency)\n",
    "# =========================\n",
    "h = df[\"horizon_days\"].to_numpy(dtype=\"float64\")\n",
    "w_h = np.where(h < 14, 0.85, np.where(h < 30, 1.00, 1.00 + 0.55*((h-30)/(91-30))**1.25)).astype(\"float64\")\n",
    "\n",
    "tmin, tmax = df[\"target_date\"].min(), df[\"target_date\"].max()\n",
    "if pd.isna(tmin) or pd.isna(tmax) or tmin == tmax:\n",
    "    w_t = np.ones(len(df), dtype=\"float64\")\n",
    "else:\n",
    "    ti = df[\"target_date\"].astype(\"datetime64[ns]\").astype(\"int64\").to_numpy()\n",
    "    frac = (ti - int(tmin.value)) / max(1, (int(tmax.value) - int(tmin.value)))\n",
    "    frac = np.clip(frac.astype(\"float64\"), 0.0, 1.0)\n",
    "    w_t = (0.85 + 0.70*(frac**1.35)).astype(\"float64\")\n",
    "\n",
    "sample_weight = (w_h * w_t).astype(\"float32\")\n",
    "\n",
    "# =========================\n",
    "# 4.4 Time-based CV (walk-forward by year)\n",
    "# =========================\n",
    "years = sorted(df[\"target_date\"].dt.year.unique().tolist())\n",
    "if len(years) >= 6:\n",
    "    val_years = years[-3:]\n",
    "elif len(years) >= 4:\n",
    "    val_years = years[-2:]\n",
    "else:\n",
    "    val_years = years[-1:]\n",
    "\n",
    "folds = []\n",
    "for vy in val_years:\n",
    "    tr = (df[\"target_date\"].dt.year < vy).to_numpy()\n",
    "    va = (df[\"target_date\"].dt.year == vy).to_numpy()\n",
    "    folds.append((f\"val_year_{vy}\", tr, va))\n",
    "\n",
    "print(\"=== STAGE 4 SETUP (REV TOP v6.2) ===\")\n",
    "print(\"Detected date_col:\", date_col, \"| label_col:\", label_col, \"| horizon_col:\", hcol)\n",
    "print(\"Rows:\", len(df), \"| Years:\", years)\n",
    "print(\"Val folds:\", [f[0] for f in folds])\n",
    "print(\"Features:\", len(FEATURE_COLS_MODEL_USED), \"| cat:\", len(CAT_FEATS_MODEL_USED), \"| num:\", len(NUM_FEATS_MODEL_USED))\n",
    "print(\"Label counts:\", df[\"y\"].value_counts().to_dict())\n",
    "\n",
    "TASK_TYPE = \"GPU\" if os.environ.get(\"CUDA_VISIBLE_DEVICES\") not in (None, \"\", \"-1\") else \"CPU\"\n",
    "\n",
    "# NOTE: early stop pakai MultiClass (loss) agar tidak \"best_iter=0..10\" karena noise F1\n",
    "CB_PARAMS = dict(\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"MultiClass\",\n",
    "    custom_metric=[\"TotalF1\"],\n",
    "    iterations=8000,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=9.0,\n",
    "    random_strength=1.0,\n",
    "    bagging_temperature=0.8,\n",
    "    bootstrap_type=\"Bayesian\",\n",
    "    boosting_type=\"Ordered\",\n",
    "    rsm=0.90,\n",
    "    border_count=254,\n",
    "    max_ctr_complexity=4,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=900,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    task_type=TASK_TYPE,\n",
    "    verbose=300\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4.5 Train (multi-seed ensemble per fold)\n",
    "# =========================\n",
    "SEEDS = [42, 777]\n",
    "oof_pred_proba = np.full((len(df), len(LABELS)), np.nan, dtype=float)\n",
    "models_by_fold = {}\n",
    "cv_rows = []\n",
    "\n",
    "for fold_name, tr_mask, va_mask in folds:\n",
    "    ntr, nva = int(tr_mask.sum()), int(va_mask.sum())\n",
    "    print(f\"\\n--- Fold {fold_name}: train={ntr} valid={nva} | task_type={TASK_TYPE} ---\")\n",
    "    if ntr == 0 or nva == 0:\n",
    "        cv_rows.append({\"fold\": fold_name, \"n_train\": ntr, \"n_valid\": nva, \"macro_f1_ens\": np.nan})\n",
    "        continue\n",
    "\n",
    "    X_tr = df.loc[tr_mask, FEATURE_COLS_MODEL_USED]\n",
    "    y_tr = df.loc[tr_mask, \"y\"].to_numpy()\n",
    "    w_tr = sample_weight[tr_mask]\n",
    "\n",
    "    X_va = df.loc[va_mask, FEATURE_COLS_MODEL_USED]\n",
    "    y_va = df.loc[va_mask, \"y\"].to_numpy()\n",
    "    w_va = sample_weight[va_mask]\n",
    "\n",
    "    tr_pool = Pool(X_tr, label=y_tr, cat_features=CAT_FEATS_MODEL_USED, weight=w_tr)\n",
    "    va_pool = Pool(X_va, label=y_va, cat_features=CAT_FEATS_MODEL_USED, weight=w_va)\n",
    "\n",
    "    proba_sum = np.zeros((nva, len(LABELS)), dtype=float)\n",
    "    models = []\n",
    "    seed_scores = []\n",
    "\n",
    "    for sd in SEEDS:\n",
    "        model = CatBoostClassifier(**CB_PARAMS, random_seed=int(sd))\n",
    "        model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
    "\n",
    "        proba = model.predict_proba(X_va)\n",
    "        proba_sum += proba\n",
    "\n",
    "        f1m = _macro_f1(y_va, proba)\n",
    "        seed_scores.append(f1m)\n",
    "        models.append(model)\n",
    "        print(f\"  seed={sd} macroF1={f1m:.5f} | best_iter={int(model.get_best_iteration())}\")\n",
    "\n",
    "    proba_ens = proba_sum / float(len(SEEDS))\n",
    "    f1_ens = _macro_f1(y_va, proba_ens)\n",
    "    print(f\"  ENSEMBLE macroF1={f1_ens:.5f} | seeds={SEEDS}\")\n",
    "\n",
    "    oof_pred_proba[va_mask] = proba_ens\n",
    "    models_by_fold[fold_name] = models\n",
    "    cv_rows.append({\n",
    "        \"fold\": fold_name,\n",
    "        \"n_train\": ntr,\n",
    "        \"n_valid\": nva,\n",
    "        \"macro_f1_ens\": float(f1_ens),\n",
    "        \"macro_f1_seed_min\": float(np.min(seed_scores)),\n",
    "        \"macro_f1_seed_max\": float(np.max(seed_scores)),\n",
    "        \"macro_f1_seed_mean\": float(np.mean(seed_scores)),\n",
    "    })\n",
    "\n",
    "cv_report = pd.DataFrame(cv_rows)\n",
    "\n",
    "oof_mask = ~np.isnan(oof_pred_proba).any(axis=1)\n",
    "oof_true = df.loc[oof_mask, \"y\"].to_numpy()\n",
    "oof_macro_f1 = _macro_f1(oof_true, oof_pred_proba[oof_mask]) if oof_mask.any() else np.nan\n",
    "\n",
    "print(\"\\n=== STAGE 4 SUMMARY (REV TOP v6.2) ===\")\n",
    "print(cv_report)\n",
    "print(\"OOF macro F1:\", oof_macro_f1)\n",
    "\n",
    "print(\"\\n[OK] Stage 4 completed. Globals ready for Stage 5:\")\n",
    "print(\"- models_by_fold (dict fold -> list[CatBoostClassifier])\")\n",
    "print(\"- oof_pred_proba, oof_macro_f1, cv_report\")\n",
    "print(\"- FEATURE_COLS_MODEL_USED, CAT_FEATS_MODEL_USED, LABELS, id_to_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d4a51",
   "metadata": {
    "papermill": {
     "duration": 0.006249,
     "end_time": "2026-02-05T14:01:00.498741",
     "exception": false,
     "start_time": "2026-02-05T14:01:00.492492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference, Ensembling, Submission & QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ecba34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-05T14:01:00.516794Z",
     "iopub.status.busy": "2026-02-05T14:01:00.516115Z",
     "iopub.status.idle": "2026-02-05T14:01:01.161135Z",
     "shell.execute_reply": "2026-02-05T14:01:01.159754Z"
    },
    "papermill": {
     "duration": 0.658259,
     "end_time": "2026-02-05T14:01:01.163514",
     "exception": false,
     "start_time": "2026-02-05T14:01:00.505255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 5 MODEL DETECTION ===\n",
      "Folds: ['val_year_2023', 'val_year_2024', 'val_year_2025']\n",
      "Fold weights: {'val_year_2023': 0.1163, 'val_year_2024': 0.2677, 'val_year_2025': 0.616}\n",
      "[OK] Fold 'val_year_2023': used 2/2 models, weight=0.1163\n",
      "[OK] Fold 'val_year_2024': used 2/2 models, weight=0.2677\n",
      "[OK] Fold 'val_year_2025': used 2/2 models, weight=0.6160\n",
      "\n",
      "=== STAGE 5 QA ===\n",
      "Used model ensemble: True\n",
      "Rows: 455 | expected: 455 | ok: True\n",
      "ID order matches sample: True\n",
      "Missing pred: 0\n",
      "Labels ok: True\n",
      "Label counts:\n",
      " category\n",
      "SEDANG         366\n",
      "TIDAK SEHAT     61\n",
      "BAIK            28\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[OK] Saved submission -> /kaggle/working/submission.csv\n",
      "[OK] Saved proba -> /kaggle/working/pred_proba.npy\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STAGE 5 — Test Inference (Fold Ensemble) + Submission + QA\n",
    "# REV TOP v5.0 — MATCH Stage 4 v6.2 output\n",
    "#\n",
    "# Expects:\n",
    "#   df_targets_feat\n",
    "#   models_by_fold (dict: fold_name -> list[CatBoostClassifier])  [from Stage 4 v6.2]\n",
    "#   FEATURE_COLS_MODEL_USED, CAT_FEATS_MODEL_USED                [from Stage 4 v6.2]\n",
    "#\n",
    "# Output:\n",
    "#   /kaggle/working/submission.csv\n",
    "#   /kaggle/working/pred_proba.npy\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# Guards\n",
    "# -----------------------------\n",
    "if \"df_targets_feat\" not in globals():\n",
    "    raise RuntimeError(\"Missing df_targets_feat. Jalankan Stage 3 dulu.\")\n",
    "if not isinstance(df_targets_feat, pd.DataFrame) or len(df_targets_feat) == 0:\n",
    "    raise RuntimeError(\"df_targets_feat kosong / bukan DataFrame.\")\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\n",
    "SAMPLE_PATH = DATA_ROOT / \"sample_submission.csv\"\n",
    "OUT_SUB = Path(\"/kaggle/working/submission.csv\")\n",
    "OUT_PROBA = Path(\"/kaggle/working/pred_proba.npy\")\n",
    "\n",
    "LABELS = [\"BAIK\", \"SEDANG\", \"TIDAK SEHAT\"]\n",
    "id_to_label = {i: k for i, k in enumerate(LABELS)}\n",
    "\n",
    "# -----------------------------\n",
    "# Controls\n",
    "# -----------------------------\n",
    "SAVE_PROBA = True\n",
    "\n",
    "# prior blend (stabil long horizon)\n",
    "USE_PRIOR_BLEND = True\n",
    "PRIOR_BLEND_MAX = 0.30   # 0.15–0.45 (naikkan kalau model terlalu \"overconfident\")\n",
    "\n",
    "# fold recency weighting (target 2025)\n",
    "WEIGHT_FOLDS_BY_RECENCY = True\n",
    "RECENCY_TAU = 1.20       # makin kecil => makin berat ke fold terbaru\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _dedup_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return df.loc[:, ~df.columns.duplicated()].copy()\n",
    "\n",
    "def sanitize_features(df: pd.DataFrame, cat_cols, num_cols) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\").fillna(\"NA\").astype(str)\n",
    "        else:\n",
    "            df[c] = \"NA\"\n",
    "    for c in num_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "        else:\n",
    "            df[c] = np.nan\n",
    "    return df\n",
    "\n",
    "def normalize_rows(p: np.ndarray) -> np.ndarray:\n",
    "    p = np.asarray(p, dtype=\"float64\")\n",
    "    s = p.sum(axis=1, keepdims=True)\n",
    "    s = np.where(s <= 0, 1.0, s)\n",
    "    return p / s\n",
    "\n",
    "def make_prior_proba(df: pd.DataFrame) -> np.ndarray:\n",
    "    n = len(df)\n",
    "    mon_cols = [\"p_baik_mon\", \"p_sedang_mon\", \"p_tidak_mon\"]\n",
    "    wk_cols  = [\"p_baik_wk\",  \"p_sedang_wk\",  \"p_tidak_wk\"]\n",
    "\n",
    "    has_mon = all(c in df.columns for c in mon_cols)\n",
    "    has_wk  = all(c in df.columns for c in wk_cols)\n",
    "\n",
    "    cnt = 0\n",
    "    acc = np.zeros((n, 3), dtype=\"float64\")\n",
    "    if has_mon:\n",
    "        acc += np.nan_to_num(df[mon_cols].to_numpy(dtype=\"float64\"), nan=0.0); cnt += 1\n",
    "    if has_wk:\n",
    "        acc += np.nan_to_num(df[wk_cols].to_numpy(dtype=\"float64\"), nan=0.0); cnt += 1\n",
    "\n",
    "    if cnt > 0:\n",
    "        return normalize_rows(acc / cnt)\n",
    "\n",
    "    # fallback terakhir: uniform\n",
    "    return np.tile(np.array([1/3, 1/3, 1/3], dtype=\"float64\")[None, :], (n, 1))\n",
    "\n",
    "def blend_with_prior(proba_model: np.ndarray, proba_prior: np.ndarray, horizon: np.ndarray, prior_max: float) -> np.ndarray:\n",
    "    h = np.asarray(horizon, dtype=\"float64\")\n",
    "    hmin, hmax = np.nanmin(h), np.nanmax(h)\n",
    "    denom = (hmax - hmin) if (hmax > hmin) else 1.0\n",
    "    t = np.clip((h - hmin) / denom, 0.0, 1.0).reshape(-1, 1)\n",
    "    w = prior_max * t\n",
    "    out = (1.0 - w) * proba_model + w * proba_prior\n",
    "    return normalize_rows(out)\n",
    "\n",
    "def iter_models(obj):\n",
    "    # flatten holder: model | list/tuple/set | dict | nested\n",
    "    out = []\n",
    "    if obj is None:\n",
    "        return out\n",
    "    if hasattr(obj, \"predict_proba\"):\n",
    "        return [obj]\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        for x in obj:\n",
    "            out.extend(iter_models(x))\n",
    "        return out\n",
    "    if isinstance(obj, dict):\n",
    "        for v in obj.values():\n",
    "            out.extend(iter_models(v))\n",
    "        return out\n",
    "    return out\n",
    "\n",
    "def fold_year_from_name(name: str):\n",
    "    m = re.search(r\"(\\d{4})\", str(name))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def build_fold_weights(fold_names):\n",
    "    if not WEIGHT_FOLDS_BY_RECENCY:\n",
    "        return {n: 1.0 / max(1, len(fold_names)) for n in fold_names}\n",
    "\n",
    "    years = [fold_year_from_name(n) for n in fold_names]\n",
    "    years_ok = [y for y in years if y is not None]\n",
    "    if len(years_ok) == 0:\n",
    "        return {n: 1.0 / max(1, len(fold_names)) for n in fold_names}\n",
    "\n",
    "    y_max = max(years_ok)\n",
    "    w = {}\n",
    "    for n in fold_names:\n",
    "        y = fold_year_from_name(n)\n",
    "        if y is None:\n",
    "            w[n] = 1.0\n",
    "        else:\n",
    "            w[n] = float(np.exp(-(y_max - y) / max(1e-6, RECENCY_TAU)))\n",
    "\n",
    "    s = sum(w.values())\n",
    "    if s <= 0:\n",
    "        return {n: 1.0 / max(1, len(fold_names)) for n in fold_names}\n",
    "    return {n: v / s for n, v in w.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# Load sample_submission (order lock)\n",
    "# -----------------------------\n",
    "df_sample = pd.read_csv(SAMPLE_PATH)\n",
    "sample_id_col = df_sample.columns[0]\n",
    "sample_target_col = df_sample.columns[1] if len(df_sample.columns) > 1 else \"kategori\"\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare prediction table\n",
    "# -----------------------------\n",
    "df_pred = _dedup_cols(df_targets_feat.copy())\n",
    "\n",
    "if \"id\" not in df_pred.columns:\n",
    "    raise RuntimeError(\"df_targets_feat harus punya kolom 'id'.\")\n",
    "if \"horizon_days\" not in df_pred.columns:\n",
    "    raise RuntimeError(\"df_targets_feat harus punya kolom 'horizon_days'.\")\n",
    "\n",
    "# Feature list from Stage 4 v6.2 (priority)\n",
    "if \"FEATURE_COLS_MODEL_USED\" in globals() and isinstance(globals()[\"FEATURE_COLS_MODEL_USED\"], list):\n",
    "    FEATURE_COLS = globals()[\"FEATURE_COLS_MODEL_USED\"]\n",
    "else:\n",
    "    raise RuntimeError(\"Missing FEATURE_COLS_MODEL_USED (jalankan Stage 4 v6.2 dulu).\")\n",
    "\n",
    "if \"CAT_FEATS_MODEL_USED\" in globals() and isinstance(globals()[\"CAT_FEATS_MODEL_USED\"], list):\n",
    "    CAT_COLS = globals()[\"CAT_FEATS_MODEL_USED\"]\n",
    "else:\n",
    "    CAT_COLS = [c for c in [\"stasiun_code\", \"day_name\", \"nama_libur\"] if c in df_pred.columns]\n",
    "\n",
    "CAT_COLS = [c for c in CAT_COLS if c in FEATURE_COLS]\n",
    "NUM_COLS = [c for c in FEATURE_COLS if c not in CAT_COLS]\n",
    "\n",
    "df_pred = sanitize_features(df_pred, CAT_COLS, NUM_COLS)\n",
    "X_test = df_pred[FEATURE_COLS].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Predict using fold ensemble (Stage 4 v6.2 structure)\n",
    "# -----------------------------\n",
    "proba = None\n",
    "used_model = False\n",
    "\n",
    "models_by_fold = globals().get(\"models_by_fold\", None)\n",
    "if isinstance(models_by_fold, dict) and len(models_by_fold) > 0:\n",
    "    fold_names = list(models_by_fold.keys())\n",
    "    fold_w = build_fold_weights(fold_names)\n",
    "\n",
    "    psum = np.zeros((len(X_test), 3), dtype=\"float64\")\n",
    "    wsum = 0.0\n",
    "\n",
    "    print(\"=== STAGE 5 MODEL DETECTION ===\")\n",
    "    print(\"Folds:\", fold_names)\n",
    "    print(\"Fold weights:\", {k: round(v, 4) for k, v in fold_w.items()})\n",
    "\n",
    "    for fold_name, holder in models_by_fold.items():\n",
    "        fold_models = iter_models(holder)  # list model\n",
    "        if len(fold_models) == 0:\n",
    "            print(f\"[WARN] Fold '{fold_name}': no usable model.\")\n",
    "            continue\n",
    "\n",
    "        p_fold_sum = np.zeros((len(X_test), 3), dtype=\"float64\")\n",
    "        ok = 0\n",
    "        for mi, model in enumerate(fold_models):\n",
    "            try:\n",
    "                p = model.predict_proba(X_test)\n",
    "                p = np.asarray(p, dtype=\"float64\")\n",
    "                if p.ndim != 2 or p.shape[1] != 3:\n",
    "                    p = normalize_rows(p.reshape(len(X_test), -1)[:, :3])\n",
    "                p_fold_sum += p\n",
    "                ok += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Fold '{fold_name}' model#{mi} skipped: {e}\")\n",
    "\n",
    "        if ok == 0:\n",
    "            print(f\"[WARN] Fold '{fold_name}': all models failed.\")\n",
    "            continue\n",
    "\n",
    "        p_fold = normalize_rows(p_fold_sum / ok)\n",
    "        w = float(fold_w.get(fold_name, 1.0))\n",
    "        psum += w * p_fold\n",
    "        wsum += w\n",
    "        print(f\"[OK] Fold '{fold_name}': used {ok}/{len(fold_models)} models, weight={w:.4f}\")\n",
    "\n",
    "    if wsum > 0:\n",
    "        proba = normalize_rows(psum / wsum)\n",
    "        used_model = True\n",
    "\n",
    "# fallback: prior-only (should almost never happen if Stage 4 OK)\n",
    "if proba is None:\n",
    "    print(\"[WARN] models_by_fold not usable -> using prior baseline.\")\n",
    "    proba = make_prior_proba(df_pred)\n",
    "\n",
    "# optional: blend with prior for long horizon\n",
    "if USE_PRIOR_BLEND:\n",
    "    proba_prior = make_prior_proba(df_pred)\n",
    "    proba = blend_with_prior(proba, proba_prior, df_pred[\"horizon_days\"].to_numpy(), PRIOR_BLEND_MAX)\n",
    "\n",
    "proba = normalize_rows(proba)\n",
    "\n",
    "pred_id = proba.argmax(axis=1)\n",
    "pred_label = pd.Series(pred_id).map(id_to_label).astype(str)\n",
    "\n",
    "# -----------------------------\n",
    "# Build submission (locked to sample order)\n",
    "# -----------------------------\n",
    "sub = pd.DataFrame({\n",
    "    sample_id_col: df_pred[\"id\"].astype(str),\n",
    "    sample_target_col: pred_label\n",
    "})\n",
    "\n",
    "sample_ids = df_sample[sample_id_col].astype(str).tolist()\n",
    "\n",
    "# safest align\n",
    "sub = df_sample[[sample_id_col]].merge(sub, on=sample_id_col, how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# QA\n",
    "# -----------------------------\n",
    "ok_rows = (len(sub) == len(df_sample) == 455)\n",
    "missing_pred = int(sub[sample_target_col].isna().sum())\n",
    "ok_labels = set(sub[sample_target_col].dropna().unique()).issubset(set(LABELS))\n",
    "id_match = sub[sample_id_col].astype(str).tolist() == sample_ids\n",
    "\n",
    "print(\"\\n=== STAGE 5 QA ===\")\n",
    "print(\"Used model ensemble:\", used_model)\n",
    "print(\"Rows:\", len(sub), \"| expected:\", len(df_sample), \"| ok:\", ok_rows)\n",
    "print(\"ID order matches sample:\", bool(id_match))\n",
    "print(\"Missing pred:\", missing_pred)\n",
    "print(\"Labels ok:\", bool(ok_labels))\n",
    "print(\"Label counts:\\n\", sub[sample_target_col].value_counts(dropna=False))\n",
    "\n",
    "# degenerate check\n",
    "if sub[sample_target_col].nunique(dropna=False) == 1:\n",
    "    print(\"\\n[WARN] Prediksi hanya 1 kelas (degenerate). Biasanya karena:\")\n",
    "    print(\"  - fitur mismatch (FEATURE_COLS tidak sama saat training vs inference)\")\n",
    "    print(\"  - model tidak benar-benar kepakai (cek log MODEL DETECTION)\")\n",
    "    print(\"  - distribusi prior terlalu dominan (turunkan PRIOR_BLEND_MAX)\")\n",
    "\n",
    "if (not ok_rows) or (not id_match) or (missing_pred > 0) or (not ok_labels):\n",
    "    raise RuntimeError(\"QA failed. Check Stage 3/4 outputs or feature alignment.\")\n",
    "\n",
    "# save\n",
    "sub.to_csv(OUT_SUB, index=False)\n",
    "print(f\"\\n[OK] Saved submission -> {OUT_SUB}\")\n",
    "\n",
    "if SAVE_PROBA:\n",
    "    np.save(OUT_PROBA, proba.astype(\"float32\"))\n",
    "    print(f\"[OK] Saved proba -> {OUT_PROBA}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15499519,
     "sourceId": 129145,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 42140.603869,
   "end_time": "2026-02-05T14:01:04.096514",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-05T02:18:43.492645",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
