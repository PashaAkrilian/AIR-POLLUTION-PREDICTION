{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":129145,"databundleVersionId":15499519,"sourceType":"competition"}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T18:45:20.057967Z","iopub.execute_input":"2026-01-30T18:45:20.058383Z","iopub.status.idle":"2026-01-30T18:45:21.906492Z","shell.execute_reply.started":"2026-01-30T18:45:20.058343Z","shell.execute_reply":"2026-01-30T18:45:21.905386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Loading & Sanity Checks","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 1 — Data Loading & Sanity Checks (ONE CELL) — CatBoost Track (FINAL CONTEXT SAFE)\n# Fixes:\n# - Series.lower() bug -> use .str.lower()\n# - Robust parsing + dedup retained\n# - LABEL SANITIZATION (critical):\n#     * normalize kategori (strip+upper)\n#     * remove invalid kategori that are actually pollutants (e.g., \"O3\", \"PM10\", ...)\n#     * handle ultra-rare classes (BERBAHAYA) -> merge into \"SANGAT TIDAK SEHAT\" (safe default)\n#\n# Outputs (globals):\n#   sub, ID_COL, SUB_TARGET_COL\n#   df_ispu_all, df_train, df_ispu_unlabeled\n#   df_ndvi, df_holiday, df_weather, df_pop, df_river\n#   test_candidates\n# ============================================================\n\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nDATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\nassert DATA_ROOT.exists(), f\"DATA_ROOT not found: {DATA_ROOT}\"\n\npd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.width\", 200)\n\n# ----------------------------\n# Helpers\n# ----------------------------\ndef _read_csv_smart(path: Path) -> pd.DataFrame:\n    seps = [\",\", \";\", \"\\t\", \"|\"]\n    encs = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n    last_err = None\n    for sep in seps:\n        for enc in encs:\n            try:\n                df = pd.read_csv(path, sep=sep, encoding=enc, low_memory=False)\n                if df.shape[1] >= 2:\n                    return df\n            except Exception as e:\n                last_err = e\n    raise RuntimeError(f\"Failed to read: {path}\\nLast error: {last_err}\")\n\ndef _norm_col(c: str) -> str:\n    c = str(c).strip().lower()\n    c = re.sub(r\"[^\\w]+\", \"_\", c)\n    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n    return c\n\ndef _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    cols = {c: _norm_col(c) for c in df.columns}\n    df = df.rename(columns=cols)\n\n    rename = {}\n    for c in df.columns:\n        if c in [\"tanggal\", \"date\", \"time\", \"waktu\"]:\n            rename[c] = \"tanggal\"\n        elif c in [\"stasiun\", \"station\", \"stasiun_id\", \"id_stasiun\"]:\n            rename[c] = \"stasiun\"\n        elif c in [\"periode_data\", \"periode\"]:\n            rename[c] = \"periode_data\"\n        elif c in [\"pm_sepuluh\", \"pm10\", \"pm_10\"]:\n            rename[c] = \"pm10\"\n        elif c in [\"pm_duakomalima\", \"pm2_5\", \"pm25\", \"pm_2_5\", \"pm2_5_\"]:\n            rename[c] = \"pm25\"\n        elif c in [\"sulfur_dioksida\", \"so2\"]:\n            rename[c] = \"so2\"\n        elif c in [\"karbon_monoksida\", \"co\"]:\n            rename[c] = \"co\"\n        elif c in [\"ozon\", \"o3\"]:\n            rename[c] = \"o3\"\n        elif c in [\"nitrogen_dioksida\", \"no2\"]:\n            rename[c] = \"no2\"\n        elif c in [\"parameter_pencemar_kritis\", \"parameter_pencemar\", \"pencemar_kritis\"]:\n            rename[c] = \"parameter_pencemar_kritis\"\n        elif c in [\"max\", \"maks\", \"nilai_maks\", \"indeks_maks\"]:\n            rename[c] = \"max\"\n        elif c in [\"ndvi\", \"vegetation_index\"]:\n            rename[c] = \"ndvi\"\n        elif c in [\"is_holiday_nasional\", \"holiday_nasional\", \"is_holiday\"]:\n            rename[c] = \"is_holiday_nasional\"\n        elif c in [\"is_weekend\", \"weekend\"]:\n            rename[c] = \"is_weekend\"\n        elif c in [\"day_name\", \"nama_hari\"]:\n            rename[c] = \"day_name\"\n        elif c in [\"nama_libur\", \"holiday_name\"]:\n            rename[c] = \"nama_libur\"\n\n        # keep common typos as *_alt\n        elif c == \"categori\":\n            rename[c] = \"kategori_alt\"\n        elif c == \"critical\":\n            rename[c] = \"parameter_pencemar_kritis_alt\"\n\n    return df.rename(columns=rename)\n\ndef parse_date_twopass(s: pd.Series) -> pd.Series:\n    s = s.astype(str).str.strip()\n    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"None\": np.nan})\n    d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n    m = d1.isna()\n    if m.any():\n        d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n        d1.loc[m] = d2\n    return d1\n\ndef _coerce_numeric(df: pd.DataFrame, cols):\n    for c in cols:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n    return df\n\ndef _dedup_keep_most_complete(df: pd.DataFrame, key_cols):\n    if not all(k in df.columns for k in key_cols):\n        return df\n    df = df.copy()\n    df[\"_nn\"] = df.notna().sum(axis=1)\n    idx = df.groupby(key_cols)[\"_nn\"].idxmax()\n    df = df.loc[idx].drop(columns=[\"_nn\"]).reset_index(drop=True)\n    return df\n\ndef _basic_sanity(name: str, df: pd.DataFrame, key_cols=None, date_col=\"tanggal\"):\n    print(f\"\\n--- {name} ---\")\n    print(\"shape:\", df.shape)\n    if key_cols is not None and all(k in df.columns for k in key_cols):\n        print(f\"duplicates on {key_cols}:\", int(df.duplicated(key_cols).sum()))\n    if date_col in df.columns:\n        print(f\"{date_col}: NaT={int(df[date_col].isna().sum())} | range=[{df[date_col].min()} .. {df[date_col].max()}]\")\n    miss = (df.isna().mean().sort_values(ascending=False).head(8) * 100).round(2)\n    print(\"top missing% cols:\")\n    print(miss.to_string())\n\n# ============================================================\n# 0) sample_submission\n# ============================================================\nsub = _standardize_columns(_read_csv_smart(DATA_ROOT / \"sample_submission.csv\"))\nID_COL = \"id\" if \"id\" in sub.columns else sub.columns[0]\nSUB_TARGET_COL = \"category\" if \"category\" in sub.columns else sub.columns[-1]\nn_test_expected = len(sub)\n\nprint(\"Loaded sample_submission:\", sub.shape, \"cols:\", list(sub.columns))\nprint(\"ID_COL:\", ID_COL, \"| SUB_TARGET_COL:\", SUB_TARGET_COL)\nprint(\"submission ID unique:\", bool(sub[ID_COL].is_unique))\n\n# ============================================================\n# 1) ISPU (concat all years) + CLEAN\n# ============================================================\nispu_files = sorted((DATA_ROOT / \"ISPU\").glob(\"*.csv\"))\nassert len(ispu_files) > 0, \"No ISPU CSV files found.\"\n\nframes = []\nfor p in ispu_files:\n    df0 = _standardize_columns(_read_csv_smart(p))\n    df0[\"source_file\"] = p.name\n    frames.append(df0)\n\ndf_ispu_all = pd.concat(frames, ignore_index=True, sort=False)\n\n# unify label + critical columns into canonical names\nif \"kategori_alt\" in df_ispu_all.columns:\n    if \"kategori\" not in df_ispu_all.columns:\n        df_ispu_all[\"kategori\"] = df_ispu_all[\"kategori_alt\"]\n    else:\n        df_ispu_all[\"kategori\"] = df_ispu_all[\"kategori\"].fillna(df_ispu_all[\"kategori_alt\"])\n\nif \"parameter_pencemar_kritis_alt\" in df_ispu_all.columns:\n    if \"parameter_pencemar_kritis\" not in df_ispu_all.columns:\n        df_ispu_all[\"parameter_pencemar_kritis\"] = df_ispu_all[\"parameter_pencemar_kritis_alt\"]\n    else:\n        df_ispu_all[\"parameter_pencemar_kritis\"] = df_ispu_all[\"parameter_pencemar_kritis\"].fillna(\n            df_ispu_all[\"parameter_pencemar_kritis_alt\"]\n        )\n\n# robust date parse\nif \"tanggal\" in df_ispu_all.columns:\n    df_ispu_all[\"tanggal\"] = parse_date_twopass(df_ispu_all[\"tanggal\"])\n\n# stasiun cleanup + code\nif \"stasiun\" in df_ispu_all.columns:\n    df_ispu_all[\"stasiun\"] = df_ispu_all[\"stasiun\"].astype(str).str.strip()\n    df_ispu_all[\"stasiun_code\"] = (\n        df_ispu_all[\"stasiun\"]\n        .str.upper()\n        .str.extract(r\"(DKI\\s*\\d+)\", expand=False)\n        .str.replace(\" \", \"\", regex=False)\n    )\nelse:\n    df_ispu_all[\"stasiun_code\"] = np.nan\n\n# numeric casts\ndf_ispu_all = _coerce_numeric(df_ispu_all, [\"pm10\", \"pm25\", \"so2\", \"co\", \"o3\", \"no2\", \"max\"])\n\n# drop rows missing key fields\ndf_ispu_all = df_ispu_all.dropna(subset=[\"tanggal\", \"stasiun\"]).copy()\n\n# dedup by key keep most complete\ndf_ispu_all = _dedup_keep_most_complete(df_ispu_all, [\"tanggal\", \"stasiun\"])\ndf_ispu_all = df_ispu_all.sort_values([\"tanggal\", \"stasiun\"]).reset_index(drop=True)\n\n_basic_sanity(\"ISPU (ALL) CLEAN (pre-label-sanitize)\", df_ispu_all, key_cols=[\"tanggal\", \"stasiun\"])\n\n# ============================================================\n# [CRITICAL] LABEL SANITIZATION & TRAIN/UNLABELED SPLIT\n# ============================================================\n# 1) normalize kategori (upper, strip)\nif \"kategori\" in df_ispu_all.columns:\n    df_ispu_all[\"kategori\"] = (\n        df_ispu_all[\"kategori\"]\n        .astype(str)\n        .str.strip()\n        .str.upper()\n        .replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan})\n    )\n\n# 2) remove invalid kategori that are actually pollutants\nINVALID_KATEGORI = {\"PM10\", \"PM25\", \"SO2\", \"CO\", \"O3\", \"NO2\", \"MAX\"}\nif \"kategori\" in df_ispu_all.columns:\n    bad = df_ispu_all[\"kategori\"].isin(INVALID_KATEGORI)\n    n_bad = int(bad.sum())\n    if n_bad > 0:\n        df_ispu_all.loc[bad, \"kategori\"] = np.nan\n    print(f\"[LABEL] invalid kategori removed (pollutant-as-label): {n_bad}\")\n\n# 3) handle ultra-rare class \"BERBAHAYA\" (default merge; keeps training stable)\nif \"kategori\" in df_ispu_all.columns:\n    vc = df_ispu_all[\"kategori\"].value_counts(dropna=True)\n    if vc.get(\"BERBAHAYA\", 0) < 10:\n        df_ispu_all.loc[df_ispu_all[\"kategori\"] == \"BERBAHAYA\", \"kategori\"] = \"SANGAT TIDAK SEHAT\"\n        print(\"[LABEL] merged BERBAHAYA -> SANGAT TIDAK SEHAT (rare class)\")\n\n# 4) build train vs unlabeled (BUGFIX: use .str.lower())\nif \"kategori\" in df_ispu_all.columns:\n    lab = df_ispu_all[\"kategori\"].astype(str).str.strip()\n    lab_low = lab.str.lower()\n    m_train = df_ispu_all[\"kategori\"].notna() & (lab != \"\") & (lab_low != \"nan\")\n    df_train = df_ispu_all.loc[m_train].copy()\n    df_ispu_unlabeled = df_ispu_all.loc[~m_train].copy()\nelse:\n    df_train = df_ispu_all.copy()\n    df_ispu_unlabeled = df_ispu_all.iloc[0:0].copy()\n\nprint(\"\\nTrain/unlabeled split:\")\nprint(\"df_train:\", df_train.shape, \"| df_ispu_unlabeled:\", df_ispu_unlabeled.shape)\nif \"kategori\" in df_train.columns:\n    print(\"\\nTarget distribution (df_train):\")\n    print(df_train[\"kategori\"].astype(str).str.strip().value_counts(dropna=False).to_string())\n\n_basic_sanity(\"ISPU (ALL) CLEAN (post-label-sanitize)\", df_ispu_all, key_cols=[\"tanggal\", \"stasiun\"])\n\n# ============================================================\n# 2) NDVI + stasiun_code\n# ============================================================\ndf_ndvi = _standardize_columns(_read_csv_smart(DATA_ROOT / \"NDVI (vegetation index)\" / \"indeks-ndvi-jakarta.csv\"))\nif \"tanggal\" in df_ndvi.columns:\n    df_ndvi[\"tanggal\"] = parse_date_twopass(df_ndvi[\"tanggal\"])\nif \"stasiun\" in df_ndvi.columns:\n    df_ndvi[\"stasiun\"] = df_ndvi[\"stasiun\"].astype(str).str.strip().str.upper().str.replace(\" \", \"\", regex=False)\n    df_ndvi[\"stasiun_code\"] = df_ndvi[\"stasiun\"].str.extract(r\"(DKI\\d+)\", expand=False)\ndf_ndvi = _coerce_numeric(df_ndvi, [\"ndvi\"])\ndf_ndvi = df_ndvi.dropna(subset=[\"tanggal\", \"stasiun\"]).copy()\ndf_ndvi = _dedup_keep_most_complete(df_ndvi, [\"tanggal\", \"stasiun\"])\n_basic_sanity(\"NDVI CLEAN\", df_ndvi, key_cols=[\"tanggal\", \"stasiun\"])\n\n# ============================================================\n# 3) Holidays (clean to one row per date)\n# ============================================================\ndf_holiday = _standardize_columns(_read_csv_smart(DATA_ROOT / \"libur-nasional\" / \"dataset-libur-nasional-dan-weekend.csv\"))\nif \"tanggal\" in df_holiday.columns:\n    df_holiday[\"tanggal\"] = parse_date_twopass(df_holiday[\"tanggal\"])\ndf_holiday = df_holiday.dropna(subset=[\"tanggal\"]).sort_values(\"tanggal\").copy()\n\nfor c in [\"is_holiday_nasional\", \"is_weekend\"]:\n    if c in df_holiday.columns:\n        df_holiday[c] = pd.to_numeric(df_holiday[c], errors=\"coerce\").fillna(0).astype(int)\n\nagg = {}\nif \"is_holiday_nasional\" in df_holiday.columns: agg[\"is_holiday_nasional\"] = \"max\"\nif \"is_weekend\" in df_holiday.columns: agg[\"is_weekend\"] = \"max\"\nif \"nama_libur\" in df_holiday.columns: agg[\"nama_libur\"] = \"first\"\ndf_holiday = df_holiday.groupby(\"tanggal\", as_index=False).agg(agg)\ndf_holiday[\"day_name\"] = df_holiday[\"tanggal\"].dt.day_name()\n\n_basic_sanity(\"HOLIDAYS CLEAN\", df_holiday, key_cols=[\"tanggal\"])\n\n# ============================================================\n# 4) Weather (multiple stations) clean\n# ============================================================\nweather_files = sorted((DATA_ROOT / \"cuaca-harian\").glob(\"*.csv\"))\nassert len(weather_files) > 0, \"No weather CSV files found.\"\n\nw_frames = []\nfor p in weather_files:\n    w = _standardize_columns(_read_csv_smart(p))\n    tag = p.stem.lower().replace(\"cuaca_harian_\", \"\").replace(\"cuaca-harian-\", \"\")\n    w[\"weather_station\"] = tag\n    w[\"weather_code\"] = (pd.Series([tag] * len(w)).str.extract(r\"(dki\\d)\", expand=False).str.upper())\n    if \"tanggal\" in w.columns:\n        w[\"tanggal\"] = parse_date_twopass(w[\"tanggal\"])\n    w_frames.append(w)\n\ndf_weather = pd.concat(w_frames, ignore_index=True, sort=False)\ndf_weather = df_weather.dropna(subset=[\"tanggal\"]).copy()\n\nfor c in df_weather.columns:\n    if c not in [\"tanggal\", \"weather_station\", \"weather_code\"]:\n        if df_weather[c].dtype == object:\n            df_weather[c] = pd.to_numeric(df_weather[c], errors=\"ignore\")\n\ndf_weather = _dedup_keep_most_complete(df_weather, [\"tanggal\", \"weather_station\"])\ndf_weather = df_weather.sort_values([\"weather_station\", \"tanggal\"]).reset_index(drop=True)\n\n_basic_sanity(\"WEATHER (ALL) CLEAN\", df_weather, key_cols=[\"tanggal\", \"weather_station\"])\n\n# ============================================================\n# 5) Population\n# ============================================================\ndf_pop = _standardize_columns(_read_csv_smart(DATA_ROOT / \"jumlah-penduduk\" / \"data-jumlah-penduduk-provinsi-dki-jakarta-berdasarkan-kelompok-usia-dan-jenis-kelamin-tahun-2013-2021-komponen-data.csv\"))\nif \"tahun\" in df_pop.columns:\n    df_pop[\"tahun\"] = pd.to_numeric(df_pop[\"tahun\"], errors=\"coerce\")\nif \"jumlah_penduduk\" in df_pop.columns:\n    df_pop[\"jumlah_penduduk\"] = pd.to_numeric(df_pop[\"jumlah_penduduk\"], errors=\"coerce\")\n_basic_sanity(\"POPULATION\", df_pop)\n\n# ============================================================\n# 6) River Quality\n# ============================================================\ndf_river = _standardize_columns(_read_csv_smart(DATA_ROOT / \"kualitas-air-sungai\" / \"data-kualitas-air-sungai-komponen-data.csv\"))\nfor c in [\"latitude\", \"longitude\", \"baku_mutu\", \"hasil_pengukuran\", \"bulan_sampling\"]:\n    if c in df_river.columns:\n        df_river[c] = pd.to_numeric(df_river[c], errors=\"coerce\")\n_basic_sanity(\"RIVER QUALITY\", df_river)\n\n# ============================================================\n# 7) Find test mapping file candidates (rows == sample_submission) with an 'id' column\n# ============================================================\ntest_candidates = []\nfor p in DATA_ROOT.rglob(\"*.csv\"):\n    if p.name == \"sample_submission.csv\":\n        continue\n    name = p.name.lower()\n    if (\"test\" in name) or (\"submission\" in name):\n        try:\n            tmp = _standardize_columns(_read_csv_smart(p).head(2))\n            if \"id\" in tmp.columns:\n                df_full = _standardize_columns(_read_csv_smart(p))\n                if len(df_full) == n_test_expected:\n                    test_candidates.append((str(p), list(df_full.columns)))\n        except Exception:\n            pass\n\nprint(\"\\n--- Test mapping file candidates (rows == sample_submission) ---\")\nif len(test_candidates) == 0:\n    print(\"None found by heuristic. Likely there is a separate test file not matching this heuristic name.\")\nelse:\n    for fp, cols in test_candidates:\n        print(fp, \"| cols:\", cols)\n\n# ============================================================\n# Preview\n# ============================================================\nprint(\"\\n--- Preview heads ---\")\ndisplay(df_train.head(3))\ndisplay(df_ndvi.head(3))\ndisplay(df_holiday.head(3))\ndisplay(df_weather.head(3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Master Table Building (Correct Joins)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 2 — Master Table Building (Correct Joins) (ONE CELL) — FINAL CONTEXT SAFE\n# Builds:\n#   df_train_master  (for training)\n#   df_test_master   (for inference; requires test mapping file with id)\n# Also provides:\n#   df_test_like     (alias for Step 5 requirement)\n#\n# Notes:\n# - Joins are leakage-safe (no rolling/lag here; that is Step 3).\n# - Safe joins:\n#     Holiday (by tanggal)\n#     NDVI (tanggal+stasiun_code)\n#     Weather local (tanggal+stasiun_code) + global fallback (tanggal)\n#     Population (year aggregate; global)\n#     River (year-month aggregate; global)\n# ============================================================\n\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# ---- guards (assumes Step 1 already ran) ----\nneed = [\"sub\",\"ID_COL\",\"SUB_TARGET_COL\",\"df_train\",\"df_ndvi\",\"df_holiday\",\"df_weather\",\"df_pop\",\"df_river\"]\nmiss = [k for k in need if k not in globals()]\nif miss:\n    raise RuntimeError(f\"Missing globals from Step 1: {miss}. Jalankan Step 1 dulu.\")\n\nDATA_ROOT = Path(\"/kaggle/input/penyisihan-datavidia-10\")\nassert DATA_ROOT.exists(), f\"DATA_ROOT not found: {DATA_ROOT}\"\n\n# ----------------------------\n# Helpers (keep consistent with Step 1)\n# ----------------------------\ndef _norm_col(c: str) -> str:\n    c = str(c).strip().lower()\n    c = re.sub(r\"[^\\w]+\", \"_\", c)\n    c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n    return c\n\ndef _standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n    return df.rename(columns={c: _norm_col(c) for c in df.columns})\n\ndef parse_date_twopass(s: pd.Series) -> pd.Series:\n    s = s.astype(str).str.strip()\n    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan, \"None\": np.nan})\n    d1 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n    m = d1.isna()\n    if m.any():\n        d2 = pd.to_datetime(s[m], errors=\"coerce\", dayfirst=False)\n        d1.loc[m] = d2\n    return d1\n\ndef _mk_stasiun_code(stasiun_series: pd.Series) -> pd.Series:\n    st = stasiun_series.astype(str).str.strip().str.upper()\n    code = st.str.extract(r\"(DKI\\s*\\d+)\", expand=False).str.replace(\" \", \"\", regex=False)\n    return code\n\ndef _prefix_cols(df: pd.DataFrame, prefix: str, keep: set) -> pd.DataFrame:\n    ren = {c: f\"{prefix}{c}\" for c in df.columns if c not in keep}\n    return df.rename(columns=ren)\n\ndef _dedup_keep_most_complete(df: pd.DataFrame, key_cols):\n    if not all(k in df.columns for k in key_cols):\n        return df\n    df = df.copy()\n    df[\"_nn\"] = df.notna().sum(axis=1)\n    idx = df.groupby(key_cols)[\"_nn\"].idxmax()\n    df = df.loc[idx].drop(columns=[\"_nn\"]).reset_index(drop=True)\n    return df\n\ndef _find_test_mapping_file(data_root: Path, n_rows: int) -> Path | None:\n    \"\"\"\n    Cari CSV (selain sample_submission) yang punya kolom 'id' dan jumlah baris == n_rows.\n    Lebih robust: baca head dulu, lalu baca full hanya kolom 'id' untuk cek panjang.\n    \"\"\"\n    for p in data_root.rglob(\"*.csv\"):\n        if p.name == \"sample_submission.csv\":\n            continue\n        try:\n            head = _standardize_columns(pd.read_csv(p, nrows=5))\n            if \"id\" not in head.columns:\n                continue\n            # cek rowcount cepat: baca hanya kolom id kalau bisa\n            try:\n                df_id = _standardize_columns(pd.read_csv(p, usecols=[\"id\"]))\n                if len(df_id) == n_rows:\n                    return p\n            except Exception:\n                df_full = _standardize_columns(pd.read_csv(p))\n                if \"id\" in df_full.columns and len(df_full) == n_rows:\n                    return p\n        except Exception:\n            continue\n    return None\n\ndef _ensure_datetime(df: pd.DataFrame, col=\"tanggal\") -> pd.DataFrame:\n    df = df.copy()\n    if col in df.columns and not np.issubdtype(df[col].dtype, np.datetime64):\n        df[col] = parse_date_twopass(df[col])\n    return df\n\ndef _select_numeric_cols(df: pd.DataFrame, keep: set):\n    cols = []\n    for c in df.columns:\n        if c in keep or pd.api.types.is_numeric_dtype(df[c]):\n            cols.append(c)\n    return cols\n\n# ----------------------------\n# Core builder\n# ----------------------------\ndef build_master(df_base: pd.DataFrame, *, has_target: bool) -> pd.DataFrame:\n    df = df_base.copy()\n\n    # ensure tanggal exists + datetime\n    if \"tanggal\" not in df.columns:\n        raise RuntimeError(\"Base df missing 'tanggal'.\")\n    df = _ensure_datetime(df, \"tanggal\")\n\n    # ensure stasiun_code\n    if \"stasiun_code\" not in df.columns:\n        if \"stasiun\" in df.columns:\n            df[\"stasiun_code\"] = _mk_stasiun_code(df[\"stasiun\"])\n        else:\n            df[\"stasiun_code\"] = np.nan\n\n    # basic calendar (safe)\n    df[\"year\"] = df[\"tanggal\"].dt.year.astype(\"Int64\")\n    df[\"month\"] = df[\"tanggal\"].dt.month.astype(\"Int64\")\n    df[\"day\"] = df[\"tanggal\"].dt.day.astype(\"Int64\")\n    df[\"dow\"] = df[\"tanggal\"].dt.dayofweek.astype(\"Int64\")\n    df[\"dayofyear\"] = df[\"tanggal\"].dt.dayofyear.astype(\"Int64\")\n\n    # ---- Holiday join: by tanggal ----\n    hol = df_holiday.copy()\n    hol = _ensure_datetime(hol, \"tanggal\")\n    hol = hol.dropna(subset=[\"tanggal\"]).drop_duplicates([\"tanggal\"])\n    df = df.merge(hol, on=\"tanggal\", how=\"left\")\n\n    # ---- NDVI join: by tanggal + stasiun_code ----\n    nd = df_ndvi.copy()\n    nd = _ensure_datetime(nd, \"tanggal\")\n    if \"stasiun_code\" not in nd.columns:\n        if \"stasiun\" in nd.columns:\n            nd[\"stasiun_code\"] = _mk_stasiun_code(nd[\"stasiun\"])\n    nd = nd.dropna(subset=[\"tanggal\",\"stasiun_code\"]).drop_duplicates([\"tanggal\",\"stasiun_code\"])\n    # drop duplicated raw station name to avoid messy columns\n    drop_raw = [c for c in [\"stasiun\"] if c in nd.columns]\n    if drop_raw:\n        nd = nd.drop(columns=drop_raw)\n    df = df.merge(nd, on=[\"tanggal\",\"stasiun_code\"], how=\"left\")\n\n    # ---- Weather join: local (tanggal+stasiun_code) + global fallback (tanggal) ----\n    wx = df_weather.copy()\n    wx = _ensure_datetime(wx, \"tanggal\")\n    if \"weather_code\" in wx.columns:\n        wx[\"weather_code\"] = wx[\"weather_code\"].astype(str).str.strip().str.upper()\n    else:\n        wx[\"weather_code\"] = np.nan\n\n    # local\n    wx_loc = wx.dropna(subset=[\"tanggal\",\"weather_code\"]).copy()\n    wx_loc = wx_loc.rename(columns={\"weather_code\":\"stasiun_code\"})\n    keep_keys = {\"tanggal\",\"stasiun_code\",\"weather_station\"}\n    wx_loc_cols = _select_numeric_cols(wx_loc, keep=keep_keys)\n    wx_loc = wx_loc[wx_loc_cols].drop_duplicates([\"tanggal\",\"stasiun_code\"])\n    wx_loc = _prefix_cols(wx_loc, \"wx_\", keep={\"tanggal\",\"stasiun_code\"})\n    df = df.merge(wx_loc, on=[\"tanggal\",\"stasiun_code\"], how=\"left\")\n\n    # global mean by tanggal\n    wx_g = wx.dropna(subset=[\"tanggal\"]).copy()\n    wx_g_num = [c for c in wx_g.columns if pd.api.types.is_numeric_dtype(wx_g[c])]\n    if len(wx_g_num) > 0:\n        wx_g = wx_g.groupby(\"tanggal\", as_index=False)[wx_g_num].mean(numeric_only=True)\n        wx_g = _prefix_cols(wx_g, \"wxg_\", keep={\"tanggal\"})\n        df = df.merge(wx_g, on=\"tanggal\", how=\"left\")\n\n        # fill local wx_ with global wxg_ (same base name)\n        for c in list(df.columns):\n            if c.startswith(\"wx_\"):\n                base = c.replace(\"wx_\", \"\")\n                cg = \"wxg_\" + base\n                if cg in df.columns:\n                    df[c] = df[c].fillna(df[cg])\n\n    # ---- Population join: total per year (global) ----\n    pop = df_pop.copy()\n    # try normalize if column names differ\n    if \"tahun\" in pop.columns:\n        pop[\"tahun\"] = pd.to_numeric(pop[\"tahun\"], errors=\"coerce\")\n    if \"jumlah_penduduk\" in pop.columns:\n        pop[\"jumlah_penduduk\"] = pd.to_numeric(pop[\"jumlah_penduduk\"], errors=\"coerce\")\n    if {\"tahun\",\"jumlah_penduduk\"}.issubset(pop.columns):\n        pop_y = pop.dropna(subset=[\"tahun\",\"jumlah_penduduk\"]).groupby(\"tahun\", as_index=False)[\"jumlah_penduduk\"].sum()\n        pop_y = pop_y.rename(columns={\"tahun\":\"year\",\"jumlah_penduduk\":\"pop_total_year\"})\n        df = df.merge(pop_y, on=\"year\", how=\"left\")\n\n    # ---- River join: global year-month aggregates ----\n    riv = df_river.copy()\n    # convert to numeric if exist\n    for cc in [\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"]:\n        if cc in riv.columns:\n            riv[cc] = pd.to_numeric(riv[cc], errors=\"coerce\")\n\n    if {\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"}.issubset(riv.columns):\n        r = riv.dropna(subset=[\"periode_data\",\"bulan_sampling\",\"baku_mutu\",\"hasil_pengukuran\"]).copy()\n        r[\"ratio_to_std\"] = r[\"hasil_pengukuran\"] / (r[\"baku_mutu\"].replace(0, np.nan))\n        r[\"exceed\"] = (r[\"hasil_pengukuran\"] > r[\"baku_mutu\"]).astype(int)\n        r_agg = r.groupby([\"periode_data\",\"bulan_sampling\"], as_index=False).agg(\n            river_exceed_rate=(\"exceed\",\"mean\"),\n            river_ratio_mean=(\"ratio_to_std\",\"mean\"),\n            river_n=(\"exceed\",\"size\"),\n        )\n        r_agg = r_agg.rename(columns={\"periode_data\":\"year\", \"bulan_sampling\":\"month\"})\n        df = df.merge(r_agg, on=[\"year\",\"month\"], how=\"left\")\n\n    # ---- keep CatBoost-friendly categoricals ----\n    for c in [\"stasiun\",\"stasiun_code\",\"parameter_pencemar_kritis\",\"day_name\",\"nama_libur\",\"weather_station\"]:\n        if c in df.columns:\n            # keep object (not forced to string \"nan\")\n            df[c] = df[c].where(df[c].notna(), np.nan).astype(\"object\")\n\n    # ---- final dedup on key ----\n    key_cols = [\"tanggal\",\"stasiun_code\"] if \"stasiun_code\" in df.columns else [\"tanggal\"]\n    df = _dedup_keep_most_complete(df, key_cols)\n\n    # target cleanup (safe)\n    if has_target and \"kategori\" in df.columns:\n        df[\"kategori\"] = df[\"kategori\"].astype(str).str.strip()\n\n    return df\n\n# ============================================================\n# 1) Build TRAIN master\n# ============================================================\ndf_train_master = build_master(df_train, has_target=True)\nprint(\"df_train_master:\", df_train_master.shape)\nif \"stasiun_code\" in df_train_master.columns:\n    print(\"train key duplicates:\", int(df_train_master.duplicated([\"tanggal\",\"stasiun_code\"]).sum()))\n\n# ============================================================\n# 2) Build TEST master (needs id mapping file)\n# ============================================================\nn_test_expected = len(sub)\n\ntest_path = None\n# Prefer Step 1 test_candidates if available\nif \"test_candidates\" in globals() and isinstance(test_candidates, list) and len(test_candidates) > 0:\n    # pick the first candidate\n    try:\n        test_path = Path(test_candidates[0][0])\n    except Exception:\n        test_path = None\n\nif test_path is None:\n    test_path = _find_test_mapping_file(DATA_ROOT, n_test_expected)\n\nif test_path is None:\n    print(\"\\n[WARN] Test mapping file (id -> tanggal/stasiun) not found yet.\")\n    print(\"       df_test_master=None, but you can still run Step 3–4 on train.\")\n    df_test_master = None\n    df_test_like = None\nelse:\n    print(\"\\nTest mapping file:\", str(test_path))\n    df_test = _standardize_columns(pd.read_csv(test_path))\n\n    # basic checks\n    if \"id\" not in df_test.columns:\n        raise RuntimeError(f\"Test file has no 'id': {test_path}\")\n    if len(df_test) != n_test_expected:\n        raise RuntimeError(f\"Test file rows != submission rows: {len(df_test)} vs {n_test_expected}\")\n\n    # ensure tanggal\n    if \"tanggal\" not in df_test.columns:\n        raise RuntimeError(\"Test mapping file missing 'tanggal'.\")\n    df_test[\"tanggal\"] = parse_date_twopass(df_test[\"tanggal\"])\n\n    # ensure stasiun_code\n    if \"stasiun_code\" not in df_test.columns:\n        if \"stasiun\" in df_test.columns:\n            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[\"stasiun\"])\n        elif \"stasiun_id\" in df_test.columns:\n            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[\"stasiun_id\"])\n        else:\n            # fallback: find any col containing DKI\n            cand = None\n            for c in df_test.columns:\n                if df_test[c].astype(str).str.contains(\"dki\", case=False, na=False).any():\n                    cand = c\n                    break\n            if cand is None:\n                raise RuntimeError(\"Cannot infer station from test mapping file.\")\n            df_test[\"stasiun_code\"] = _mk_stasiun_code(df_test[cand])\n\n    # build master\n    df_test_master = build_master(df_test, has_target=False)\n    # keep id column for submission\n    df_test_master[\"id\"] = df_test[\"id\"].values\n\n    # IMPORTANT: Provide alias required by your Step 5 code\n    df_test_like = df_test_master.copy()\n\n    print(\"df_test_master:\", df_test_master.shape)\n    print(\"[OK] df_test_like created for Step 5:\", df_test_like.shape)\n\n# ============================================================\n# Preview\n# ============================================================\ndisplay(df_train_master.head(3))\nif df_test_master is not None:\n    display(df_test_master.head(3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineering (Time-Series + Calendar + Robustness)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 3 — Feature Engineering (Time-Series + Calendar + Robustness) — FINAL\n# Requires:\n#   df_train_master  (from Step 2)\n#   df_test_master   (optional; from Step 2)\n# Produces:\n#   df_train_fe, df_test_fe\n# Notes:\n# - STRICT leakage-safe (all rolling on shifted values only)\n# - Station-aware (grouped by stasiun_code)\n# - CatBoost-friendly (categorical kept as object)\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\n\n# ----------------------------\n# Guards\n# ----------------------------\nif \"df_train_master\" not in globals():\n    raise RuntimeError(\"Missing df_train_master. Run Step 2 first.\")\n\nfor c in [\"tanggal\", \"stasiun_code\"]:\n    if c not in df_train_master.columns:\n        raise RuntimeError(f\"df_train_master missing required column: {c}\")\n\n# ----------------------------\n# Calendar / time features\n# ----------------------------\ndef add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    df[\"year\"] = df[\"tanggal\"].dt.year.astype(\"Int64\")\n    df[\"month\"] = df[\"tanggal\"].dt.month.astype(\"Int64\")\n    df[\"day\"] = df[\"tanggal\"].dt.day.astype(\"Int64\")\n    df[\"dow\"] = df[\"tanggal\"].dt.dayofweek.astype(\"Int64\")\n    df[\"dayofyear\"] = df[\"tanggal\"].dt.dayofyear.astype(\"Int64\")\n\n    # cyclic encoding (safe)\n    doy = df[\"dayofyear\"].astype(float)\n    df[\"doy_sin\"] = np.sin(2 * np.pi * doy / 365.25)\n    df[\"doy_cos\"] = np.cos(2 * np.pi * doy / 365.25)\n\n    mon = df[\"month\"].astype(float)\n    df[\"mon_sin\"] = np.sin(2 * np.pi * mon / 12.0)\n    df[\"mon_cos\"] = np.cos(2 * np.pi * mon / 12.0)\n\n    # weekend fallback\n    if \"is_weekend\" not in df.columns:\n        df[\"is_weekend\"] = (df[\"dow\"].isin([5, 6])).astype(int)\n\n    return df\n\n# ----------------------------\n# Lag + rolling (STRICT leakage-safe)\n# ----------------------------\ndef add_station_lag_rolling(\n    df: pd.DataFrame,\n    group_col: str = \"stasiun_code\",\n    base_cols = (\"pm10\",\"pm25\",\"so2\",\"co\",\"o3\",\"no2\",\"max\"),\n    lags = (1, 2, 3, 7, 14),\n    windows = (3, 7, 14, 30),\n) -> pd.DataFrame:\n\n    df = df.copy()\n    df = df.sort_values([group_col, \"tanggal\"]).reset_index(drop=True)\n\n    base_cols = [c for c in base_cols if c in df.columns]\n    if len(base_cols) == 0:\n        raise RuntimeError(\"No pollutant columns found for lag/rolling features.\")\n\n    g = df.groupby(group_col, sort=False)\n\n    # --- lags ---\n    for c in base_cols:\n        for L in lags:\n            df[f\"{c}_lag{L}\"] = g[c].shift(L)\n\n    # --- rolling (on shifted series only) ---\n    for c in base_cols:\n        s = g[c].shift(1)  # critical: past only\n        for w in windows:\n            df[f\"{c}_rmean{w}\"] = (\n                s.rolling(w, min_periods=max(2, w // 3))\n                 .mean()\n                 .reset_index(level=0, drop=True)\n            )\n            df[f\"{c}_rstd{w}\"] = (\n                s.rolling(w, min_periods=max(2, w // 3))\n                 .std()\n                 .reset_index(level=0, drop=True)\n            )\n\n    # --- deltas (safe, lag-based) ---\n    for c in base_cols:\n        if f\"{c}_lag1\" in df.columns and f\"{c}_lag2\" in df.columns:\n            df[f\"{c}_d12\"] = df[f\"{c}_lag1\"] - df[f\"{c}_lag2\"]\n        if f\"{c}_lag1\" in df.columns and f\"{c}_rmean7\" in df.columns:\n            df[f\"{c}_d1_rm7\"] = df[f\"{c}_lag1\"] - df[f\"{c}_rmean7\"]\n\n    return df\n\n# ----------------------------\n# Weather interactions (robust)\n# ----------------------------\ndef add_weather_interactions(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    # helper: pick first existing column\n    def pick(cols):\n        for c in cols:\n            if c in df.columns:\n                return c\n        return None\n\n    wind = pick([\n        \"wx_wind_speed_10m_mean_km_h\",\n        \"wxg_wind_speed_10m_mean_km_h\",\n        \"wind_speed_10m_mean_km_h\",\n    ])\n    prec = pick([\n        \"wx_precipitation_sum_mm\",\n        \"wxg_precipitation_sum_mm\",\n        \"precipitation_sum_mm\",\n    ])\n    rad = pick([\n        \"wx_shortwave_radiation_sum_mj_m²\",\n        \"wxg_shortwave_radiation_sum_mj_m²\",\n        \"shortwave_radiation_sum_mj_m²\",\n    ])\n    rh = pick([\n        \"wx_relative_humidity_2m_mean\",\n        \"wxg_relative_humidity_2m_mean\",\n        \"relative_humidity_2m_mean\",\n    ])\n    tmp = pick([\n        \"wx_temperature_2m_mean_c\",\n        \"wxg_temperature_2m_mean_c\",\n        \"temperature_2m_mean_c\",\n    ])\n\n    # interactions based on lag1 (already leakage-safe)\n    if \"pm25_lag1\" in df.columns and wind is not None:\n        df[\"pm25_lag1_x_wind\"] = df[\"pm25_lag1\"] * df[wind]\n\n    if \"pm10_lag1\" in df.columns and wind is not None:\n        df[\"pm10_lag1_x_wind\"] = df[\"pm10_lag1\"] * df[wind]\n\n    if \"o3_lag1\" in df.columns and rad is not None:\n        df[\"o3_lag1_x_rad\"] = df[\"o3_lag1\"] * df[rad]\n\n    if \"pm25_lag1\" in df.columns and prec is not None:\n        df[\"pm25_lag1_div_prec\"] = df[\"pm25_lag1\"] / (df[prec].fillna(0) + 1.0)\n\n    if \"co_lag1\" in df.columns and rh is not None:\n        df[\"co_lag1_x_rh\"] = df[\"co_lag1\"] * df[rh]\n\n    if \"pm25_lag1\" in df.columns and tmp is not None:\n        df[\"pm25_lag1_x_temp\"] = df[\"pm25_lag1\"] * df[tmp]\n\n    return df\n\n# ----------------------------\n# Final type fixing for CatBoost\n# ----------------------------\ndef finalize_types_for_catboost(df: pd.DataFrame) -> pd.DataFrame:\n    df = df.copy()\n\n    # categorical columns (keep object)\n    for c in [\n        \"stasiun\",\n        \"stasiun_code\",\n        \"parameter_pencemar_kritis\",\n        \"day_name\",\n        \"nama_libur\",\n        \"weather_station\",\n    ]:\n        if c in df.columns:\n            df[c] = df[c].astype(\"object\")\n\n    # binary flags\n    for c in [\"is_weekend\", \"is_holiday_nasional\"]:\n        if c in df.columns:\n            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n\n    return df\n\n# ----------------------------\n# Build FE\n# ----------------------------\ndef build_fe(df: pd.DataFrame) -> pd.DataFrame:\n    df = add_time_features(df)\n    df = add_station_lag_rolling(df)\n    df = add_weather_interactions(df)\n    df = finalize_types_for_catboost(df)\n    return df\n\n# ----------------------------\n# Apply to train / test\n# ----------------------------\ndf_train_fe = build_fe(df_train_master)\n\nif \"df_test_master\" in globals() and df_test_master is not None:\n    df_test_fe = build_fe(df_test_master)\nelse:\n    df_test_fe = None\n\n# ----------------------------\n# Sanity checks\n# ----------------------------\nprint(\"df_train_fe:\", df_train_fe.shape)\nif df_test_fe is not None:\n    print(\"df_test_fe :\", df_test_fe.shape)\n\ntop_miss = (df_train_fe.isna().mean()\n            .sort_values(ascending=False)\n            .head(15) * 100).round(2)\n\nprint(\"\\nTop missing% (train_fe):\")\nprint(top_miss.to_string())\n\ndisplay(df_train_fe.head(3))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training (Time-Based CV + CatBoost Optimization)","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# STEP 4 — Model Training (Time-Based CV + CatBoost) — REVISED (OOF FIX)\n# Fixes vs your last:\n# - OOF computed ONLY on rows that actually appear in validation (no zero-proba rows)\n# - Time folds do NOT require train to contain ALL classes (rare-class safe)\n# - Still CPU only, auto categorical detection + sanitization\n# Outputs:\n# - feature_cols, cat_cols, classes, class_to_id, id_to_class\n# - folds\n# - models\n# - oof_proba (only valid rows filled), oof_pred (for valid rows), oof_macro_f1\n# - oof_valid_mask, oof_valid_idx\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import f1_score, classification_report\n\nif \"df_train_fe\" not in globals():\n    raise RuntimeError(\"Missing df_train_fe. Run Step 3 first.\")\n\n# ----------------------------\n# Config\n# ----------------------------\nN_SPLITS = 4\nGAP_DAYS = 0\nSEEDS = [42]\n\nITERATIONS = 8000\nLR = 0.05\nDEPTH = 8\nL2 = 6.0\n\nMISSING_CAT = \"__MISSING__\"\n\n# ----------------------------\n# Prepare data\n# ----------------------------\ndf = df_train_fe.copy()\nif \"tanggal\" not in df.columns or \"kategori\" not in df.columns:\n    raise RuntimeError(\"df_train_fe must contain 'tanggal' and 'kategori'.\")\n\ndf = df.dropna(subset=[\"tanggal\"]).sort_values(\n    [\"tanggal\", \"stasiun_code\"] if \"stasiun_code\" in df.columns else [\"tanggal\"]\n).reset_index(drop=True)\n\ndrop_cols = {\"kategori\", \"tanggal\", \"source_file\", \"periode_data\"}\nif \"id\" in df.columns:\n    drop_cols.add(\"id\")\n\n# align features with test (if exists)\nif \"df_test_fe\" in globals() and df_test_fe is not None:\n    common = [c for c in df.columns if c in df_test_fe.columns]\n    feature_cols = [c for c in common if c not in drop_cols]\nelse:\n    feature_cols = [c for c in df.columns if c not in drop_cols]\n\nX = df[feature_cols].copy()\n\n# clean target\ny_str = df[\"kategori\"].astype(str).str.strip()\ny_str = y_str[y_str.str.lower() != \"nan\"]\ndf = df.loc[y_str.index].reset_index(drop=True)\nX  = X.loc[y_str.index].reset_index(drop=True)\ny_str = y_str.reset_index(drop=True)\n\n# ----------------------------\n# AUTO categorical detection + sanitization\n# ----------------------------\nis_num = X.apply(pd.api.types.is_numeric_dtype)\ncat_cols = X.columns[~is_num].tolist()\n\nfor c in cat_cols:\n    X[c] = X[c].where(X[c].notna(), MISSING_CAT).astype(str)\n    X[c] = X[c].replace({\"nan\": MISSING_CAT, \"None\": MISSING_CAT, \"\": MISSING_CAT})\n\nnum_cols = X.columns[is_num].tolist()\nfor c in num_cols:\n    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n\nbad_num = [c for c in num_cols if X[c].dtype == object]\nif bad_num:\n    raise RuntimeError(f\"Numeric columns still object after coercion: {bad_num[:10]}\")\n\n# ----------------------------\n# classes + weights (handles rare classes)\n# ----------------------------\nclasses = sorted(y_str.unique().tolist())\ncounts = y_str.value_counts()\n\n# avoid division by zero (shouldn't happen because classes from y_str)\nclass_weights = [float(len(y_str) / (len(classes) * counts[c])) for c in classes]\n\nclass_to_id = {c: i for i, c in enumerate(classes)}\nid_to_class = {i: c for c, i in class_to_id.items()}\ny = y_str.map(class_to_id).astype(int)\n\nprint(\"Train rows:\", len(df), \"| n_features:\", len(feature_cols), \"| n_cat(auto):\", len(cat_cols))\nprint(\"Classes:\", classes)\nprint(\"Class counts:\\n\", counts.to_string())\n\n# ----------------------------\n# Time folds (year-based preferred; no \"all-classes-in-train\" constraint)\n# ----------------------------\ndef make_time_folds(df_in: pd.DataFrame, n_splits=4, gap_days=0):\n    d = df_in.copy()\n    d[\"year\"] = d[\"tanggal\"].dt.year\n    years = sorted(d[\"year\"].dropna().unique().tolist())\n\n    folds = []\n\n    # year-based: take newest years as validation (classic time split)\n    if len(years) >= 2:\n        for vy in years[::-1]:\n            tr_mask = d[\"year\"] < vy\n            va_mask = d[\"year\"] == vy\n            if not va_mask.any():\n                continue\n            if gap_days > 0:\n                va_start = d.loc[va_mask, \"tanggal\"].min()\n                tr_mask = tr_mask & (d[\"tanggal\"] <= (va_start - pd.Timedelta(days=gap_days)))\n\n            tr_idx = d.index[tr_mask].to_numpy()\n            va_idx = d.index[va_mask].to_numpy()\n            if len(tr_idx) == 0 or len(va_idx) == 0:\n                continue\n\n            folds.append((tr_idx, va_idx))\n            if len(folds) >= n_splits:\n                break\n\n        if folds:\n            return folds\n\n    # fallback: date blocks\n    uniq_dates = np.array(sorted(d[\"tanggal\"].unique()))\n    blocks = np.array_split(uniq_dates, n_splits + 1)\n    for b in blocks[1:][::-1]:\n        va_start, va_end = b.min(), b.max()\n        tr_mask = d[\"tanggal\"] < va_start\n        va_mask = (d[\"tanggal\"] >= va_start) & (d[\"tanggal\"] <= va_end)\n        if gap_days > 0:\n            tr_mask = tr_mask & (d[\"tanggal\"] <= (va_start - pd.Timedelta(days=gap_days)))\n\n        tr_idx = d.index[tr_mask].to_numpy()\n        va_idx = d.index[va_mask].to_numpy()\n        if len(tr_idx) == 0 or len(va_idx) == 0:\n            continue\n\n        folds.append((tr_idx, va_idx))\n        if len(folds) >= n_splits:\n            break\n\n    # final fallback: last 20%\n    if not folds:\n        cut = int(len(d) * 0.8)\n        folds = [(d.index[:cut].to_numpy(), d.index[cut:].to_numpy())]\n    return folds\n\nfolds = make_time_folds(df, n_splits=N_SPLITS, gap_days=GAP_DAYS)\n\nprint(\"\\nFolds:\")\nfor i, (tr, va) in enumerate(folds):\n    dtr = (df.loc[tr, \"tanggal\"].min(), df.loc[tr, \"tanggal\"].max())\n    dva = (df.loc[va, \"tanggal\"].min(), df.loc[va, \"tanggal\"].max())\n    print(f\"fold{i}: train={len(tr)} [{dtr[0]}..{dtr[1]}] | valid={len(va)} [{dva[0]}..{dva[1]}]\")\n\n# ----------------------------\n# Train CV (CPU only) + OOF (VALID-ONLY)\n# ----------------------------\nK = len(classes)\noof_proba = np.zeros((len(df), K), dtype=np.float32)\noof_valid_mask = np.zeros(len(df), dtype=bool)\n\nmodels = []\nfold_scores = []\n\nfor seed in SEEDS:\n    print(f\"\\n=== SEED {seed} | task_type=CPU ===\")\n    for fi, (tr_idx, va_idx) in enumerate(folds):\n        X_tr, y_tr = X.iloc[tr_idx], y.iloc[tr_idx]\n        X_va, y_va = X.iloc[va_idx], y.iloc[va_idx]\n\n        train_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n        valid_pool = Pool(X_va, y_va, cat_features=cat_cols)\n\n        model = CatBoostClassifier(\n            loss_function=\"MultiClass\",\n            eval_metric=\"TotalF1\",\n            classes_count=K,\n            class_weights=class_weights,\n            iterations=ITERATIONS,\n            learning_rate=LR,\n            depth=DEPTH,\n            l2_leaf_reg=L2,\n            random_strength=1.0,\n            bagging_temperature=0.5,\n            border_count=128,\n            random_seed=seed,\n            od_type=\"Iter\",\n            od_wait=400,\n            task_type=\"CPU\",\n            thread_count=-1,\n            verbose=250\n        )\n\n        model.fit(train_pool, eval_set=valid_pool, use_best_model=True)\n\n        proba = model.predict_proba(X_va)\n        pred_int = np.argmax(proba, axis=1)\n        score = f1_score(y_va, pred_int, average=\"macro\")\n\n        # fill OOF only for valid rows\n        oof_proba[va_idx] += (proba / len(SEEDS))\n        oof_valid_mask[va_idx] = True\n\n        fold_scores.append(float(score))\n        models.append(model)\n\n        print(f\"[seed {seed} fold {fi}] macroF1={score:.5f} | best_iter={model.get_best_iteration()}\")\n\n# ----------------------------\n# OOF summary (VALID ONLY, IMPORTANT)\n# ----------------------------\noof_valid_idx = np.where(oof_valid_mask)[0]\nif len(oof_valid_idx) == 0:\n    raise RuntimeError(\"No OOF validation rows were filled. Check folds logic.\")\n\noof_pred_int = np.argmax(oof_proba[oof_valid_idx], axis=1)\noof_pred = np.array([id_to_class[i] for i in oof_pred_int])\n\noof_macro_f1 = f1_score(y_str.iloc[oof_valid_idx], oof_pred, average=\"macro\")\n\nprint(\"\\n=== OOF RESULTS (VALID ONLY) ===\")\nprint(\"Fold macroF1:\", [round(s, 5) for s in fold_scores])\nprint(\"OOF macroF1 :\", round(oof_macro_f1, 6))\nprint(f\"OOF covered rows: {len(oof_valid_idx)} / {len(df)}\")\n\nprint(\"\\nOOF classification report (valid-only):\")\nprint(classification_report(y_str.iloc[oof_valid_idx], oof_pred, digits=4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference, Ensembling, Submission & QA","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}